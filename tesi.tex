\documentclass[corpo=11pt, stile=classica, tipotesi=custom,
greek, evenboxes, english]{toptesi}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{hyperref}
\hypersetup{%
	pdfpagemode={UseOutlines},
	bookmarksopen,
	pdfstartview={FitH},
	colorlinks,
	linkcolor={blue},
	citecolor={blue},
	urlcolor={blue}
}

\usepackage{geometry} %for the margins
\newcommand\fillin[1][4cm]{\makebox[#1]{\dotfill}} %for the dotted line in the frontispiace

\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1} } %to vetical align numbers in tables, along the decimal dot

\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathrsfs}
%\usepackage{dutchcal}

\DeclareMathAlphabet{\mathdutchcal}{U}{dutchcal}{m}{n}
\SetMathAlphabet{\mathdutchcal}{bold}{U}{dutchcal}{b}{n}
\DeclareMathAlphabet{\mathdutchbcal}{U}{dutchcal}{b}{n}

\usepackage{esint}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{systeme}
\usepackage{mathtools}
\hypersetup{colorlinks, linkcolor=blue, citecolor=blue}
\usepackage{cancel}
\usepackage{aligned-overset}
\usepackage{color}
\usepackage[
backend=bibtex,
style=numeric,
sorting=nyt
]{biblatex}
\addbibresource{bibliografia.bib}


\numberwithin{equation}{chapter}
\newtheorem{teo}{Theorem}[chapter] %in questo modo la numerazione ricomincia da capo ad ogni nuovo capitolo
\newtheorem{defi}[teo]{Definition}
\newtheorem{lem}[teo]{Lemma}
\newtheorem{cor}[teo]{Corollary}
\newtheorem{prop}[teo]{Proposition}
\theoremstyle{definition}
\newtheorem{es}[teo]{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\R}{\mathbb{R}} %scorciatoia per R reali
\newcommand{\N}{\mathbb{N}} %scorciatoia per N naturali
\newcommand{\Q}{\mathbb{Q}} %scorciatioia per Q razionali
\newcommand{\V}{\mathcal{V}} %STFT
\newcommand{\F}{\mathscr{F}} %Fourier transform
\newcommand{\Fock}{\mathcal{F}} %Fock space
\newcommand{\C}{\mathbb{C}} %Complex numbers
\newcommand{\B}{\mathscr{B}} %Bounded linear operators
\newcommand{\Barg}{\mathcal{B}} %Bargmann transform
\renewcommand{\L}{\mathscr{L}} %sesquilinear form localization operator
\newcommand{\dxdo}{dxd\omega}
\newcommand{\notazione}{\underline{\textbf{Remark Notazionale}}}
\newcommand{\Log}{\ensuremath{\mathrm{Log}_-}}
\newcommand{\finire}{\fbox{\LARGE DA FINIRE}}
\newcommand{\pdfrac}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator*{\esssup}{ess\,sup}

\begin{document}

	
\input{./title.tex}

\tableofcontents

\ringraziamenti

\english

\ringraziamenti


\chapter{Introduction}
Among all problems in time-frequency analysis, arguably one of the main one is the problem of localizing a signal (which can be an audio signal, an image, etc.) in order to derive information about its energy content in a given time domain and for a given frequency window. Over time, various tools have been developed to achieve this. In the early 1960s, Landau, Slepain and Pollak investigated in a series of articles the properties of certain localization operators obtained using time and frequency projection operators. Later, in 1988, Daubechies introduced so-called \emph{time-frequency localization operators}, which give a more geometric perspective to the localization problem. These operators, also called Daubechies' localization operators, are the focus of this thesis.
Before introducing these operators, basic concepts of functional and Fourier analysis are recalled in Chapter \ref{chapter preliminaries}. Subsequently, in Chapter \ref{chapter STFT}, a fundamental tool of time-frequency analysis is introduced, namely the \emph{short-time Fourier transform}. These two chapters provide the basis for the theory of localization operators, which is presented in Chapter \ref{chapter localization operators} in an essential way. Once these operators have been introduced, a natural question might be the following: how well can a signal be localized? Is it possible to localize a signal in an arbitrarily small subset of the time-frequency plane? The negative answer to the latter question stems from the so-called \emph{uncertainty principles}. In Chapter \ref{chapter uncertainty principles}, various uncertainty principles are presented, concerning both different transforms and different notions of concentration. Nevertheless, the central idea of each uncertainty principle is the following: a signal cannot be too concentrated in both time and frequency. This barrier forces us to change perspective and instead of asking how well a signal can be concentrated, a more appropriate question might be: how can a signal be optimally concentrated? Uncertainty principles do indeed place a lower limit on the ability to localize a signal, but in general do not tell us how to reach this limit. The last chapter of this thesis focuses precisely on the problem, between time-frequency analysis and the calculus of variations, of finding upper bounds for the norm of localization operators and determining which operators reach this limit. In the first section of Chapter \ref{chapter recent results}, a recent result by Nicola and Tilli concerning this problem is presented, while in the second section we show the solution of a generalized version of their problem.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preliminaries}\label{chapter preliminaries}
This chapter is devoted to a brief recall of some basic definitions and results of functional and Fourier analysis. In Section \ref{section basics of functional analysis} elemental concepts about operators between Banach and Hilbert spaces are presented. In addition, certain classes of operators, namely the trace class and the Hilbert-Schmidt class, are introduced with some of their most important properties. Then, in Section \ref{Fourier transform and its properties section} Fourier transform is defined and some of its essential properties are given.
\section{Basics of Functional Analysis}\label{section basics of functional analysis}
In this section we turn our attention to linear operators between Banach spaces. Throughout the section, we will refer to a generic Banach space as $X$ (or $Y$), endowed with the norm $\| \cdot \|_X$. If we are dealing with a Hilbert space, we will denote it by $H$ (or $K$) and its inner product by $\langle \cdot, \cdot \rangle_H$. Pedex in the norm and scalar product may be dropped in case there is no ambiguity.  Moreover, the whole theory is presented under the assumption that spaces are infinite dimensional, but, unless otherwise stated, everything can be adapted almost directly for finite dimensional spaces.

A generic linear operator between two Banach spaces $X$ and $Y$ will be denoted as $T : X \rightarrow Y$. As a standard notation, the image of $x \in X$ through $T$ will be indicated as $T(x)$, or equivalently as $Tx$.
\begin{defi}\label{bounded operator}
	A linear operator $T : X \rightarrow Y$ is \textbf{bounded} if there exist $C>0$ such that
	\begin{equation}\label{boundedness property}
		\| Tx \|_Y \leq C \| x \|_X \quad \forall x \in X.
	\end{equation}
\end{defi}
For linear operators,  boundedness is strictly related to continuity, as the following theorem states.
\begin{teo}\label{equivalente boundedness continuity}
	For a linear operator $T$ the following statements are equivalent:
	\begin{itemize}
		\item $T$ is continuous;
		\item $T$ is bounded.
	\end{itemize}
\end{teo}
We denote the set of linear bounded (continuous) operators from $X$ to $Y$ as $\B(X,Y)$, while if $X=Y$ we will just write $\B(X)$.

The smallest constant for which \eqref{boundedness property} holds is the \emph{norm} of $T$.
\begin{defi}\label{norm operator}
	Given $T \in \B(X,Y)$ we define its \textbf{norm} as the following number:
	\begin{equation*}
		\|T\| \coloneqq \inf\{C>0 : \| Tx \|_Y \leq C \| x \|_X \  \forall x \in X\} = \sup \left\{ \dfrac{\| Tx \|_Y}{\| x \|_X} : x \in X \setminus \{0\}\right\}.
	\end{equation*}
\end{defi}
The proof of the equivalence between two definitions is straightforward. Sometimes, in order to emphasize the spaces between which $T$ operates, we may write the norm of $T$ as $\| T \|_{X \rightarrow Y}$.

In what follows we will mostly deal with $X$ and $Y$ being $L^2(\R^d)$, that is a Hilbert space. For operators between Hilbert spaces, we can express the norm of an operator using the dual norm:
\begin{equation}\label{dual norm}
	\| T \|_H = \sup\{|\langle Tx, y \rangle_H| : x,y \in H,\,\ \|x\|_H = \|y\|_H = 1 \}.
\end{equation}

Among all operators, a rather important class is the one of \emph{compact} operators.
\begin{defi}\label{compact operator}
	An operator $T \in \B(X,Y)$ is \textbf{compact} if, for every bounded sequence $\{x_n\}_{n \in \N} \subset X$, the sequence of the images $\{Tx_n\}_{n \in \N} \subset Y$ has a converging subsequence.
\end{defi}
It is not difficult to prove that a linear combination of compact operators is still compact, so the set of compact operators is a subspace of $\B(X,Y)$. The following theorem states that it is also closed.
\begin{teo}\label{space of compact operators is closed}
	The set of compact operators is a closed subspace of $\B(X,Y)$ with respect to the operator norm topology.
\end{teo}
A first simple example of compact operators is given by \emph{finite-rank} operators.
\begin{defi}\label{finite-rank operator definition}
	An operator $T \in \B(X,Y)$ is said to be \textbf{finite-rank} if $\mathrm{Im}(T)$ is finite-dimensional.
\end{defi}
Finite rank operators are of some importance in the light of the following immediate corollary of Theorem \ref{space of compact operators is closed}.
\begin{cor}\label{limit of finite rank operator is compact}
	Let $\{T_n\}_{n \in \N} \subset \B(X,Y)$ be a sequence of finite-rank operators that converges to $T \in \B(X,Y)$. Then $T$ is compact.
\end{cor}
Therefore, if one wants to show that an operator is compact, a possible approach is to find a sequence of finite-rank operators that is converging to it.
Another crucial property of compact operators is the following.
\begin{teo}\label{composition of compact operators is compact}
	Let $X,Y,Z$ be three Banach spaces and let $T \in \B(X,Y)$, $S \in \B(Y,Z)$. Then, if at least one between $T$ and $S$ is compact, then $ST \in \B(X,Z)$ is compact.
\end{teo}

From now on we suppose that $X$ is over the field $\C$ and that $T \in \B(X)$.
\begin{defi}\label{spectrum def}
	The set $\sigma(T) = \{\lambda \in \C : T - \lambda I \text{ is not invertible}\}$ is called  the \textbf{spectrum} of $T$.
\end{defi}
For operators between finite-dimensional spaces (matrices), the spectrum consists of \emph{eigenvalues}, those $\lambda \in \C$ such that $T-\lambda I$ is not injective. However, this is no longer true for infinte-dimensional spaces. The eigenvalues are in the so-called \emph{point spectrum}, which is generally only a part of the whole spectrum. However, the following theorem states that the spectrum of compact operators resembles the spectrum of operators on finite dimensional spaces.
\begin{teo}[Fredholm's alternative]\label{Fredholm alternative}
	Let $T \in \B(X)$ be a compact operator. Then one and only one of the following happens:
	\begin{itemize}
		\item $T-I$ is invertible;
		\item $T-I$ is not injective.
	\end{itemize}
\end{teo}
Therefore, for compact operators, all the values in the spectrum, except at most for 0, are eigenvalues.

In conclusion, we focus our attention on operators on Hilbert spaces. Given $T \in \B(H,K)$, it can be shown that there exists a unique operator $T^* \in \B(K,H)$, called \textbf{adjoint} of $T$, such that:
\begin{equation*}
	\langle Tx,y \rangle_K = \langle x, T^* y \rangle_H \quad \forall\, x \in H,\, y \in K.
\end{equation*}
If $H=K$, then both $T$ and $T^*$ are in $\B(H)$ and if $T=T^*$ we say that $T$ is \textbf{self-adjoint}. If an operator is both compact and self-adjoint the following theorem states that it can be diagonalized in some suitable basis (\cite{brezis}).
\begin{teo}\label{self-adjoint compact operators are diagonalizable}
	Let $H$ be a separable Hilbert space and $T \in \B(H)$ a compact and self-adjoint operator. Then, there exists an orthonormal basis of $H$ composed of eigenvectors of $T$, with corresponding eigenvalues $\{\lambda_n\}_{n \in \N}$. Moreover $\lim_{n \rightarrow +\infty} \lambda_n = 0$.
\end{teo}
From this theorem follows the next corollary, which relates the eigenvalues of a compact self-adjoint operator with its norm.
\begin{cor}\label{norm is greatest eigenvalue}
	Let $T \in \B(H)$ be a self-adjoint compact operator on a separable Hilbert space $H$ and suppose its eigenvalues are ordered in such a way that $|\lambda_1| \geq |\lambda_2| \geq \ldots$. Then $\|T\| = |\lambda_1|$.
\end{cor}

In light of Theorem \ref{self-adjoint compact operators are diagonalizable}, it is clear that working with compact self-adjoint operators is of great importance. Thus, if an operator $T$ is compact but not self-adjoint, it could be useful to construct an operator, associated with $T$, that is also self-adjoint. This task is easily accomplished considering $T^*T$. The relation between $T$ and $T^*T$ is stated by the following corollary of Theorem \ref{self-adjoint compact operators are diagonalizable}.
\begin{cor}\label{canonical form for compact operators corollary}
	Let $T \in \B(H)$ be a compact operator. Then, there exists orthonormal sets $\{e_n\}_{n \in \N}$ and $\{y_n\}_{n \in \N}$ and non-negative real numbers $\{\mu_n\}_{n \in \N}$, with $\lim_{n \rightarrow +\infty} \mu_n = 0$, so that
	\begin{equation}\label{canonical form for compact operators formula}
		T = \sum_{n=1}^{+\infty} \mu_n \langle \cdot , e_n \rangle y_n,
	\end{equation}
	where the series converges in norm. These $\mu_n$ are called \textbf{singular values} of $T$ and are the square root of the eigenvalues of $T^*T$.
\end{cor}

\subsection{Trace-class and Hilbert-Schmidt operators}
In this section we are going to introduce two important classes of operators: the \emph{trace-class} and the \emph{Hilbert-Schmidt} class.

The trace of an operator can be defined as it is for matrices. However, since we are in infinite-dimensional spaces the usual definition has to be handled carefully. Before proceeding, we must define what it means for an operator to be non-negative.
\begin{defi}\label{non-negative operator}
	Let $H$ be a Hilbert space. An operator $T \in \B(H)$ is said \textbf{non-negative} if
	\begin{equation}\label{non-negative operator formula}
		\langle T x, x \rangle \geq 0 \quad \forall x \in H.
	\end{equation}
\end{defi}
Condition \eqref{non-negative operator formula} is sufficient to show that, on Hilbert spaces over the field of complex numbers, non-negative operators are automatically self-adjoint. This result is an immediate corollary of the following form of the polarization identity.
\begin{prop}\label{polarization identity for sesquilinear forms}
	Let $\mathcal{S} : H \times H \rightarrow \C$ be a sesquilinear form over a complex Hilbert space $H$. Then, for every $x,y \in H$:
	\begin{equation}\label{polarization identity for sesquilinear forms formula}
		\mathcal{S}(x,y) = \dfrac{1}{4}\sum_{k=0}^3 i^k \mathcal{S}(x + i^k y, x + i^k y).
	\end{equation}
\end{prop}
\begin{proof}
	The proof follows from a direct computation of the right-hand side:
	\begin{align*}
		&\sum_{k=0}^3 i^k \mathcal{S}(x + i^k y, x + i^k y) = \mathcal{S}(x,x)\sum_{k=0}^{3}i^k + \mathcal{S}(x,y) \sum_{k=0}^{3} i^k i^{-k} + \mathcal{S}(y,x) \sum_{k=0}^{3} i^k i^k\\
		&+ \mathcal{S}(y,y)\sum_{k=0}^{3} i^k i^k i^{-k} = 4 \mathcal{S}(x,y) \implies \mathcal{S}(x,y) = \dfrac{1}{4}\sum_{k=0}^3 i^k \mathcal{S}(x + i^k y, x + i^k y).
	\end{align*}
\end{proof}
\begin{prop}\label{positive operators are self-adjoint}
	Let $T \in \B(H)$ be a non-negative operator over a complex Hilbert space $H$. Then $T$ is self-adjoint.
\end{prop}
\begin{proof}
	First of all we notice that if $T$ is non-negative, the quantity $\langle Tx,x \rangle$ is real, therefore $\langle Tx,x \rangle = \overline{\langle Tx,x \rangle} = \langle x,Tx \rangle$. Letting $\mathcal{T}(\cdot,\cdot) = \langle T \cdot, \cdot \rangle$, and using the polarization identity \eqref{polarization identity for sesquilinear forms formula}:
	\begin{align}
		\overline{\mathcal{T}(y,x)} &= \dfrac{1}{4}\sum_{k=0}^{3}  \overline{i^{k}\mathcal{T}(y + i^k x, y + i^k x)} \overset{T\ \mathrm{positive}}{=} \dfrac{1}{4}\sum_{k=0}^{3} i^{-k} \mathcal{T}(y + i^k x, y + i^k x) \nonumber\\
									&= \dfrac{1}{4}\sum_{k=0}^{3} i^{-k} \mathcal{T}\left(i^k(x + i^{-k}y), i^k(x + i^{-k}y)\right) \nonumber\\
									&= \dfrac{1}{4}\sum_{k=0}^{3} i^{-k} \mathcal{T}(x + i^{-k}, x + i^{-k}y) = \mathcal{T}(x,y) \label{polarization identity}.
	\end{align}
\end{proof}
Non-negative operators are somewhat ``special'', since their behaviour resembles, in some sense, the one of complex numbers. In particular, one can define the \emph{square root} of non-negative operator. Before we do this, we need a preceding lemma.
\begin{lem}\label{power series of square root lemma}
	The power series of $\sqrt{1-z}$ about 0 converges absolutely for all complex numbers such that $|z| \leq 1$.
\end{lem}
\begin{proof}
	The power series of $\sqrt{1-z}$ about the origin is given by
	\begin{equation*}
		\sqrt{1-z} = \sum_{n=0}^{+\infty}c_n z^n = 1 + \sum_{n=1}^{+\infty} (-1)^n\binom{1/2}{n} z^n,
	\end{equation*}
	where the binomial $\binom{r}{n}$ is defined by $\frac{r(r-1)\cdots(r-n+1)}{n!}$ for every $r \in \R$ and every $n \in \N$, while if $n=0$ we have $\binom{r}{0}=1$. Since $\sqrt{1-z}$ is analytic for $|z|<1$, the series here converges absolutely, so now we have to consider the case $|z|=1$. We notice that $c_n < 0$ for every $n \geq 1$ because when $n$ is even $\binom{1/2}{n}$ is negative, while if $n$ is odd $\binom{1/2}{n}$ is positive, therefore
	\begin{align*}
		1-\sqrt{1-x} = \sum_{n=1}^{+\infty} (-c_n) x^n
	\end{align*}
	is a positive series. Using this fact, given $N \in \N$, we have
	\begin{align*}
		\sum_{n=0}^{N} |c_n| &= 1 + \sum_{n=1}^{N} (-c_n) = 1 + \lim_{x \rightarrow 1^-} \sum_{n=1}^{N} (-c_n)x^n  \\
							 &\overset{\mathrm{positive\ series}}{\leq} 1 + \lim_{x \rightarrow 1^-} \sum_{n=1}^{+\infty} (-c_n)x^n = 1 + \lim_{x \rightarrow 1^-}(1-\sqrt{1-x}) = 2.
	\end{align*}
	Since this holds for every $N$, taking the limit $N \rightarrow +\infty$ we obtain $\sum_{n=1}^{+\infty} |c_n| < +\infty$, which means exactly that the power series is absolutely convergent.
\end{proof}
\begin{teo}\label{existence of square root}
	Let $T \in \B(H)$ be a non-negative operator. Then, there exist a unique non-negative operator $S \in \B(H)$ such that $S^2 = T$. Moreover, $S$ commutes with all bounded operators commuting with $T$.	We call $S$ the \textbf{square root} of $T$ and we denote it by $S = \sqrt{T}$.
\end{teo}
\begin{proof}
	If $T=0$ we let $\sqrt{T}=0$, otherwise we define $B = I - \|T\|^{-1}T$, where $I$ is the identity operator.
	Since $T$ is non-negative, for every $x \in H$ such that $\|x\| = 1$, we have
	\begin{align*}
		\langle Bx,x \rangle = \langle (I-\|T\|^{-1}T)x, x\rangle = \|x\|^2 - \|T\|^{-1}\langle Tx,x \rangle \leq \|x\|^2 = 1,
	\end{align*}
	which implies, using polarization identity, that $\|B\| \leq 1$. Thanks to Lemma \ref{power series of square root lemma}, this means that the series $\sum_{n=0}^{+\infty}c_n B^n$ is absolutely convergent, therefore convergent, in $\B(H)$ to an operator we indicate with $B_{1/2}$. We define
	\begin{equation}\label{square root of an operator}
		S = \|T\|^{1/2} B_{1/2}
	\end{equation}
	and we want to show that $S$ is non-negative and satisfies $S^2 = T$. We start proving that $B_{1/2}$, hence $S$, is non-negative. Taking $x \in H$, we have
	\begin{align*}
		\langle B_{1/2}x,x \rangle &= \langle (I + \sum_{n=1}^{+\infty} c_n B^n),x \rangle = \|x\|^2 + \sum_{n=1}^{+\infty} c_n\langle B^n x,x \rangle \geq \|x\|^2 + \sum_{n=1}^{+\infty} c_n \|B\|^n \|x\|^2 \\
								   &\geq \|x\|^2 + \sum_{n=1}^{+\infty} c_n \|x\|^2 = 0,
	\end{align*}
	where we used the fact that $c_n$ are negative, $\|B\| \leq 1$ and that $1 + \sum_{n=1}^{\infty}c_n = \sqrt{1-x}\rvert_{x=1} = 0$.
	
	Now we shall prove that $S^2 = T$, which means $\|T\|(B_{1/2})^2 = T$:
	\begin{align*}
		(B_{1/2})^2 = \left(\sum_{n=0}^{+\infty}c_n B^n\right) \left(\sum_{m=0}^{+\infty}c_m B^m\right) = \sum_{n=0}^{+\infty} \left(\sum_{m=0}^{n} c_m c_{n-m}\right) B^n = \sum_{n=0}^{+\infty} d_n B^n,
	\end{align*}
	where the rearrangement is justified since all series are absolutely converging. In order to compute $d_n = \sum_{m=0}^{n} c_m c_{n-m}$, we notice the following:
	\begin{align*}
			1-x &= \sqrt{1-x} \sqrt{1-x} = \left(\sum_{n=0}^{+\infty}c_n x^n\right) \left(\sum_{m=0}^{+\infty}c_m x^m\right) = \sum_{n=0}^{+\infty} \left(\sum_{m=0}^{n} c_m c_{n-m}\right) x^n ,
	\end{align*}
	which implies that $d_0=1$, $d_1 = -1$ and $d_n = 0$ for $n \geq 2$, therefore $B_{1/2} = I - B$ and, in the end:
	\begin{align*}
		S^2 = \|T\|(B_{1/2})^2 = \|T\|(I - B) = \|T\|(I - I + \|T\|^{-1}T) = T.
	\end{align*}
	Lastly, since the series that defines $B_{1/2}$, hence $S$, is absolutely convergent, it commutes with every bounded operator commuting with $T$.

	Up to now we only proved the existence of a square root for $T$, in particular the one given by the expression \eqref{square root of an operator}. To prove uniqueness of the square root we start supposing $S_0$ is another non-negative operator in $\B(X)$ such that $S_0^2 = T$. We notice that $S_0$ commutes with $T$, indeed $S_0 T = S_0 S_0^2 = S_0^2 S_0 = T S_0$. Therefore, $S_0$ commutes also with $S$, thus we have:
	\begin{align*}
		(S-S_0)^2S + (S-S_0)^2S_0 &= (S-S_0)[(S-S_0)S + (S-S_0)S_0]\\
								  &= (S-S_0)(S^2 - S_0S + SS_0 -S_0^2)\\
								  &= (S-S_0)(S^2 - S_0^2) = (S-S_0)(T-T)=0.
	\end{align*}
	But $(S-S_0)^2 S$ and $(S-S_0)^2 S_0$ are non-negative operator, so they must vanish, in particular also their difference $(S-S_0)^2 S - (S-S_0)^ S_0 = (S-S_0)^3$ is zero. This implies $(S-S_0)^4 = (S-S_0) (S-S_0)^3 = 0$, but since both $S$ and $S_0$ are self-adjoint, for every $x \in H$ we have:
	\begin{align*}
		0 = \langle (S-S_0)^4 x,x \rangle = \langle (S-S_0)^2 u, (S-S_0)^2 u \rangle =  \|(S-S_0)^2 x\|^2 \implies (S-S_0)^2 = 0
	\end{align*}
	and with the same argument we conclude that also $S-S_0 = 0$.
\end{proof}
\begin{remark}
	From expression \eqref{square root of an operator} it is clear that, if $T$ is compact then also $\sqrt{T}$ is compact, since it is a limit of compact operators.
\end{remark}

Now that we have established the notion of square root of an operator, we will show how to decompose an operator $T$ into a positive operator and a \emph{partial isometry}, which will be defined later. This decomposition is called \emph{polar decomposition} of $T$.
\begin{defi}\label{absolute value of an operator definition}
	Given an operator $T \in \B(H)$ we define its \textbf{absolute value} as
	\begin{equation}\label{absolute value of an operator formula}
		|T| \coloneqq \sqrt{T^*T}.
	\end{equation}
\end{defi}
\begin{defi}\label{partial isometry}
	A linear operator $U \in \B(H)$ is a \textbf{partial isometry} if it is an isometry over $(\mathrm{Ker}\,U)^{\perp}$, i.e. $\|Ux\| = \|x\|$ for every $x \in (\mathrm{Ker}\,U)^{\perp}$.
\end{defi}
\begin{prop}\label{U^* is a partial isometry}
	Let $U \in \B(H)$ be a partial isometry. Then also $U^*$ is a partial isometry.
\end{prop}
\begin{proof}
	Since $U$ is a partial isometry, from polarization identity it follows that $\langle Ux,Uy \rangle = \langle x,y \rangle$ for every $x,y \in (\mathrm{Ker}(U))^{\perp}$. Clearly the same equality holds if $x \in (\mathrm{Ker}(U))^{\perp}$ while $y \in \mathrm{Ker}(U)$, so $U^*U$ is the identity over $(\mathrm{Ker}(U))^{\perp}$. This implies that $U^*$ is an isometry on $\overline{\mathrm{Im}(U)} = (\mathrm{Ker}(U^*))^{\perp}$, hence it is a partial isometry too.
\end{proof}
\begin{teo}\label{polar decomposition theorem}
	Given $T \in \B(H)$ there exist unique a partial isometry $U$ such that
	\begin{equation}\label{polar decomposition formula}
		T = U|T|,
	\end{equation}
	which is uniquely determined by the condition $\mathrm{Ker}\,U = \mathrm{Ker}\,T$.
\end{teo}
\begin{proof}
	We start defining $\tilde{U} : \mathrm{Im}|T| \rightarrow \mathrm{Im}T$. Every $x \in \mathrm{Im}|T|$ can be written as $x = |T|y$ for some $y \in H$, so we can define $\tilde{U}(x) = \tilde{U}(|T|y) \coloneqq T y$. We notice that	
	\begin{align*}
		\|x\|^2 &= \||T| y\|^2 = \langle |T|y, |T|y \rangle  = \langle |T|^2 y, y \rangle = \langle T^*T y, y \rangle = \|T y\|^2 = \|\tilde{U}x\|.
	\end{align*}
	This computation ensures us that the definition of $\tilde{U}$ is consistent (if $x = |T|y_1 = |T|y_2$ for some $y_1,y_2 \in H$, then $|T|(y_1 - y_2)=0$, but $\| |T|(y_1 - y_2)\| = \| T(y_1 - y_2)\|$, therefore $Ty_1 = Ty_2$) and it implies that $\tilde{U}$ is an isometry over $\mathrm{Im}(|T|)$, thus it can be uniquely extended to an isometry of $\overline{\mathrm{Im}(|T|)}$ over $\overline{\mathrm{Im}T}$. The definition of the map $U$ is straightforward extending $\tilde{U}$ to all $H$ defining it 0 on $(\mathrm{Im}(|T|))^{\perp}$. Since $|T|$ is self-adjoint $(\mathrm{Im}(|T|))^{\perp} = \mathrm{Ker}(|T|)$. Furthermore, since $\| |T|y\| = \| Ty\|$, $|T|y=0$ if and only if $Ty=0$, therefore $\mathrm{Ker}|T| = \mathrm{Ker}T$ which implies, in the end, that $\mathrm{Ker}U = \mathrm{Ker}T$.
	
	Lastly we shall prove that $U$ is unique. Suppose $V$ is another partial isometry such that $T = V|T|$ and $\mathrm{Ker}V = \mathrm{Ker}T$. First condition implies that $T = U|T| = V|T|$, so $U$ and $V$ coincide over $\mathrm{Im}|T|$ (and by continuity over its closure), while second condition implies that $\mathrm{Ker}T = \mathrm{Ker}U = \mathrm{Ker}T$, so $U$ and $V$ coincide over $\mathrm{Ker}|T| = (\mathrm{Im}|T|)^{\perp}$, therefore $U$ and $V$ coincide over $H = \overline{\mathrm{Im}|T|} \oplus \mathrm{Ker}|T|$.
\end{proof}

\begin{defi}\label{trace def}
	Let $H$ be a separable Hilbert space with orthonormal basis $\{e_n\}_{n \in \N}$. Given $T \in \B(H)$ a non-negative operator we define the \textbf{trace} of $T$ as
	\begin{equation}\label{trace expression}
		\mathrm{tr}(T) = \sum_{n=1}^{\infty} \langle T e_n, e_n \rangle.
	\end{equation}
\end{defi}
Since $T$ is non-negative, every term of the sum in \eqref{trace expression} is non-negative, so the series is either convergent or divergent. Nevertheless, in principle, it could depend on the basis $\{e_n\}_{n \in \N}$. The following theorem states that the definition is indeed well-posed, namely that the trace does not depend on the basis.
\begin{prop}\label{trace is well-defined}
	The definition of $\mathrm{tr}$ given by \eqref{trace expression} is independent of the basis.
\end{prop}
\begin{proof}
	Let $T \in \B(H)$ be a non-negative operator and $\{e_n\}_{n \in \N}$ and $\{f_n\}_{n \in \N}$ be two orthonormal basis of $H$. We have
	\begin{align*}
		\sum_{n=1}^{+\infty} \langle Te_n,e_n \rangle &= \sum_{n=1}^{+\infty} \|\sqrt{T}e_n\|^2 = \sum_{n=1}^{+\infty} \left(\sum_{m=1}^{+\infty} |\langle \sqrt{T}e_n, f_m \rangle|^2 \right)\\
															&= \sum_{m=1}^{+\infty} \left(\sum_{n=1}^{+\infty} |\langle \sqrt{T}f_m, e_n \rangle|^2 \right) = \sum_{m=1}^{+\infty} \|\sqrt{T}f_m\|^2 \\
															&= \sum_{m=1}^{+\infty} \langle T f_m, f_m \rangle,
	\end{align*}
	where we used the fact that $\sqrt{T}$ is self-adjoint, while the exchange of series is allowed because all terms are non-negative.
\end{proof}
\begin{defi}\label{Hilbert-Schmidt operator def}
	An operator $T \in \B(H)$ is called \textbf{Hilbert-Schmidt} if and only if $\mathrm{tr}(T^*T) < \infty$. We define the \textbf{Hilbert-Schmidt norm} of an operator as $\|T\|_{\mathrm{HS}} = \sqrt{\mathrm{tr}(T^*T)}$.
\end{defi}
From the definition of the trace we can see that, given an orthonormal basis $\{e_n\}_{n=1}^{+\infty}$,
\begin{equation}\label{Hilbert-Schmidt norm expression}
	\|T\|_{\mathrm{HS}}^2 = \mathrm{tr}(T^*T) = \sum_{n=1}^{+\infty} \langle T^*Te_n,e_n \rangle = \sum_{n=1}^{+\infty} \langle Te_n,Te_n \rangle = \sum_{n=1}^{+\infty} \|Te_n\|^2,
\end{equation}
so we can say, in an equivalent way, that an operator is Hilbert-Schmidt if and only if $\sum_{n=1}^{+\infty} \|Te_n\|^2 < +\infty$. Thanks to Proposition \ref{trace is well-defined} we immediately see that the Hilbert-Schmidt norm is independent on the choice of the basis.

We are now going to show some properties of Hilbert-Schmidt and trace-class operators.
\begin{prop}\label{|T| and T^* are Hilbert-Schmidt}
	Let $T \in \B(H)$ be a Hilbert-Schmidt operator. Then, also $|T|$ and $T^*$ are Hilbert-Schmidt operators and 
	\begin{equation}\label{Hilbert-Schmidt norm of |T| and T^*}
		\| |T| \|_{\mathrm{HS}} = \| T^* \|_{\mathrm{HS}} = \| T \|_{\mathrm{HS}}.
	\end{equation}
\end{prop}
\begin{proof}
	We already know that, for every $x \in H$, $\|Tx\|^2 = \|\,|T| x\|^2$, therefore from \eqref{Hilbert-Schmidt norm expression} we immediately see that $ \| |T| \|_{\mathrm{HS}} = \| T \|_{\mathrm{HS}}$.
	
	Consider now an orthonormal basis $\{e_n\}_{n \in \N}$ of $H$. From \eqref{Hilbert-Schmidt norm expression} we have:
	\begin{align*}
		\|T\|_{\mathrm{HS}}^2 &= \sum_{n=1}^{+\infty} \|Te_n\|^2 = \sum_{n=1}^{+\infty} \left(\sum_{m=1}^{+\infty} |\langle Te_n,e_m \rangle|^2\right) = \sum_{m=1}^{+\infty} \left(\sum_{n=1}^{+\infty} |\langle e_n, T^*e_m \rangle|^2\right)\\
							&= \sum_{m=1}^{+\infty} \|T^* e_m\|^2 = \|T^*\|_{\mathrm{HS}}^2,
	\end{align*}
	where exchange of series is allowed since all terms are non-negative.
\end{proof}
\begin{teo}\label{Hilbert-Schmidt operators are compact and bounded}
	Let $T \in \B(H)$ be a Hilbert-Schmidt operator. Then, $\|T\| \leq \|T\|_{\mathrm{HS}}$ and $T$ is compact. Moreover, a compact operator $T$ is Hilbert-Schmidt if and only if $\sum_{n=1}^{+\infty} \mu_n^2$, where $\{\mu_n\}_{n=1}^{+\infty}$ are the singular values of $T$.
\end{teo}
\begin{proof}
	Let $\{e_n\}_{n \in \N}$ be an orthonormal basis of $H$ and let $x \in H$. We have:
	\begin{align*}
		\|Tx\|^2 &= \sum_{n=1}^{+\infty} |\langle Tx,e_n \rangle|^2 = \sum_{n=1}^{+\infty} |\langle x,T^*e_n \rangle|^2 \leq \|x\|^2 \sum_{n=1}^{+\infty} \|T^*e_n\|^2\\
				 &= \|x\|^2 \|T^*\|_{\mathrm{HS}}^2 \overset{\eqref{Hilbert-Schmidt norm of |T| and T^*}}{=} \|x\|^2 \|T\|_{\mathrm{HS}}^2.
	\end{align*}
	Taking the supremum over all $x \in H$ gives us that $\|T\| \leq \|T\|_{\mathrm{HS}}$.
	
	To prove that $T$ is compact, consider the following sequence of finite-rank operators $T_N = \sum_{n=1}^{N} \langle \cdot , e_n \rangle Te_n$. We have that $T_N \rightarrow T$ in $\B(H)$, indeed:
	\begin{align*}
		\|T-T_N\| \leq \|T-T_N\|_{\mathrm{HS}}^2 = \sum_{n=N+1}^{+\infty} \|T e_n\|^2 \rightarrow 0 \quad \mathrm{as\ } N \rightarrow +\infty,
	\end{align*}
	therefore, from Corollary \ref{limit of finite rank operator is compact}, $T$ is compact.
	
	Lastly, if $T$ is a compact operator then also $T^*T$ is compact, as well as self-adjoint, hence we can consider as orthonormal basis of $H$ the one given by its eigenvectors $\{e_n\}_{n \in \N}$. Recalling Corollary \ref{canonical form for compact operators corollary}, we know that $\mu_n^2$ are exactly the eigenvalues of $T^*T$.  For such basis we have:
	\begin{align*}
		\|T\|_{\mathrm{HS}}^2 &= \sum_{n=1}^{+\infty} \|Te_n\|^2 = \sum_{n=1}^{+\infty} \langle Te_n,Te_n \rangle = \sum_{n=1}^{+\infty} \langle T^*T e_n,e_n \rangle = \sum_{n=1}^{+\infty} \mu_n^2.
	\end{align*}
\end{proof}
\begin{prop}\label{Hilbert-Schmidt operator are an ideal}
	Let $T \in \B(H)$ be a Hilbert-Schmidt operator and $S \in \B(H)$. Then $TS$ and $ST$ are both Hilbert-Schmidt operators.
\end{prop}
\begin{proof}
	Given an orthonormal basis $\{e_n\}_{n \in \N}$ of $H$ we have:
	\begin{align*}
		\|ST\|_{\mathrm{HS}}^2 = \sum_{n=1}^{+\infty} \|ST e_n\|^2 \leq \|S\|^2 \sum_{n=1}^{+\infty} \|T e_n\|^2 = \|S\|^2 \|T\|_{\mathrm{HS}}^2,
	\end{align*}
	so $ST$ is Hilbert-Schmidt. Moreover, from Proposition \ref{Hilbert-Schmidt norm of |T| and T^*} follows:
	\begin{align*}
		\| TS \|_{\mathrm{HS}} = \| (TS)^* \|_{\mathrm{HS}} = \| S^* T^* \|_{\mathrm{HS}} \leq \|S^*\| \|  T^* \|_{\mathrm{HS}} = \|S \| \| T \|_{\mathrm{HS}}.
	\end{align*}
\end{proof}
We can now turn back to trace-class operator. It is evident that trace-class operators and Hilbert-Schmidt operators are strictly related. Indeed, as seen in the proof of Proposition \ref{trace is well-defined}, we have
\begin{equation*}
	\mathrm{tr}|T| = \sum_{n=1}^{+\infty} \langle |T|e_n, e_n \rangle = \sum_{n=1}^{+\infty} \|\sqrt{|T|}e_n\|^2 = \| T \|_{\mathrm{HS}}^2,
\end{equation*}
so an operator is trace-class if and only if $\sqrt{|T|}$ is Hilbert-Schmidt. By virtue of this link, we can exploit properties of Hilbert-Schmidt operators in order to obtain information about trace-class operators. First of all, we are going to prove that expression \eqref{trace expression} is well-defined for every trace-class operator and not only for non-negative ones.
\begin{teo}
	Let $T \in \B(H)$ be a trace-class operator and $\{e_n\}_{n \in \N}$ an orthonormal basis of $H$. Then $\sum_{n=1}^{+\infty} \langle Te_n,e_n \rangle$ converges absolutely and the limit is independent of the basis.
\end{teo}
\begin{proof}
	We start proving that the series converges absolutely. Letting $T = U |T|$ be the polar decomposition of $T$, we want to show that:
	\begin{equation}\label{absoulte convergence trace intermediate}
		\sum_{n=1}^{+\infty} |\langle Te_n,e_n  \rangle| = \sum_{n=1}^{+\infty} |\langle U\sqrt{|T|}\sqrt{|T|}e_n,e_n \rangle| = \sum_{n=1}^{+\infty} |\langle \sqrt{|T|}e_n,\sqrt{|T|}U^*e_n\rangle| < +\infty.
	\end{equation}
	From Cauchy-Schwarz' inequality, for every term we have that $|\langle \sqrt{|T|}e_n,\sqrt{|T|}U^*e_n\rangle| \leq \|\sqrt{|T|}e_n \|\,\| \sqrt{|T|}U^*e_n\|$.
	Since $T$ is trace-class, $\sqrt{|T|}$ is Hilbert-Schmidt and, thanks to Proposition \ref{Hilbert-Schmidt operator are an ideal}, also $\sqrt{|T|}U^*$ is a Hilbert-Schmidt operator, therefore both $\{\|\sqrt{|T|}e_n\|\}_{n \in \N}$ and $\{\|\sqrt{|T|}U^* e_n\|\}_{n \in \N}$ are in $\ell^2(\N)$. This allows us to use the Cauchy-Schwarz inequality in the last expression of \eqref{absoulte convergence trace intermediate}, thus obtaining:
	\begin{align*}
		\sum_{n=1}^{+\infty} |\langle Te_n,e_n  \rangle| &= \sum_{n=1}^{+\infty} |\langle \sqrt{|T|}e_n,\sqrt{|T|}U^*e_n\rangle| \leq \sum_{n=1}^{+\infty} \| \sqrt{|T|}e_n\|\, \|\sqrt{|T|}U^*e_n \| \\
		&\overset{\mathrm{C-S}}{\leq} \left(\sum_{n=1}^{+\infty} \| \sqrt{|T|}e_n\|^2 \right)^{1/2} \left(\sum_{n=1}^{+\infty} \| \sqrt{|T|}U^*e_n\|^2 \right)^{1/2} \leq \|\sqrt{|T|}\|_{\mathrm{HS}}^2.
	\end{align*}
	The proof of the independence of the basis is exactly the same as the one for Proposition \ref{trace is well-defined}, because now the exchange of series is allowed since the series is convergent.
\end{proof}
Another immediate corollary of Proposition \ref{Hilbert-Schmidt operator are an ideal} is the following proposition
\begin{prop}\label{trace-class operators are Hilbert-Schmidt}
	Let $T \in \B(H)$ be a trace-class operator. Then $T$ is also Hilbert-Schmidt.
\end{prop}
\begin{proof}
	Let $T = U |T|$ be the polar decomposition of $T$. Since $T$ is trace-class $\sqrt{|T|}$ is a Hilbert-Schmidt operator. Therefore, from Proposition \ref{Hilbert-Schmidt operator are an ideal} follows that $T = U \sqrt{|T|} \sqrt{|T|}$ is a Hilbert-Schmidt operator.
\end{proof}

\begin{teo}\label{trace-class operators are compact and bounded}
	Let $T \in \B(H)$. Then $T$ is trace-class if and only if it is compact and $\sum_{n=1}^{+\infty} \mu_n$, where $\{\mu_n\}_{n \in \N}$ are the singular values of $T$. Moreover, if $T$ is trace-class then $\mathrm{tr}|T| = \sum_{n=1}^{+\infty} \mu_n$ and additionally, if it is also self-adjoint, $\mathrm{tr}T = \sum_{n=1}^{+\infty} \lambda_n$ where $\{\lambda_n\}_{n \in \N}$. Finally, we have $\|T\| \leq \mathrm{tr}|T|$.
\end{teo}
\begin{proof}
	Let $T = U|T|$ be the polar decomposition of $T$. If $T$ is trace-class $\sqrt{|T|}$ is Hilbert-Schmidt, but from Theorem \ref{Hilbert-Schmidt operators are compact and bounded} we have that $\sqrt{|T|}$ is compact, therefore also $T = U \sqrt{|T|}\sqrt{|T|}$ and $|T|$ are compact. Not only $|T|$ is compact, but it is also self-adjoint and its eigenvalues are exactly $\{\mu_n\}_{n \in \N}$. Letting $\{e_n\}_{n \in \N}$ be an orthonormal basis made up of eigenvectors of $|T|$, we have
	\begin{align*}
		\mathrm{tr}|T| = \sum_{n=1}^{+\infty} \langle |T|e_n,e_n \rangle = \sum_{n=1}^{+\infty} \mu_n.
	\end{align*}
	Conversely, if $T$ is a compact operator the previous formula still holds and shows that $T$ is also trace-class. With the same reasoning, if $T$ is self-adjoint and trace-class, we can write the trace with respect to the basis made up its eigenvectors thus obtaining $\mathrm{tr}T = \sum_{n=1}^{+\infty} \lambda_n$.
	
	For the last part of the Theorem, we notice that $|T|$ is a compact self-adjoint non-negative operator, therefore, assuming its eigenvalues are decreasingly ordered, from Corollary \ref{norm is greatest eigenvalue} we have that $\| |T| \| = \mu_1$, hence:
	\begin{equation*}
		\| T \| = \| U |T| \| \leq \| |T| \| = \mu_1 \leq \sum_{n=1}^{+\infty} \mu_n = \mathrm{tr}|T|. \qedhere
	\end{equation*} 	
\end{proof}
In the special case $H = L^2(\R^d)$ we are able to give a characterization of Hilbert-Schmidt operators.
\begin{teo}\label{representation of Hilbert-Schmidt integral operator}
	An operator $T \in \B(L^2(\R^d))$ is Hilbert-Schmidt if only if there exists a function $K_T \in L^2(\R^d \times \R^d)$, called integral kernel, such that
	\begin{equation}\label{Hilbert-Schmidt integral operator}
		(Tf)(x) = \int_{\R^d} K_T(x,y)f(y)dy \quad \forall f \in L^2(\R^d).
	\end{equation}
	Moreover $\|T\|_{\mathrm{HS}} = \|K_T\|_2$.
\end{teo}
\begin{proof}
	We start proving that, given $K \in L^2(\R^{2d})$, the corresponding integral operator defined by \eqref{Hilbert-Schmidt integral operator} is continuous. Denoting with $T_K$ such operator, for every $f \in L^2(\R^d)$, we have:
	\begin{align*}
		\| T_K f \|_{L^2(\R^d)}^2 &= \int_{\R^{d}} |T_K f(x)|^2 dx = \int_{\R^d} \Big|\int_{\R^{d}} K(x,y) f(y) dy\Big|^2 dx\\
						&\leq \int_{\R^d} \left(\int_{\R^{d}} |K(x,y)| |f(y)| dy\right)^2 dx.
	\end{align*}
	From Fubini's theorem we have that $|K(x,\cdot)| \in L^2(\R^d)$ for almost every $x \in \R^d$. Therefore, we can apply Cauchy-Schwarz' inequality in the inner integral, thus obtaining:
	\begin{equation}\label{representation of Hilbert-Schmidt integral operator bound 1}
		\| T_K f \|_{L^2(\R^d)}^2 \leq \|f\|_{L^2(\R^d)}^2 \int_{\R^d} \int_{\R^{d}} |K(x,y)|^2 dy dx = \| K \|_{L^2(\R^{2d})}^2 \|f\|_{L^2(\R^d)},
	\end{equation}
	which shows that $T_K$ is a bounded operator. Moreover, $T_K$ is clearly linear, therefore $T_K \in \B(L^2(\R^d))$. 
	
	We can now suppose to have a Hilbert-Schmidt operator $T \in \B(H)$ and an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$. We want to show that \eqref{Hilbert-Schmidt integral operator} holds with the following kernel:
	\begin{equation}\label{integral kernel formula}
		K_T(x,y) = \sum_{(m,n) \in \N^2} \langle Te_n,e_m \rangle_{L^2(\R^d)} e_m(x)\overline{e_n(y)}.
	\end{equation}
	We point out that $\{e_m \otimes \overline{e_n}\}_{(m,n) \in \N^2}$ is an orthonormal basis of $L^2(\R^{2d})$. Moreover, from \eqref{representation of Hilbert-Schmidt integral operator bound 1}, it follows that, for every $f \in L^2(\R^d)$, the operator $K \in L^2(\R^{2d}) \mapsto A_f(K) = T_K f \in L^2(\R^d)$ is linear and continuous. Thus, for every $f \in L^2(\R^d)$ we have:
	\begin{align*}
		\int_{\R^d} K_T(x,y)f(y)dy &= A_f(K_T)(x) = A_f \Big( \sum_{(m,n) \in \N^2} \langle Te_n,e_m \rangle e_m \otimes\overline{e_n} \Big)(x) \\
								   &\overset{\ref{exchange of series and operator}}{=} \sum_{(m,n) \in \N^2} A_f \big(\langle Te_n,e_m \rangle e_m \otimes\overline{e_n}\big)(x) \\
								   &\overset{\ref{exchange order double series}}{=} \sum_{m=1}^{+\infty} \sum_{n=1}^{+\infty} \langle Te_n,e_m \rangle A_f(e_m \otimes e_n)(x) \\
								   &= \sum_{m=1}^{+\infty} \sum_{n=1}^{+\infty} \langle Te_n,e_m \rangle \int_{\R^d} e_m(x)\overline{e_n(y)}f(y) dy \\
								   &= \sum_{m=1}^{+\infty} \sum_{n=1}^{+\infty} \langle T(\langle f,e_n\rangle e_n), e_m \rangle e_m(x) \\
								   &= \sum_{m=1}^{+\infty} \langle Tf, e_m \rangle e_m(x) = (Tf)(x), 
	\end{align*} 
	where the use of \ref{exchange of series and operator} is justified because, $\sum_{(m,n) \in \N^2} \langle Te_n,e_m \rangle e_m \otimes\overline{e_n}$ converges unconditional, while the use of \ref{exchange order double series} is justified because $A_f$ is continuous, thus \eqref{exchange of series and operator} implies that $\sum_{(m,n) \in \N^2} A_f \big(\langle Te_n,e_m \rangle e_m \otimes\overline{e_n}\big)$ converges unconditionally. Finally, we have:
	\begin{equation}\label{equivalence norm L2 and Hilbert-Schmidt norm}
		\begin{aligned}
			\|K_T\|_{L^2(\R^{2d})}^2 &= \sum_{(n,m) \in \N^2} |\langle Te_n,e_m \rangle|^2 = \sum_{n=1}^{+\infty} \sum_{m=1}^{+\infty} |\langle Te_n,e_m \rangle|^2 \\
			&= \sum_{n=1}^{+\infty} \|T e_n\|^2_{L^2(\R^d)} = \|T\|_{\mathrm{HS}}^2.
		\end{aligned}		
	\end{equation}

	Conversely, if we have $K \in L^2(\R^{2d})$ and we define $T_K$ by \eqref{Hilbert-Schmidt integral operator}, we have to show that this is a Hilbert-Schmidt operator. We already proved that $T_K \in B(H)$. The proof that it is also Hilbert-Schmidt is straightforward since \eqref{equivalence norm L2 and Hilbert-Schmidt norm} still holds.
\end{proof}
In light of this theorem, operators defined by \eqref{Hilbert-Schmidt integral operator} are called \emph{Hilbert-Schmidt integral operators}.

\begin{prop}\label{condition integral operator self-adjoint}
	Let $T$ be a Hilbert-Schmidt integral operator over $L^2(\R^d)$ with integral kernel $K \in L^2(\R^{2d})$. Then, its adjoint operator is given by
	\begin{equation}\label{Hilbert-Schmidt integral operator adjoint}
		T^*f(x) = \int_{\R^d} \overline{K(y,x)} f(y)dy.
	\end{equation}
	Therefore $T$ is self-adjoint if and only if $K(x,y) = \overline{K(y,x)}$.
\end{prop}
\begin{proof}
	Let $f,g \in L^2(\R^d)$. We start showing that $K(x,y)f(y)\overline{g(x)} \in L^1(\R^{2d})$:
	\begin{align*}
		&\int_{\R^{2d}} |K(x,y)| |f(y)| |g(x)| dxdy \overset{\mathrm{Tonelli}}{=} \int_{\R^d} \left(\int_{\R^d} |K(x,y)||f(y)| dy\right) |g(x)| dx \\
		&= \int_{\R^d} (T_{|K|} |f|)(y) |g(y)| \overset{\mathrm{C-S}}{\leq} \|T_{|K|}\|_2 \|g\|_2 \leq \|K\|_2 \| f \|_2 \|g\|_2 < +\infty,
	\end{align*}
	where $T_{|K|}$ denotes the Hilbert-Schmidt integral operator with kernel $|K|$ and $\|T_{|K|} f\|_2 \leq \|K\|_2 \| f \|_2$ because $T_{|K|}$ is Hilbert-Schmidt, therefore $\|T_{|K|}\| \leq \|T_{|K|}\|_{\mathrm{HS}} = \| K \|_2$. We are now in the position to use Fubini's theorem:
	\begin{align*}
		\langle Tf,g \rangle &= \int_{\R^d} \left(\int_{\R^d} K(x,y)f(y)dy\right)\overline{g(x)}dx = \int_{\R^d} \left(\int_{\R^d} K(x,y)\overline{g(x)}dx\right) f(y)dy \\
							 &= \int_{\R^d} \overline{\left(\int_{\R^d} \overline{K(x,y)} g(x)dx\right)} f(y)dy = \langle f,T^*g \rangle,
	\end{align*}
	where the expression of $T^*$ is exactly the one in \eqref{Hilbert-Schmidt integral operator adjoint}.
\end{proof}


\section{Fourier Transform and its properties}\label{Fourier transform and its properties section}
In this section we introduce the Fourier transform with its elementary properties, its relation with some fundamental operators in time-frequency analysis and with the convolution product.
\begin{defi}\label{Fourier transform def}
	Let $f \in L^1(\R^d)$. We define the \textbf{Fourier transform} of $f$ the function as
	\begin{equation}\label{Fourier transform formula}
		\F f(\omega) = \hat{f}(\omega) \coloneqq \int_{\R^d} e^{-2 \pi i \omega \cdot t} f(t) dt.
	\end{equation}
\end{defi}
It is straightforward to see that the definition is well-posed and that $\F f \in L^{\infty}(\R^d)$ with $\|\F f\|_{\infty} \leq \| f \|_1$. Therefore, $\F$ can be seen as a linear operator between $L^1(\R^d)$ and $L^{\infty}(\R^d)$ with $\| \F \| \leq 1$. Actually, taking $f \geq 0$ a.e.,  we have that $\hat{f}(0) = \| f \|_1$, which gives us the equality.

The Fourier transform of an $L^1(\R^d)$ is not only bounded, as stated by the \emph{Riemann-Lebesgue lemma}.
\begin{teo}[Riemann-Lebesgue lemma]\label{Riemann-Lebesgue lemma}
	Let $f \in L^1(\R^d)$. Then $\hat{f} \in C_0(\R^d) = \{f : \R^d \rightarrow \C \text{ continuous such that } \lim_{|t| \rightarrow \infty} |f(t)|=0\}$.
\end{teo}
\begin{defi}\label{inverse Fourier transform def}
	Let $f \in L^1(\R^d)$. We define the \textbf{inverse Fourier transform} of the function $f$ as
	\begin{equation}\label{inverse Fourier transform formula}
		\F^{-1} f(t) = \check{f}(t) \coloneqq \int_{\R^d} e^{2 \pi i \omega \cdot t} f(\omega) d\omega.
	\end{equation}
\end{defi}
The inverse Fourier transform is denoted with $\F^{-1}$ because it is actually the inverse operator of the Fourier transform as stated by the \emph{inversion theorem}.
\begin{teo}[Inversion theorem]\label{inversion theorem}
	Let $f \in L^1(\R^d)$ and suppose that also $\hat{f} \in L^1(\R^d)$. Then
	\begin{equation*}
		f(t) = \F^{-1} \circ \F f(t) = \int_{\R^d} \hat{f}(\omega) e^{2 \pi i \omega \cdot t}d\omega.
	\end{equation*}
\end{teo}
If $f$ is in $L^2(\R^d)$, the integral in \eqref{Fourier transform formula} in general will not converge. Nevertheless, we can define the Fourier transform of an $L^2$ function through a density argument. For example, one can use $L^1(\R^d) \cap L^2(\R^d)$, which is a dense subspace of $L^2(\R^d)$. On this space one can show that the Fourier transform is an isometry with respect to the $L^2$ norm and therefore it extends to an isometry on the whole $L^2(\R^d)$. This is stated by the \emph{Plancherel theorem}.
\begin{teo}[Plancherel theorem]\label{Plancherel theorem}
	If $f \in L^1(\R^d) \cap L^2(\R^d)$ then $\| f \|_2 = \| \hat{f} \|_2$.
\end{teo}
Thanks to the polarization identity this implies that $\F$ preserves the inner product in $L^2(\R^d)$:
\begin{equation}\label{Parseval formula}
	\langle f,g \rangle_{L^2(\R^d)} = \langle \hat{f}, \hat{g} \rangle_{L^2(\R^d)} \quad \forall f,g \in L^2(\R^d),
\end{equation}
therefore the Fourier transform $\F$ is a unitary operator on $L^2(\R^d)$. Result \eqref{Parseval formula} is called \emph{Parseval formula}.

So far we have seen that the Fourier transform is defined on $L^1$ and $L^2$. It can be shown, through Riesz-Thorin's interpolation theorem, that this is enough to extended the Fourier transform to all $L^p$ spaces for $1 < p < 2$ (see \cite[][Section 2.2.4]{grafakos}). Moreover, the following inequality holds.
\begin{teo}[Hausdorff-Young]
	Let $1 \leq p \leq 2$ and let $p'$ such that $\frac{1}{p} + \frac{1}{p'} = 1$. Then $\| \hat{f }\|_{p'} \leq \| f \|_p$.
\end{teo}
In what follows we will need the sharp version of the Hausdorff-Young inequality: 
\begin{equation}\label{Hausdorff-Yound inequality}
	\| \hat{f} \|_{p'} \leq \left(\dfrac{p^{1/p}}{p'^{1/p'}}\right)^{d/2} \|f\|_p = A_p^d \|f\|_p,
\end{equation}
where $A_p$ is the so-called Babenko-Bechner constant.

Having considered the spaces over which the Fourier transform is defined, we can focus on some of its properties. In particular, we want to focus on the close relationship that arises between regularity and decay properties. This is explained by the following theorems.
\begin{teo}\label{duality decay-regularity theorem}
	Let $f \in L^1(\R^d)$. If $|t|^k f \in L^1(\R^d)$ for some $k \in \N$, then $\hat{f} \in C_0^k(\R^d)$ and the following holds for every $\alpha \in \N^d$ with $|\alpha| \leq k$:
	\begin{equation}\label{derivative of transform}
		\F \left((-2 \pi i t)^{\alpha} f \right) (\omega) = \partial^{\alpha} \F f(\omega).
	\end{equation}
\end{teo}
\begin{teo}\label{duality regularity-decay theorem}
	Let $f \in C^k(\R^d)$ for some $k \in \N$. If $f, \partial^{\alpha}f \in L^1(\R^d)$ for every $\alpha \in \N^d$ with $|\alpha| \leq k$ then
	\begin{equation}\label{transform of derivative}
		\F \left(\partial^{\alpha} f\right) (\omega) = \left(2 \pi i \omega\right)^{\alpha} \F f(\omega).
	\end{equation}
	In particular this implies that $\hat{f}(\omega) = o\left(|\omega|^{-k} \right) $ as $| \omega | \rightarrow \infty$.
\end{teo}
In summary, previous theorems establish a duality between regularity and decay: if a function is smooth, then its Fourier transform decays rapidly and vice versa.

We now introduce some fundamental operators in Fourier and time-frequency analysis. Given $x,\xi \in \R^d$ and $\lambda > 0$ we define the \emph{time-shift} (or translation) operator $T_x$
\begin{equation}\label{time-shift operator def}
	T_x f(t) = f(t-x) \quad \forall\, t \in \R^d,
\end{equation}
the \emph{modulation} operator $M_{\xi}$
\begin{equation}\label{modulation operator def}
	M_{\xi} f(t) = e^{2 \pi i \xi \cdot t} f(t) \quad \forall\, t \in \R^d,
\end{equation}
and the \emph{dilation} operator $D_{\lambda}$
\begin{equation}\label{dilation operator def}
	D_{\lambda}f(t) = \lambda^d f(\lambda t) \quad \forall\, t \in \R^d.
\end{equation}
Moreover, time-shift and modulation operators can be combined into a \emph{time-frequency shift} operator
\begin{equation}\label{time-frequency shift def}
	\pi(x,\xi) f(t) = M_{\xi} T_x f(t) \quad \forall\, t \in \R^d.
\end{equation}
It is easy to check that all these operators are isometric isomorphisms with respect to the $L^1$ norm. We show how these operators act under the Fourier transform.
\begin{prop}\label{properties of translation modulation and dilation operators}
	Let $f \in L^1(\R^d)$. Then the following holds:
	\begin{enumerate}[label=(\roman*)]
		\item $\displaystyle \F(T_xf)(\omega) = M_{-x}\hat{f}(\omega)$;\label{Fourier transform of translation}
		\item $\displaystyle \F(M_{\xi} f)(\omega) = T_{\xi} \hat{f}(\omega)$;\label{Fourier transform of modulation}
		\item $\displaystyle \F(D_{\lambda}f)(\omega) = \hat{f}\left(\frac{\omega}{\lambda}\right)$.\label{Fourier transform of dilation}
	\end{enumerate}
\end{prop}

We point out that the second property, namely that $\F(M_{\xi}f) = T_{\xi}\hat{f}$, is shedding light on the role of modulation operator: while $T_x$ acts as a translation in the time domain, $M_{\xi}$ is a translation in the frequency domain. Therefore the time-frequency shift operator $\pi(x,\xi)$ is indeed a shift operator because it acts as a translation in the joint time-frequency domain.

Thanks to these properties, Theorems \ref{duality decay-regularity theorem} and \ref{duality regularity-decay theorem} we can compute, as a useful example, the Fourier transform of Gaussians.
\begin{es}\label{Fourier transform of Gaussian proposition}
	We want to compute the Fourier transform of Guassians of the kind $e^{-\lambda \pi |t|^2}$, where $\lambda > 0$ and $|t|^2$ is the Euclidean norm of $t \in \R^d$. We point out that the Fourier transform of a Gaussian is well-defined because it is a function in the Schwartz class, which is a subspace of $L^1(\R^d)$. We will show that the Fourier transform maps Gaussian into Gaussians. More precisely, the following formula holds:
	\begin{equation}\label{Fourier transform of Gaussian formula}
		\F(e^{-\lambda \pi |\cdot|^2})(\omega) = \dfrac{1}{\lambda^{d/2}} e^{-\frac{1}{\lambda}}\pi |\omega|^2 \quad \forall\,\omega \in \R^d.
	\end{equation}

	We start considering the 1-dimensional case and for the ease of notation we let $\varphi_{\lambda}(t) = e^{-\lambda \pi t^2}$ for $t \in \R$.	First of all we consider $\varphi = \varphi_1$, for which we have:
	\begin{equation*}
		\frac{d \varphi}{dt}(t) = -2 \pi t \varphi(t).
	\end{equation*}
	If we take the Fourier transform of both members, using \eqref{transform of derivative} in the former and \eqref{derivative of transform} in the latter we obtain:
	\begin{equation*}
		2 \pi i \omega \hat{\varphi}(\omega) = -i \dfrac{d\hat{\varphi}}{d\omega}(\omega) \implies \dfrac{d\hat{\varphi}}{d\omega}(\omega) = - 2 \pi \omega \hat{\varphi}(\omega),
	\end{equation*}
	which is exactly the same equation satisfied by $\varphi$, therefore $\hat{\varphi}(\omega) = C e^{-\pi \omega^2}$ for some $C \in \R$. Since $\hat{\varphi}(0) = \int_\R \varphi(t)dt = \|\varphi\|_1=1$, we obtain that $C=1$.
	
	The general case can be proved using the dilation operator:
	\begin{align*}
		&\varphi_{\lambda}(t) = e^{-\lambda \pi t^2} = e^{-\pi \left(\sqrt{\lambda} t\right)^2} = \dfrac{1}{\sqrt{\lambda}} D_{\sqrt{\lambda}}\varphi(t) \implies\\
		                     &\implies \hat{\varphi}_{\lambda}(\omega) = \dfrac{1}{\sqrt{\lambda}} \F ( D_{\sqrt{\lambda}}\varphi )(\omega) \overset{\ref{properties of translation modulation and dilation operators}\ref{Fourier transform of dilation}}{=} \dfrac{1}{\sqrt{\lambda}} \hat{\varphi}\left(\frac{\omega}{\sqrt{\lambda}}\right) = \dfrac{1}{\sqrt{\lambda}} e^{-\frac{1}{\lambda}\pi\omega^2}.
	\end{align*}
	The passage to the multidimensional case is almost straightforward since $e^{-\lambda|t|^2} = \prod_{j=1}^d e^{-\lambda t_j^2}$, therefore:
	\begin{align*}
		\F(e^{-\lambda\pi|\cdot|^2})(\omega) &= \int_{\R^d} e^{-\lambda \pi |t|^2} e^{-2\pi i \omega \cdot t}dt = \prod_{j=1}^d \int_{\R} e^{-\lambda \pi t_j^2} e^{-2\pi i \omega_j t_j} dt_j \\
										  &= \prod_{j=1}^d \dfrac{1}{\sqrt{\lambda}}e^{-\frac{1}{\lambda}\pi \omega_j^2} = \dfrac{1}{\lambda^{d/2}} e^{-\frac{1}{\lambda}\pi|\omega|^2}.
	\end{align*}
\end{es}

We conclude this section by considering how convolution product relates to the Fourier transform. Given two functions $f,g : \R^d \rightarrow \R^d$, we recall that their convolution is given by:
\begin{equation*}\label{convolution formula}
	(f * g) (x) = \int_{\R^d} f(y) g(x - y)dy.
\end{equation*}
The well-posedness of the convolution is given by Young's theorem.
\begin{teo}[Young]\label{Young theorem}
	Given $f \in L^p(\R^d)$ and $g \in L^q(\R^d)$, suppose that $\frac{1}{p}+\frac{1}{q}=1+\frac{1}{r}$ with $r \geq 1$. Then $f * g \in L^r(\R^d)$ and $\|f * g\|_r \leq \|f\|_p \|g\|_q$.
\end{teo}
We notice that, if $p=q=1$, then $r=1$ so the convolution of two $L^1$ functions is still in $L^1$. Thanks to this, using Fubini's theorem it is immediate to see that:
\begin{equation*}\label{Fourier transform of convolution}
	\F(f * g) = \F f \cdot \F g,
\end{equation*}
which explains the connection between convolution and Fourier transform.
Just like as Young's inequality, in what follows we will need the sharp version of  Hausdorff-Young's inequality, namely:
\begin{equation}\label{Young inequality sharp}
	\|f * g\|_r \leq (A_p A_q A_{r'})^d \|f\|_p \|g\|_q.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Short-Time Fourier Transform}\label{chapter STFT}

The Fourier transform is a widely used tool in both theoretical and applied settings. In particular, from an applied point of view, the importance of the Fourier transform lies primarily in the possibility of examining a signal in the frequency domain, and thus obtaining information that would otherwise be difficult to derive from the time domain. However, the distinction between these domains is rigid, whereas it is of great importance to have a tool with which time and frequency characteristics can be examined simultaneously. There are many representations that can accomplish this task, such as the Wigner distribution, the ambiguity function and so on. The basic time-frequency representation of a signal is probably the \emph{short-time Fourier transform} or \emph{STFT}. In this chapter we will define the STFT and describe some of its properties. The reference for this chapter is the book by Gr\"ochenig \cite{grochenig}.
\section{STFT: definition and properties}\label{section STFT}
The \emph{short-time Fourier transform} or \emph{STFT} is a powerful tool used in signal processing and time-frequency analysis to study the properties of a signal locally in both time and frequency. The main idea behind STFT is the following: if we want some information about the spectrum (i.e. frequencies) of a signal $f$ at a certain time, say $T$, we could choose an interval $(T-\Delta T, T + \Delta T)$ and take the Fourier transform of $f \chi_{(T-\Delta T, T + \Delta T)}$. Normally, multiplication by a characteristic function will not yield a regular function (not even continuous) and, in light of the duality between regularity and decay, the Fourier transform of $f \chi_{(T-\Delta T, T + \Delta T)}$ will not decay quickly. The problem with this not quick decay is that in this case the energy in the frequency domain will be spread all over the domain. Therefore, a sharp cutoff in the time domain leads to a ``poor'' localization in the frequency domain. In order to avoid this kind of problem, we could think to multiply the signal $f$ by a smooth function.
\begin{defi}\label{STFT def}
	Fix a function $y \neq 0$, called \emph{window function}. The \textbf{short-time Fourier transform} of a function $f$ with window $\phi$ is defined as
	\begin{equation}\label{STFT formula}
		\V_{\phi}f(x,\omega) = \int_{\R^d} f(t)\overline{\phi(t-x)}e^{-2 \pi i \omega \cdot t} dt, \quad (x,\omega) \in \R^{2d}.
	\end{equation}
\end{defi}
In the above definition we did not specify where $f$ and $\phi$ are chosen. The reason is that various combinations of $f$ and $\phi$ can lead to a ``good'' definition. We notice that \eqref{STFT formula} can be seen in an alternative way:
\begin{equation}\label{STFT formula as Fourier transform}
	\V_{\phi} f(x, \omega) = \F(f T_x \overline{\phi})(\omega),
\end{equation}
therefore STFT is well-defined whenever the Fourier transform of this function is. For example, if both $f$ and $\phi$ are in $L^2(\R^d)$ then $f T_x\overline{\phi}$, is in $L^1(\R^d)$ for every $x \in \R^d$ and so the integral in \eqref{STFT formula} is defined. In this special case the STFT can be written as a scalar product in $L^2(\R^d)$:
\begin{equation*}
	\V_{\phi}f(x,\omega) = \langle f, M_{\omega} T_x \phi \rangle = \langle f, \pi(x,\omega) \phi \rangle.
\end{equation*}
In general, the STFT of $f$ with respect to $\phi$ will be defined whenever  $\langle f, M_{\omega} T_x \phi \rangle$ is an expression of some sort of duality. For example, if $f \in \mathcal{S}'(\R^d)$ and $\phi \in \mathcal{S}(\R^d)$, then $M_{\omega} T_x \phi \in \mathcal{S}(\R^d)$, therefore $\langle f, M_{\omega} T_x \phi \rangle$ can be seen as the usual duality between tempered distributions and functions in the Schwartz space. Despite this remark, we will mainly focus on the case in which both the window and the signal are in some Lebesgue space $L^p(\R^d)$.

\subsection{Properties of STFT}\label{properties of STFT subsection}
In this section we will introduce and prove some basic properties of STFT. In particular, as we did for the Fourier transform, we want to know to which spaces $\V_{\phi} f$ belongs.
\begin{teo}\label{orthogonality relation theorem}
	Let $f_1,f_2,\phi_1,\phi_2 \in L^2(\R^d$). Then $\V_{\phi_i}f_i \in L^2(\R^{2d})$ and the following holds:
	\begin{equation}\label{orthogonality relation formula}
		\langle \V_{\phi_1} f_1, \V_{\phi_2} f_2 \rangle = \langle f_1, f_2 \rangle \overline{\langle \phi_1, \phi_2 \rangle}.
	\end{equation}
\end{teo}
\begin{proof}
	We start proving that, if $f \in L^2(\R^d)$ and $\phi \in \mathcal{S}(\R^d)$, the STFT of $f$ with window $\phi$ is in $L^2(\R^{2d})$. Since we supposed $\phi \in \mathcal{S}(\R^d)$, the function $f T_x \overline{\phi}$ is in $L^2(\R^d)$ for every $x \in \R^d$, hence:
	\begin{align*}
		\| \V_{\phi} f \|_2^2 &= \int_{\R^{2d}} |\V_{\phi} f(x,\omega)|^2 dx d\omega \overset{\mathrm{Tonelli}}{=} \int_{\R^d} \left(\int_{\R^d} |\V_{\phi} f(x,\omega)|^2 \right) d\omega  \\
							  \overset{\eqref{STFT formula as Fourier transform}}&{=} \int_{\R^d} \left(\int_{\R^d} |\F(f T_x \overline{\phi})|^2(\omega) d\omega\right) dx \overset{\mathrm{Plancherel}}{=}  \int_{\R^d} \left(\int_{\R^d} |f(t) \overline{\phi(t-x)}|^2 dt \right) dx \\
							  \overset{\mathrm{Tonelli}}&{=} \int_{\R^d} |f(t)|^2 \left(\int_{\R^d} |\phi(t-x)|^2 dx\right) dt = \|f\|_2^2 \|\phi\|_2^2.
	\end{align*}
	
	Now, consider $f_1, f_2 \in L^2(\R^d)$ and $\phi_1, \phi_2 \in \mathcal{S}(\R^d)$. Both $\V_{\phi_1} f_1$ and $\V_{\phi_2} f_2$ are in $L^2(\R^{2d})$, so their product is in $L^1(\R^{2d})$, hence we can use Fubini's theorem in the following:
	\begin{align*}
		\langle \V{\phi_1} f_1, \V_{\phi_2} f_2 \rangle &= \int_{\R^{2d}} \V_{\phi_1} f_1(x,\omega) \overline{\V_{\phi_2} f_2(x,\omega)} dx d\omega  \\
													    \overset{\mathrm{Fubini}}&{=} \int_{\R^d} \left(\int_{\R^d} \F(f_1 T_x \overline{\phi_1})(\omega) \overline{\F(f_2 T_x \overline{\phi_2})(\omega)} d\omega \right) dx \\
													    \overset{\mathrm{Parseval}}&{=} \int_{\R^d} \left( \int_{\R^d} f_1(t) \overline{\phi_1(t-x)} \, \overline{f_2(t)} \phi_2(t-x) dt \right)	dx \\
													    \overset{\mathrm{Fubini}}&{=} \int_{\R^d} f_1(t) \overline{f_2(t)} \left(\int_{\R^{d}}\phi_2(t-x) \overline{\phi_1(t-x)}dx\right) dt = \langle f_1, f_2 \rangle \langle \phi_2,\phi_1\rangle.    
	\end{align*}
	The transition from $\mathcal{S}(\R^d)$ to whole $L^2(\R^d)$ is done trough a density argument. Indeed, for $f_1, f_2 \in L^2(\R^d)$ and $\phi_{1} \in \mathcal{S}(\R^d)$ fixed, the mapping $\phi_2 \in L^2(\R^d) \mapsto \langle \V_{\phi_1} f_1, \V_{\phi_2} f_2 \rangle$ is a linear functional and we just showed that it is bounded over $\mathcal{S}(\R^d)$, where it coincides with $\langle f_1, f_2 \rangle \langle \phi_2, \phi_1 \rangle$. Since Schwartz's class is a dense subspace of $L^2(\R^d)$ it extends to a bounded linear operator for every $\phi_2 \in L^2(\R^d)$. Similarly, for fixed $f_1, f_2 \in L^2(\R^d)$ and $\phi_2 \in L^2(\R^d)$ the mapping $\phi_1 \in L^2(\R^d) \mapsto \langle \V_{\phi_1} f_1, \V_{\phi_2} f_2 \rangle$ is an antilinear functional that coincides with $\langle f_1, f_2 \rangle \langle \phi_2, \phi_1 \rangle$ over $\mathcal{S}(\R^d)$, therefore it extends to a bounded linear functional over whole $L^2(\R^d)$.
\end{proof}
\begin{cor}
	If $f, \phi \in L^2(\R^d)$ then
	\begin{equation}\label{STFT is an isometry formula}
		\| \V_{\phi} f\|_2 = \| f \|_2 \| \phi \|_2.
	\end{equation} 
	In particular, if $\| \phi \|_2 = 1$, $\V_{\phi}$ is an isometry from $L^2(\R^d)$ into $L^2(\R^{2d})$.
\end{cor}
\begin{proof}
	It is sufficient to consider \eqref{orthogonality relation formula} with $\phi_1 = \phi_2 = \phi$ and $f_1 = f_2 = f$.
\end{proof}
From the Cauchy-Schwarz inequality we immediately see that $\V_{\phi} f$ is in $L^{\infty}(\R^{2d})$:
\begin{equation}\label{STFT is bounded}
	|\V_{\phi} f(x,\omega)| = |\langle f, M_{\omega} T_x \phi\rangle| \overset{\mathrm{C-S}}{\leq} \|f\|_2 \|M_{\omega}T_x \phi\|_2 = \|f\|_2 \|\phi\|_2.
\end{equation}
Combing this with \eqref{STFT is an isometry formula} and using a simple interpolation argument we see that $\V_{\phi}f \in L^p(\R^{2d})$ for every $p \in [2,+\infty]$ and that 
\begin{equation}\label{STFT is in L^p for p>=2}
	\|\V_{\phi}f\|_p \leq \|f\|_2 \|\phi\|_2.
\end{equation}
This result is improved by the following theorem due to Lieb \cite{lieb_integral}.
\begin{teo}\label{Lieb's inequality}
	If $f,\phi \in L^2(\R^d)$ and $2 \leq p < \infty$, then:
	\begin{equation}\label{Lieb's inequality formula}
		\| \V_{\phi} \|_p^p = \int_{\R^{2d}} |\V_{\phi}(x,\omega)|^p dxd\omega \leq \left(\dfrac{2}{p}\right)^d \|f\|_2^p \cdot \|g\|_2^p.
	\end{equation}
\end{teo}
\begin{proof}
	Using the Cauchy-Schwarz inequality it is immediate to see that $f T_x \overline{\phi} \in L^1(\R^d)$ for every $x \in \R^d$. In addition to that, since $\V_{\phi} f = \F(f T_x \overline{\phi}) \in L^2(\R^{2d})$, from Fubini's theorem we can say that $ \F(f T_x \overline{\phi}) \in L^2(\R^d) $ for almost every $x \in R^d$ and therefore also $f T_x \overline{\phi} \in L^2(\R^d)$ for a.e. $x \in \R^d$. Through an interpolation argument we obtain that $f T_x \overline{\phi} \in L^q(\R^d)$ for every $q \in [1,2]$.
	
	We start considering the $L^p$ norm of $\V_{\phi} f$:
	\begin{align}
		\|\V_{\phi} f\|_p &= \left(\int_{\R^{2d}} |\V_{\phi}f(x,\omega)|^p dxd\omega\right)^{1/p} \overset{\mathrm{Tonelli}}{=} \left[\int_{\R^d} \left(\int_{\R^d} |\V_{\phi} f (x,\omega)|^p d\omega\right)dx\right]^{1/p} =\nonumber \\
						  &= \left[\int_{\R^d} \left(\int_{\R^d} |\F(f T_x \overline{\phi})(\omega)|^p d\omega\right)dx\right]^{1/p} \nonumber\\
						  \overset{\eqref{Young inequality sharp}}&{\leq} A_{p'}^d\left[\int_{\R^d} \left(\int_{\R^d} |f(t) \overline{\phi(t-x)}|^{p'}dt\right)^{p/p'}dx\right]^{1/p}, \label{Lieb's inequality intermediate}
	\end{align}
	where the use of Young's inequality \eqref{Young inequality sharp} is justified since we noticed that $fT_x \overline{\phi}$ is in $L^q(\R^d)$ for every $q \in [1,2]$, so in particular it is in $L^{p'}(\R^d)$. Letting $\phi^*(t) = \overline{\phi(-t)}$ and considering the inner integral we have
	\begin{align*}
		\int_{\R^d} |f(t) \overline{\phi(t-x)}|^{p'}dt = \int_{\R^d} |f(t)|^{p'} |\phi^*(x-t)|^{p'}dt = \left(|f|^{p'} * |\phi^*|^{p'}\right)(x),
	\end{align*}
	so the expression in \eqref{Lieb's inequality intermediate} is the $L^{p/p'}(\R^d)$ norm of $|f|^{p'} * |\phi^*|^{p'}$. Since both $f$ and $\phi$ are in $L^2(\R^d)$ and $p' \leq 2$ we have that $|f|^{p'},|\phi^*|^{p'} \in L^{2/p'}(\R^d)$. Thanks to Young's theorem \ref{Young theorem} $|f|^{p'} * |\phi^*|^{p'}$ belongs to $L^r(\R^d)$, where $r$ is given by:
	\begin{equation*}
		\dfrac{1}{(2/p')} + \dfrac{1}{(2/p')} = 1 + \dfrac{1}{r} \implies r = \dfrac{1}{p'-1} = \dfrac{1}{\frac{p}{p-1}-1} = p-1 = \dfrac{p}{p'},
	\end{equation*}
	therefore, using the sharp version of Young's inequality \eqref{Young inequality sharp} in \eqref{Lieb's inequality intermediate} we obtain:
	\begin{align*}
		\|\V_{\phi} f\|_p &\leq A_{p'}^d \left(A_{2/p'}^d A_{2/p'}^d A_{(p/p')'}^d \| \,|f|^{p'} \|_{2/p'} \|\,|\phi^*|^{p'}\|_{2/p'} \right)^{1/p'}.
	\end{align*}
	However $\| \,|f|^{p'} \|_{2/p'} = (\int_{\R^d} (|f(x)|^{p'})^{2/p'}dx)^{p'/2} = \|f\|_2^{p'}$ and from a direct calculation (which can be found in \ref{constant in Lieb's inequality calculation}) one can see that $A_{p'}^d A_{2/p'}^{2d/p'}A_{(p/p')'}^{d/p'}=(2/p)^{d/p}$, which corresponds to the desired result.
\end{proof}

In light of Theorem \ref{orthogonality relation theorem}, the STFT with window $\phi$ in $L^2(\R^d)$ can be seen as a unitary operator from $L^2(\R^d)$ into $L^2(\R^{2d})$. Therefore, we can find its adjoint operator. From a direct computation it can be seen that this is given by:
\begin{equation}\label{STFT adjoint}
	\V_{\phi}^* g(t) = \int_{\R^{2d}} g(x,\omega) \phi(t-x) e^{2 \pi i \omega \cdot t} dxd\omega = \int_{\R^{2d}} g(x,\omega) M_{\omega}T_x \phi (t) dxd\omega\quad \forall g \in L^2(\R^{2d}).
\end{equation}
This adjoint operator appears in the following nice property, named \emph{inversion formula for the STFT}.
\begin{teo}\label{inversion formula theorem}
	Let $f \in L^2(\R^d)$ and $\phi, \gamma \in L^2(\R^{2d})$ such that $\langle \phi, \gamma \rangle \neq 0$. Then:
	\begin{equation}\label{inversion formula}
		f(t) = \dfrac{1}{\langle \phi, \gamma \rangle} \V_{\gamma}^* \V_{\phi} f(t) = \dfrac{1}{\langle \phi, \gamma \rangle} \int_{\R^{2d}} \V_{\phi}f(x,\omega)M_{\omega}T_x \gamma (t) dxd\omega \quad \forall t \in \R^d.
	\end{equation}
\end{teo}
\begin{proof}
	Given $f,g \in L^2(\R^d)$ from \eqref{orthogonality relation formula} we have:
	\begin{equation*}
		\langle \V_{\phi} f, \V_{\gamma} g \rangle = \langle f,g \rangle \langle \gamma, \phi \rangle.
	\end{equation*}
	On the other hand:
	\begin{equation*}
		\langle \V_{\phi} f, \V_{\gamma} g \rangle = \langle \V_{\gamma}^* \V_{\phi} f, g \rangle,
	\end{equation*}
	therefore, letting $I$ be the identity operator over $L^2(\R^d)$, we have:
	\begin{align*}
		\langle \V_{\gamma}^* \V_{\phi} f, g \rangle = \langle f,g \rangle \langle \gamma, \phi \rangle \implies \langle (\V_{\gamma}^* \V_{\phi} - \langle \gamma, \phi \rangle I)f, g \rangle = 0.
	\end{align*}
	Since this holds for every $g \in L^2(\R^d)$ necessarily:
	\begin{equation*}
		(\V_{\gamma}^* \V_{\phi} - \langle \gamma, \phi \rangle I)f = 0 \implies \dfrac{1}{\langle \gamma, \phi \rangle}\V_{\gamma}^* \V_{\phi} f = f.
	\end{equation*}
\end{proof}
Therefore, the adjoint operator $\V_{\gamma}^*$ acts, in some sense, as an inverse operator. This will be of paramount importance afterwards.
\section{Bargmann Transform and Fock Space}\label{section Fock Space and Bargmann transform}
In the introduction of the chapter we justified the multiplication of a function $f$ by a window $\phi$ in the definition of the STFT through the relation between decay and regularity under the action of Fourier transform. Hence, it seems reasonable to make furhter consideration on the STFT when the window function is a regular and rapidly-decaying one. In this section, we will consider the specific case in which the window is a Gaussian. In particular, we choose a Gaussian of the following form:
\begin{equation}\label{gaussian normalized}
	\varphi(t) = 2^{d/4} e^{-\pi |t|^2}.
\end{equation}
The factor $2^{d/4}$ is chosen so that $\|\varphi\|_2=1$. The STFT with Gaussian window becomes
\begin{equation}
	\V_{\varphi}f(x,\omega) = 2^{d/4} \int_{\R^d} f(t) e^{-\pi |t-x|^2} e^{-2 \pi i \omega \cdot t} dt.
\end{equation}
Our goal is to rearrange the terms in the above expression in order to make $z=x+i\omega \in \C^d$ appear. We want to highlight the fact that, when talking about complex quantities, $|z|^2 = z \overline{z} = |x|^2 + |\omega|^2$.
\begin{align*}
	\V_{\varphi}f(x,\omega) &= 2^{d/4} \int_{\R^d} f(t) e^{-\pi |t|^2 + 2 \pi x \cdot t - \pi |\omega|^2} e^{ - 2 \pi i \omega \cdot t} dt\\
							&= 2^{d/4} \int_{\R^d} f(t) e^{-\pi |t|^2} e^{2 \pi (x - i \omega) \cdot t} e^{-\frac{\pi}{2}\left(|x|^2 - 2 i x \cdot \omega - |\omega|^2\right)} e^{-\frac{\pi}{2}\left(|x|^2 + |\omega|^2 + 2 i x \cdot \omega\right)}dt\\
							&= 2^{d/4} e^{-\pi i x \cdot \omega} e^{-\frac{\pi}{2}\left(|x|^2 + |\omega|^2\right)} \int_{\R^d} f(t) e^{-\pi |t|^2} e^{2 \pi (x-i\omega) \cdot t}e^{-\frac{\pi}{2}(x-i\omega)^2}dt.
\end{align*}
The rearrangement may seem arbitrary, but actually it is done in such a way that inside the integral $x$ and $\omega$ enter only via $\overline{z}$. This leads to the following definition.
\begin{defi}\label{Bargmann transform}
	The \textbf{Bargmann transform} of a function $f$ on $\R^d$ is the function $\Barg f$ on $\C^d$ given by
	\begin{equation}\label{Bargmann transform formula}
		\Barg f(z) = 2^{d/4} \int_{\R^d} f(t) e^{2 \pi t \cdot z - \pi |t|^2 - \frac{\pi}{2}z^2}dt.
	\end{equation}
\end{defi}
The existence of a connection between the STFT with Gaussian windows and the Bargmann transform is clear and it is formally stated in the following proposition.
\begin{prop}\label{connection between Bargmann tranform and STFT proposition}
	If $f$ is a function on $\R^d$ with polynomial growth then its Bargmann transform $\Barg f$ is an entire function on $\C^d$. Moreover, letting $z = x + i\omega$, the Bargmann transform of $f$ is related to its STFT through the following
	\begin{equation}\label{connection between Bargmann transform and STFT}
		\V_{\varphi} f(x,-\omega) = e^{\pi i x \cdot \omega} \Barg f(z) e^{-\pi |z|^2/2}
	\end{equation}
\end{prop}
We recall that a function defined over $\C^d$ is \emph{entire} if it is holomorphic over all $\C^d$.
\begin{defi}\label{Fock space}
	The \textbf{Fock space} $\Fock^2(\C^d)$ is the Hilbert space of all entire functions $F$ on $\C^d$ for which the norm
	\begin{equation}\label{Fock space norm}
		\|F\|^2_{\Fock^2} = \int_{\C^d} |F(z)|^2 e^{-\pi |z|^2}dz
	\end{equation}
	is finite, where $dz$ stands for the Lebesgue measure on $\C^d$.
\end{defi}
Clearly the norm of the Fock space is induced by the following scalar product
\begin{equation}\label{Fock space scalar product}
	\langle F,G \rangle_{\Fock^2} = \int_{\C^d} F(z) \overline{G(z)} e^{-\pi |z|^2}dz.
\end{equation}

\begin{prop}\label{Bargmann transform is an isometry}
	If $f \in L^2(\R^d)$ then
	\begin{equation}
		\|f\|_2 = \left(\int_{\C^d} |\Barg f(z)|^2 e^{-\pi |z|^2}dz\right)^{1/2} = \|\Barg f\|_{\Fock^2}.
	\end{equation}
	Thus $\Barg$ is an isometry from $L^2(\R^d)$ into $\Fock^2(\C^d)$.
\end{prop}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Localization Operators}\label{chapter localization operators}
In the previous chapter we defined the STFT, which, roughly speaking, gives us a ``time-frequency picture'' of a signal. Once we have our joint representation, we might be interested in highlighting some of its features. For example, we might be interested in understanding where most of the energy is in phase-space.

In this chapter we will look at two ways in which we can solve the problem of creating operators able to localize a signal.
\section{Localization with projections}\label{Localization with projections section}
Our first attempt to localize a signal is arguably the most straightforward one, namely using a sharp cutoff. If we suppose to have a signal $f \in L^2(\R^d)$ and we want to localize it in a measurable subset $T \subseteq \R^d$ of the time domain we can consider the natural projection operator:
\begin{equation}\label{time projection operator}
	P_T : L^2(\R^d) \rightarrow L^2(\R^d), \quad P_T f(t) = \chi_T(t) f(t).
\end{equation}
This is clearly a projection operator, which means that $P_T^2 = P_T = P_T^*$.

In the same fashion we can define an operator able to localize on a measurable subset $\Omega \subseteq \R^d$ in the frequency domain. Its definition it is not as direct as the one for time projections but it is still easy to understand:
\begin{equation}\label{frequency projection operator}
	Q_{\Omega} : L^2(\R^d) \rightarrow L^2(\R^d) \quad Q_{\Omega} f(t) = \F^{-1} \left(\chi_{\Omega} \F f \right)(t) = \int_{\Omega} \hat{f}(\omega) e^{2 \pi i \omega \cdot t} d\omega.
\end{equation}
It is also quite simple to show that this is a projection operator:
\begin{align*}
	&Q_{\Omega}^2 = \F^{-1} \chi_{\Omega} \F \F^{-1} \chi_{\Omega} \F = \F^{-1} \chi_{\Omega} \chi_{\Omega} \F = \F^{-1} \chi_{\Omega} \F = Q_{\Omega},\\
	&Q_{\Omega}^* = \left( \F^{-1} \chi_{\Omega} \F\right)^* = \F^* \chi_{\Omega}^* \left(\F^{-1}\right)^* = \F^{-1} \chi_{\Omega} \F = Q_{\Omega},
\end{align*}
where we used the fact that the Fourier transform is a unitary operator on $L^2(\R^d)$, namely that $\F^* = \F^{-1}$.\\
Since both operators are projections, their norm is less or equal than 1, independently of $T$ and $\Omega$, indeed:
\begin{align*}
	&\| P_T f \|_{L^2(\R^d)} = \| f \|_{L^2(T)} \leq \| f \|_{L^2(\R^d)},\\
	&\| Q_{\Omega} f \|_{L^2(\R^d)} = \| \F^{-1} \chi_{\Omega} \F f\|_{L^2(\R^d)} \overset{\text{Plancherel}}{=} \| \F f\|_{L^2(\Omega)} \leq \| \F f \|_{L^2(\R^d)} \overset{\text{Plancherel}}{=} \| f \|_{L^2(\R^d)}.
\end{align*}

Clearly, those operators do not answer our original question about localization in time-frequency, since $P_T$ and $Q_{\Omega}$ act only in time and frequency, respectively. However, we may think to combine these projections into a single operator:
\begin{equation*}\label{composition of projections}
	Q_{\Omega} P_T, \; P_T Q_{\Omega} : L^2(\R^d) \rightarrow L^2(\R^d),
\end{equation*}
which hopefully is able to localize a signal both in time and frequency ``near'' to the set $T \times \Omega$.

It is clear that these operators are linear and bounded, in particular their norms are less or equal than 1. Moreover, they are one the adjoint of the other, indeed:
\begin{equation}\label{projection operators adjoint}
	(Q_{\Omega} P_T)^* = P_T^* Q_{\Omega}^* = Q_{\Omega} P_T.
\end{equation}

Up to now the only (essential) hypothesis on $T$ and $\Omega$ is that they are measurable. Clearly, by adding some requirements on $T$ and $\Omega$ we expect $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ to gain some properties.
\begin{prop}\label{projection operators are Hilbert-Schmidt}
	Let $T,\Omega \subset \R^d$ with finite measure. Then $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ are Hilbert-Schmidt integral operators of the form
	\begin{align}
		Q_{\Omega} P_T f(x) &= \int_{\R^d} K(x,t) f(t) dt, \\
		P_T Q_{\Omega} f(x) &= \int_{\R^d} \overline{K(t,x)}f(t)dt,
	\end{align}
	where
	\begin{equation}\label{integral kernel projection operators}
		K(x,t) = \chi_T(t) \int_{\Omega} e^{2 \pi i \omega \cdot (x-t)}d\omega,
	\end{equation}
	and $\| K \|_{L^2(\R^{2d})} = \sqrt{|T||\Omega|}$.
\end{prop}
\begin{proof}
	Given $f \in L^2(\R^d)$ we have:
	\begin{align*}
		Q_{\Omega}P_T f(x) = \int_{\Omega} e^{2 \pi i \omega \cdot x} \left(\int_{T} e^{-2\pi i \omega \cdot t} f(t)dt\right)d\omega \overset{\mathrm{Fubini}}{=} \int_{\R^d} \chi_T(t) \left(\int_{\Omega} e^{2 \pi i \omega \cdot (x -t)} d\omega\right)f(t)dt,
	\end{align*}
	where the use of Fubini's theorem is allowed since $\Omega$ and $T$ have finite measure. This gives us the expression of $Q_{\Omega}P_T$. In order to obtain also the expression for $P_T Q_{\Omega}$ it suffices to recall from \eqref{projection operators adjoint} that $(Q_{\Omega} P_T)^* = P_T Q_{\Omega}$. Therefore, the integral kernel of $P_T Q_{\Omega}$ is given by Proposition \ref{condition integral operator self-adjoint}. Lastly, from Theorem \ref{representation of Hilbert-Schmidt integral operator}, we have that $\|Q_{\Omega} P_T\|_{\mathrm{HS}} = \|P_T Q_{\Omega}\|_{\mathrm{HS}} = \|K\|_{L^2(\R^{2d})}$, and:
	\begin{align*}
		\|K\|_{L^2(\R^{2d})} &= \left(\int_{\R^{2d}}   |K(x,t)|^2 dxdt\right)^{1/2} \\
							 &= \left(\int_{\R^{2d}} \chi_T(t) \Bigg| \int_{\R^d} e^{2 \pi i \omega \cdot (x-t)} \chi_{\Omega}(\omega)d\omega \Bigg|^2 dxdt \right)^{1/2} \\
							 &= \left(\int_{\R^{2d}} \chi_T(t) \big|\F^{-1}(\chi_{\Omega})(x-t)\big|^2dxdt\right)^{1/2}  \\
							 \overset{\mathrm{Tonelli}}&{=} \left[ \int_{\R^d} \chi_T(t) \left(\int_{\R^d} \big|\F^{-1}(\chi_{\Omega})(x-t)\big|^2 dx\right)dt\right]^{1/2} \\
							 &= \left(\int_{\R^d} \| T_t \F^{-1} (\chi_{\Omega})\|_2^2 \chi_T(t)dt\right)^{1/2} \\
							 \overset{T_t\,\mathrm{isometry} + \mathrm{Plancherel}}&{=} \| \chi_{\Omega} \|_2 \|\chi_T\|_2 = \sqrt{|T| |\Omega|}.
	\end{align*}
\end{proof}
If we compare the integral kernels of $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ we see that $K(x,t) \neq \overline{K(t,x)}$, hence, by Proposition \ref{condition integral operator self-adjoint}, we immediately conclude that both operator are not self-adjoint. We already know that, if possible, it is better to deal with self-adjoint operators, so we should consider the following operators:
\begin{align}
	(Q_{\Omega} P_T)^* Q_{\Omega} P_T &= P_T^* Q_{\Omega}^* Q_{\Omega} P_T= P_T Q_{\Omega} P_T;\\
	(P_T Q_{\Omega})^* P_T Q_{\Omega} &= Q_{\Omega}^* P_T^* P_T Q_{\Omega} = Q_{\Omega} P_T Q_{\Omega}.
\end{align} 
By construction, these are self-adjoint operators, and since both $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ are compact, thanks to Proposition \ref{projection operators are Hilbert-Schmidt}, Theorem \ref{Hilbert-Schmidt operators are compact and bounded} and Theorem \ref{composition of compact operators is compact} they are also compact. Hence, by Theorem \ref{self-adjoint compact operators are diagonalizable}, they can be diagonalized. In the particular but relevant case where $T$ and $\Omega$ are intervals (disks in the multi-dimensional case) the eigenfunctions of these operators are the \emph{prolate spheroidal wave functions} and have been studied by Slepian, Pollak and Landau in a series of papers \cite{prolate_I, prolate_II, prolate_III, prolate_IV}.

\section{Daubechies' localization operators}\label{Daubechies' localization operators section}
The projection operators considered in the previous section fulfil the task of localizing a signal in both time and frequency. However, those are still treated separately. Indeed, if we consider, for example, $Q_{\Omega} P_T$, we see that at the first moment we perform localization in time and only then in frequency. Since our task is to localize in both domains at the same time, it would be more natural to have an operator that treats time and frequency in a joint way. This is exactly what Ingrid Daubechies did in her remarkable 1988-paper \cite{daubechies}. 

In Chapter \ref{chapter STFT} we defined a time-frequency representation of a signal, namely the STFT. Therefore, to reach our goal, it seems more natural to use STFT instead of the Fourier transform. Moreover, from Theorem \ref{inversion formula theorem} we know that the adjoint operator of $\V_{\phi}$ acts, in some sense, as an inverse operator. If we choose a window $\phi \in L^2(\R^{2d})$ normalized, \ref{inversion formula} becomes
\begin{equation*}
	f(t) = \V_{\phi}^* \V_{\phi}f(t).
\end{equation*}
The key idea is to multiply $\V_{\phi} f$ by a \emph{weight function} $F(x,\omega)$, which logically should highlight some features of $\V_{\phi}f$, before applying the adjoint operator. This leads to the definition of \emph{time-frequency localization operators}:
\begin{equation}\label{Daubechies' localization operator def}
	L_{F,\phi} : L^2(\R^d) \rightarrow L^2(\R^d), \quad L_{F,\phi}f(t) = \V_{\phi}^* F \V_{\phi} f(t).
\end{equation}
Related to this localization operator is the sesquilinear form $\L_{F,\phi} : L^2(\R^d) \times L^2(\R^d) \rightarrow \C$ defined by the expression:
\begin{equation}\label{sesquilinear form localization operator}
	\L_{F,\phi}(f,g) = \int_{\R^{2d}} F(x,\omega) \V_{\phi}f(x,\omega) \overline{\V_{\phi}g(x,\omega)} dxd\omega.
\end{equation}
Indeed, assuming $\L_{F,\phi}$ is bounded, we could define $ L_{F,\phi} f$ through Riesz' representation theorem as the only element of $L^2(\R^d)$ such that:
\begin{equation}\label{L_F through duality}
	\L_{F,\phi}(f,g) = \langle L_{F,\phi}f,g \rangle  = \int_{\R^d} L_{F,\phi}f(t) \overline{g(t)}dt \quad \forall g \in L^2(\R^d),
\end{equation}
and therefore $L_{F,\phi}$ as the operator which maps $f$ into its representation.


Now that we have defined time-frequency localization operators our goal is to study their properties, starting from boundedness, compactness and the belonging to trace class or Hilbert-Schmidt class. We recall that the window function $\phi \in L^2(\R^d)$ is fixed, so it is clear that the properties of $L_{F,\phi}$ will depend upon $F$.
\begin{prop}\label{F in L^p L_F bounded}
	Let $F \in L^p(\R^{2d})$ for $p \in [1,+\infty]$. Then $L_{F,\phi}$ is bounded and $\|L_{F,\phi}\| \leq \|F\|_p$.
\end{prop}
\begin{proof}
	Letting $f,g \in L^2(\R^d)$, we have:
	\begin{align*}
		|\L_{F,\phi} (f,g)| \leq \int_{\R^{2d}} |F(x,\omega)| |\V_{\phi}f(x,\omega)| |\V_{\phi}g(x,\omega)| dxd\omega.
	\end{align*}
	From \eqref{STFT is in L^p for p>=2} we know that, given $f \in L^2(\R^d)$, $\V_{\phi} f \in L^p(\R^d)$ for every $p \in [2,+\infty]$ and $\|\V_{\phi} f\|_p \leq \|f\|_2$. We want to find an exponent $q \geq 2$ in order to apply (generalized) H\"older's inequality:
	\begin{align*}
		\dfrac{1}{p} + \dfrac{1}{q} + \dfrac{1}{q} = 1 \implies q =  \dfrac{2p}{p-1},
	\end{align*}
	which is greater or equal than 2, regardless of $p$. Applying H\"older's inequality with exponents $p$, $q$ and $q$ we have:
	\begin{align*}
		|\L_{F,\phi} (f,g)| \leq \|F\|_p \|\V_{\phi}f\|_q \|\V_{\phi}g\|_q \leq \|F\|_p \|f\|_2 \|g\|_2.
	\end{align*}
	Taking the supremum above all normalized $f,g \in L^2(\R^d)$ gives us the boundedness of $\L_{F,\phi}$ and, in the end, of $L_{F,\phi}$.
\end{proof}

In previous section we managed to prove that projection operators $Q_{\Omega}P_T$ and $P_T Q_{\Omega}$ are Hilbert-Schmidt operators, provided both $T$ and $\Omega$ have finite measure, which is equivalent to asking that $\chi_T$ and $\chi_{\Omega}$ are in $L^1(\R^d)$. An analogous result holds for Daubechies' localization operators.
\begin{teo}\label{F integrable L_F Hilbert-Schmidt}
	Let $F \in L^1(\R^{2d})$. Then $L_{F,\phi}$ is a Hilbert-Schmidt integral operator with kernel
	\begin{equation}\label{integral kernel localization operator}
		K_F(s,t) = \int_{\R^{2d}} F(x,\omega) M_{\omega} T_x \phi(s) \overline{M_{\omega} T_x \phi(t)} dx d\omega.
	\end{equation}
	Moreover, $\|K_F\|_2 \leq \|F\|_1$.
\end{teo}
\begin{proof}
		Let $f,g \in L^2(\R^d)$. We begin showing that $F(x,\omega) f(t) \overline{M_{\omega}T_{x} \phi(t)} \, \overline{g(s)}M_{\omega}T_x \phi(s)$ belongs to $L^1(\R^{2d} \times \R^d \times \R^d)$:
	\begin{align*}
		&\int_{\R^{4d}} |F(x,\omega) f(t) \overline{M_{\omega}T_{x} \phi(t)} \, \overline{g(s)}M_{\omega}T_x \phi(s)| dx d\omega dt ds \\
		\overset{\mathrm{Tonelli}}&{=} \int_{\R^{2d}} |F(x,\omega)|\left( \int_{\R^d} |f(t)| |M_{\omega}T_x \phi(t)|dt\right) \left( \int_{\R^d} |g(s)| |M_{\omega}T_x g(s)|ds\right)dx d\omega \\
		\overset{\mathrm{C-S}}&{\leq}  \|f\|_2 \|\phi\|_2 \|g\|_2 \|\phi\|_2 \int_{\R^{2d}} |F(x,\omega)|dx d\omega = \|F\|_1 \|f\|_2 \|g\|_2.
	\end{align*}
	Now we can apply Fubini's theorem in the expression of $\langle L_{F,\phi}f,g\rangle$:
	\begin{align*}
		\langle L_{F,\phi}f,g \rangle &= \int_{\R^{2d}} F(x,\omega) \left(\int_{\R^d} f(t) \overline{M_{\omega}T_x \phi(t)}dt\right) \overline{\left(\int_{\R^d} g(s) \overline{M_{\omega}T_x \phi(s)}ds\right)} dx d\omega  \\
									  \overset{\mathrm{Fubini}}&{=} \int_{\R^d} \left[\int_{\R^d}\left( \int_{\R^{2d}} F(x,\omega) M_{\omega}T_x \phi(s) \overline{M_{\omega} T_x \phi(t)} dx d\omega \right) f(t) dt\right] \overline{g(s)}ds \\
									  &= \int_{\R^d} \left(\int_{\R^d} K_F(s,t) f(t)dt\right) \overline{g(s)} ds.
	\end{align*}
	Since this holds for every $f$ and $g$ we can conclude that $L_{F,\phi} f = \int_{\R^d} K_F(\cdot,t) f(t)dt $. Thanks to Proposition \ref{Hilbert-Schmidt integral operator}, we know that such integral operator is a Hilbert-Schmidt operator if and only if $K_F \in L^2(\R^{2d})$, so all we have to do is to compute its norm.
	\begin{align*}
		\|K_F\|_2^2 &= | \langle K_F, K_F \rangle |\leq \int_{\R^{2d}} \left( \int_{\R^{2d}} |F(x,\omega)| |M_{\omega}T_x \phi(s)| |M_{\omega} T_x \phi(t)| dx d\omega\right)\\
					&\phantom{=| \langle K_F, K_F \rangle |\leq \int_{\R^{2d}}} \! {\cdot} \left(\int_{\R^{2d}} |F(y,\xi)| |M_{\xi}T_y \phi(s)| |M_{\xi} T_y \phi(t)| dy d\xi\right) ds dt  \\
					\overset{\mathrm{Fubini}}&{=} \int_{\R^{4d}} |F(x,\omega)| |F(y,\xi)| \left(\int_{\R} |M_{\omega} T_x \phi(t)| |M_{\xi} T_y \phi(t)| dt\right) \\
					&\phantom{= \int_{\R^{4d}} |F(x,\omega)| |F(y,\xi)|} \!\cdot\left(\int_{\R} |M_{\omega}T_x \phi(s)| |M_{\xi}T_y \phi(s)| ds\right) dx d\omega dy d\xi \\
					\overset{\mathrm{C-S}}&{\leq}\|\phi\|_2^4 \int_{\R^{2d}} |F(x,\omega)| dx d\omega \int_{\R^{2d}} |F(y,\xi)| dy d\xi = \|F\|_1^2.
	\end{align*}
\end{proof}
Reminding  that Hilbert-Schmidt operators are compact (Theorem \ref{Hilbert-Schmidt operators are compact and bounded}) we observe that, if $F$ is integrable, the corresponding localization operator $L_{F,\phi}$ is compact. Moreover, since we have the explicit expression of the integral kernel, from Proposition \ref{condition integral operator self-adjoint} follows immediately the next sufficient condition on $F$ in order to make $L_{F,\phi}$ self-adjoint.
\begin{prop}\label{condition localization operator self-adjoint}
	If $F \in L^1(\R^{2d})$ is a real-valued function then $L_{F,\phi}$ is self-adjoint.
\end{prop}
We will now prove that localization operators with integrable weight function are trace-class operators. We want to emphasize that, in light of Proposition \ref{trace-class operators are Hilbert-Schmidt}, this is a stronger condition than just being a Hilbert-Schmidt operator.
\begin{teo}\label{F integrable L_F trace-class}
	Let $F \in L^1(\R^{2d})$. Then $L_{F,\phi}$ is a trace-class operator. Moreover, given an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$, the following holds:
	\begin{equation}\label{trace of localization operators}
		\sum_{n=1}^{+\infty} |\langle L_{F,\phi}e_n,e_n \rangle| \leq \|F\|_1, \quad \mathrm{tr}L_{F,\phi} = \int_{\R^{2d}} F(x,\omega) dx d\omega.
	\end{equation}
\end{teo}
\begin{proof}
	We start proving that $L_{F,\phi}$ is a trace-class operator. Given an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$:
	\begin{align*}
		\sum_{n=1}^{+\infty} |\langle L_{F,\phi} e_n, e_n \rangle| &= \sum_{n=1}^{+\infty} \Big| \int_{\R^{2d}} F(x,\omega) \V_{\phi}e_n(x,\omega) \overline{\V_{\phi} e_n(x,\omega)} dx d\omega \Big|  \\
																   &\leq \sum_{n=1}^{+\infty} \int_{\R^{2d}} |F(x,\omega)| |\V_{\phi} e_n(x,\omega)\|^2 dxd\omega \\
																   &= \int_{\R^{2d}} |F(x,\omega)| \sum_{n=1}^{+\infty} |\langle e_n, M_{\omega}T_x \phi \rangle|^2 dx d\omega \\
																   \overset{\mathrm{Parseval}}&{=} \int_{\R^{2d}} |F(x,\omega)| \|M_{\omega} T_x \phi\|_2^2 dx d\omega = \|\phi\|_2^2 \|F\|_1 = \|F\|_1,
	\end{align*}
	where the exchange between series and integral is due to the monotone convergence theorem. Now that we know that $L_{F,\phi}$ is trace-class we can compute its trace:
	\begin{align*}
		\mathrm{tr}L_{F,\phi} &= \sum_{n=1}^{+\infty} \langle L_{F,\phi}e_n e_n \rangle = \sum_{n=1}^{+\infty}  \int_{\R^{2d}} F(x,\omega) |\V_{\phi}e_n(x,\omega)|^2 dx d\omega \\
							  &= \lim_{N \rightarrow +\infty} \sum_{n=1}^{N} \int_{\R^{2d}} F(x,\omega) |\V_{\phi}e_n(x,\omega)|^2 dx d\omega.
	\end{align*}
	Since
	\begin{align*}
		|F(x,\omega)| \sum_{n=1}^{N} |\V_{\phi}e_n(x,\omega)|^2 \leq |F(x,\omega)| \sum_{n=1}^{+\infty} |\V_{\phi}e_n(x,\omega)|^2 = |F(x,\omega)| \in L^1(\R^{2d}),
	\end{align*}
	we can apply Lebesgue's dominated convergence theorem to conclude that: 
	\begin{align*}
		\mathrm{tr}L_{F,\phi} = \lim_{N \rightarrow +\infty} \sum_{n=1}^{N} \int_{\R^{2d}} F(x,\omega) |\V_{\phi}e_n(x,\omega)|^2 dx d\omega = \int_{\R^{2d}} F(x,\omega) dx d\omega.
	\end{align*}
\end{proof}
So far, except for \ref{F in L^p L_F bounded}, we considered only the case $F \in L^1(\R^{2d})$. As the last result of the section we will deal with the more generic case $F \in L^p(\R^{2d})$ for $p < +\infty$.
\begin{prop}\label{F in L^p L_F compact}
	Let $F \in L^p(\R^{2d})$ with $1 \leq p < \infty$. Then the corresponding localization operator $L_{F,\phi}$ is compact.
\end{prop}
\begin{proof}
	Given $F \in L^p(\R^{2d})$, thanks to Theorem \ref{space of compact operators is closed} it is sufficient to consider a sequence $F_n$ of functions in $L^1(\R^{2d})$ such that $F_n \rightarrow F$ in $L^p(\R^{2d})$. For example, we can suppose that $F_n$ are in Schwartz's class $\mathcal{S}(\R^{2d})$, which is a well-known dense subspace of $L^p(\R^{2d})$ for $p < +\infty$. Indeed, from Proposition \ref{F in L^p L_F bounded}, we have that $\| L_{F_n,\phi} - L_{F,\phi}\| \leq \|F_n - F\|_p$, so $L_{F_n,\phi} \rightarrow L_{F,\phi}$ in $\B(L^2(\R^d))$. Since $\mathcal{S}(\R^{2d}) \subset L^1(\R^{2d})$, $L_{F_n,\phi}$ are compact, thus also $L_{F,\phi}$ is.
\end{proof}

\subsection{Spherically Symmetric Weights}\label{section spherically symmetric weights}
In previous section we managed to prove that, if the weight function $F$ is in $L^p(\R^{2d})$ for some $p < +\infty$ and it is real-valued then the corresponding localization operator $L_{F,\phi}$ is compact and self-adjoint. Thus, it is natural to ask which are its eigenfuctions with corresponding eigenvalues. However, this in general is not feasible. Hence, we shall consider some specific class of weight and window functions. In particular, in this section we will consider the special case in which the window for the STFT is a Gaussian \eqref{gaussian normalized} and the weight $F$ is spherically symmetric. Letting $r_j^2 = x_j^2 + \omega_j^2$ for $j=1,\ldots,d$ and $r^2=(r_1^2\ldots,r_d^2) \in \R^d$, the hypothesis about $F$ can be rephrased in the following way
\begin{equation}\label{spherically symmetric weight}
	F(x,\omega) = \mathdutchcal{F}(r^2).
\end{equation}
In order to highlight the dependence of $F$ through $\mathdutchcal{F}$, the corresponding localization operator will be denoted as $L_{\mathdutchcal{F},\varphi}$.
For this operators a complete characterization of the spectrum and eigenspaces is given in the already cited paper of Daubechies \cite{daubechies}. 

Before stating we need to introduce some special function, namely \emph{Hermite functions}. In dimension $d=1$, Hermite functions are given by:
\begin{equation}\label{Hermite function 1-dimensional}
	H_k(t) = \dfrac{2^{1/4}}{\sqrt{k!}}\left(-\dfrac{1}{2\sqrt{\pi}}\right)^k e^{\pi t^2} \dfrac{d^k}{dt^k}\left(e^{-2\pi t^2}\right),
\end{equation}
where $k \in \N \cup \{0\} \coloneqq \N_0$. Hermite functions have lots of interesting and useful properties. A standard reference is \cite[][Section 1.7]{folland_harmonic}. We cite some of them which will be useful in the following.
\begin{enumerate}[label=(\roman*)]
	\item $\{H_k\}_{k \in \N_0}$ is an orthonormal basis of $L^2(\R)$;
	\item $H_0(t) = \varphi(t)$, where $\varphi$ is the normalized Gaussian given by \eqref{gaussian normalized};
	\item Setting $H_{-1} = 0$, the following recursive relation holds
	\begin{equation}\label{Hermite functions recursion}
		2\sqrt{\pi}t H_k = \sqrt{k+1}H_{k+1} + \sqrt{k}H_{k-1} \quad \text{for } k=0,1,\ldots;
	\end{equation}
	\item Hermite functions are eigenfunctions of $\F$, specifically
	\begin{equation}\label{Hermite functions are eigenfunctions of Fourier transform}
		\F H_k =(-i)^k H_k.
	\end{equation}
\end{enumerate}
Hermite functions in generic dimension are just the tensor product of 1-dimensional Hermite functions. Explicitly, given a multi-index $k = (k_1,\ldots,k_d) \in \N_0^d$, the corresponding Hermite function is given by:
\begin{equation}\label{Hermite function d-dimensional}
	H_k(t) = \prod_{j=1}^d H_{k_j}(t_j).
\end{equation}
It is still true that $d$-dimensional Hermite functions (now ranging between all possible multi-indices) form an orthonormal basis of $L^2(\R^d)$. Moreover, using \eqref{Hermite functions are eigenfunctions of Fourier transform}, it is easy to see that $d$-dimensional Hermite functions are still eigenfunction of the Fourier transform and that the following holds:
\begin{equation}\label{d-dimensional Hermite functions are eigenfunctions of Fourier transform}
	\F H_k = (-i)^{|k|}H_k,
\end{equation}
where $|k|=k_1 + \cdots k_d$ is the length of the multi-index.

The introduction of Hermite functions is necessary, since they are exactly the eigenfunctions of $L_{\mathdutchcal{F}, \varphi}$.
\begin{teo}\label{eigenvalues and eigenvectors localization operator spherically symmetric weight}
	Eigenfuctions of $L_{\mathdutchcal{F},\varphi}$ are the $d$-dimensional Hermite functions $H_k$, with corresponding eigenvalues:
	\begin{equation}\label{eigenvalues of localization operators with radial weight}
		\lambda_k = \dfrac{1}{k!}\int_0^{+\infty} {\cdots} \int_0^{+\infty} \mathdutchcal{F}\left(\frac{s_1}{\pi},\ldots,\frac{s_d}{\pi}\right) \left(\prod_{j=1}^{d} s_j^{k_j} \right) e^{-(s_1 + \cdots + s_d)} ds_1 \cdots ds_d,
	\end{equation}
	where $k \in \N_0^d$ and $k! = k_1 ! \cdots k_d!$.
\end{teo}
Before proving the theorem we need the following lemma.
\begin{lem}\label{integral of translated Gaussian lemma}
	Given $z \in \C$, it holds:
	\begin{equation}\label{integral of translated Gaussian formula}
		\int_{\R} e^{-2\pi(t^2 + zt)}dt = \dfrac{1}{2^{1/2}} e^{\pi z^2/2}.
	\end{equation}
\end{lem}
\begin{proof}
	Letting $z = \mathrm{Re}z + i\mathrm{Im}z = u + iv$ we have:
	\begin{align*}
		\int_{\R} e^{-2\pi(t^2 + zt)}dt &= \int_{\R} e^{-2\pi(t^2 + ut)} e^{-2\pi i v t}dt = e^{\pi u^2/2} e^{-2\pi(t^2 + ut + u^2/4)}e^{-2\pi i v t}dt\\
										&= e^{\pi u^2/2} \int_{\R} e^{-2\pi(t + u/2)^2} e^{-2\pi i v t}dt = e^{\pi u^2/2} \F(T_{-u/2}e^{-2\pi(\cdot)^2})(v) \\
										\overset{\ref{properties of translation modulation and dilation operators}\ref{Fourier transform of translation} + \ref{Fourier transform of Gaussian proposition}}&{=} e^{\pi u^2/2} e^{2 \pi i (u/2)v}\dfrac{1}{2^{1/2}}e^{-\pi v^2/2} = \dfrac{1}{2^{1/2}} e^{\pi (u^2 + 2iuv - v^2)/2} = \dfrac{1}{2^{1/2}} e^{\pi z^2/2}
		\end{align*}
\end{proof}
\begin{proof}[Proof of Theorem \ref{eigenvalues and eigenvectors localization operator spherically symmetric weight}]
	Since Hermite functions are an orthonormal basis of $L^2(\R^n)$ it is sufficient to prove that $\langle  L_{\mathdutchcal{F},\varphi}H_k,H_l\rangle = \langle \mathdutchcal{F} \V_{\varphi}H_k, \V_{\varphi}H_l \rangle =  \lambda_k \prod_{j=1}^d \delta_{k_j,l_j}$, which means that the scalar product is different from zero if and only if $k=l$. We start by computing the STFT of a Hermite function:
	{\allowdisplaybreaks[1]
	\begin{align}\label{STFT of Hermite functions}
		\V_{\varphi}H_k(x,\omega) &= \int_{\R^d} H_k(t) e^{-2\pi i \omega \cdot t}\, 2^{d/4} e^{-\pi|t-x|^2}dt = \int_{\R^d} \prod_{j=1}^d H_{k_j}(t_j) e^{-2\pi i \omega \cdot t}\, 2^{d/4} e^{-\pi|t-x|^2}dt \nonumber \\
		&= \prod_{j=1}^{d} 2^{1/4}\int_{\R} H_{k_j}(t_j) e^{-2\pi i \omega_j t_j} e^{-\pi(t_j-x_j)^2}dt_j \nonumber\\
		&= \prod_{j=1}^{d} 2^{1/4}\int_{\R} \dfrac{2^{1/4}}{\sqrt{k_j!}}\left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} e^{\pi t_j^2} \dfrac{d^{k_j}}{dt^{k_j}}\left(e^{-2\pi t_j^2}\right) e^{-2\pi i \omega_j t_j} e^{-\pi(t_j-x_j)^2}dt_j \nonumber\\
		&= \prod_{j=1}^d \dfrac{2^{1/2}}{\sqrt{k_j!}} \left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} \int_{\R} \dfrac{d^{k_j}}{dt^{k_j}}\left(e^{-2\pi t_j^2}\right) e^{\pi(t_j^2 - 2i\omega_j t_j -t_j^2 + 2t_j x_j - x_j^2)}dt_j \nonumber\\
		&= \prod_{j=1}^d \dfrac{2^{1/2}}{\sqrt{k_j!}} \left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} e^{-\pi x_j^2}\int_{\R} \dfrac{d^{k_j}}{dt^{k_j}}\left(e^{-2\pi t_j^2}\right) e^{2\pi(x_j - i\omega_j)t_j}dt_j \nonumber\\
		&= \prod_{j=1}^d \dfrac{2^{1/2}}{\sqrt{k_j!}} \left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} e^{-\pi x_j^2} [2\pi(x_j - i\omega_j)]^{k_j} (-1)^{k_j}\int_{\R}  e^{-2\pi[t_j^2 - (x_j - i \omega_j)t_j]} dt_j \nonumber\\
		&=\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}}2^{1/2}( x_j - i\omega_j)^{k_j} e^{-\pi x_j^2} \dfrac{1}{2^{1/2}} e^{\pi (x_j - i \omega_j)^2/2} \nonumber\\
		&= \prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}} (x_j - i \omega_j)^{k_j} e^{-\pi i \omega_j x_j} e^{-\pi(x_j^2 + \omega_j^2)/2} \nonumber\\
		&= \left(\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}} (x_j - i \omega_j)^{k_j} \right) e^{-\pi i \omega \cdot x} e^{-\pi(r_1^2 + \cdots + r_d^2)/2}.
	\end{align}}
	Before computing the scalar product between $L_{\mathdutchcal{F},\varphi} H_k$ and $H_l$, we introduce the angular coordinate $\theta_j$, such that $x_j + i \omega_j = r_j e^{i\theta_j}$. Therefore we have:

	\begin{align}
		\langle  L_{\mathdutchcal{F},\varphi}H_k,H_l\rangle &= \int_{\R^{2d}} F(x,\omega) \left(\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}} r_j^{k_j}e^{-i k_j \theta_j}\right)e^{-\pi i \omega \cdot x} e^{-\pi(r_1^2 + \cdots + r_d^2)/2} \cdot \nonumber\\
															&\hphantom{=\int_{\R^{2d}} F(x,\omega)} \,\overline{\left(\prod_{m=1}^d \sqrt{\dfrac{\pi^{l_m}}{l_m!}} r_m^{l_m}e^{-i l_m \theta_m}\right)e^{-\pi i \omega \cdot x} e^{-\pi(r_1^2 + \cdots + r_d^2)/2}}dxd\omega \nonumber\\
															&=\int_{\R^{2d}} F(x,\omega) \left(\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j + l_j}}{k_j!l_j!}} r_j^{k_j + l_j} e^{i(l_j - k_j)\theta_j} \right) e^{-\pi(r_1^2 + \cdots + r_d^2)}dxd\omega. \label{scalar product Hermite functions}
	\end{align}
	For every pair of coordinates $(x_j,\omega_j)$ we can switch to polar coordinates $(r_j,\theta_j)$. Since $F(x,\omega) = \mathdutchcal{F}(r^2)$ is independent of angular coordinates, only functions depending on those are $e^{i(l_j - k_j)\theta_j}$, for which:
	\begin{equation*}
		\int_0^{2\pi} e^{i(l_j - k_j)\theta_j}d\theta_j = \begin{cases}
			\displaystyle \frac{e^{i(l_j - k_j)\theta_j}}{i(l_j - k_j)} \bigg\rvert_0^{2\pi}=0 \quad & \mathrm{if\ } k_j \neq l_j\\
			\phantom{a}\\
			\displaystyle \int_0^{2\pi} d\theta_j = 2\pi \quad &\mathrm{if\ } k_j=l_j
		\end{cases}.
	\end{equation*}
	Therefore, if $k \neq l$, the whole integral is 0, otherwise, letting $k_j = l_j$ in \eqref{scalar product Hermite functions}:
	\begin{align*}
		\langle  L_{\mathdutchcal{F},\varphi}H_k,H_k\rangle &= (2\pi)^d \dfrac{\pi^{|k|}}{k!}\int_0^{+\infty} {\cdots} \int_0^{+\infty} \mathdutchcal{F}(r_1^2,\ldots,r_d^2) e^{-\pi(r_1^2 + \cdots + r_d^2)}\left(\prod_{j=1}^{d} r_j^{2k_j + 1} \right) dr_1 \cdots dr_d \\
															&= \dfrac{1}{k!} \int_0^{+\infty} \cdots \int_0^{+\infty} \mathdutchcal{F}(r_1^2,\ldots,r_d^2) e^{-\pi(r_1^2 + \cdots + r_d^2)} \left(\prod_{j=1}^{d} (\pi r_j^2)^{k_j} \right) \pi r_1 dr_1 \cdots \pi r_d dr_d.
	\end{align*}
	With the change of variable $s_j = \pi r_j^2$, we finally obtain:
	\begin{equation*}
		\langle  L_{\mathdutchcal{F},\varphi}H_k,H_k\rangle = \dfrac{1}{k!} \int_0^{+\infty} \cdots \int_0^{+\infty} \mathdutchcal{F}\left(\frac{s_1}{\pi},\ldots,\frac{s_d}{\pi}\right) \left(\prod_{j=1}^{d} s_j^{k_j} \right) e^{-(s_1 + \cdots + s_d)} ds_1 \cdots ds_d,
	\end{equation*}
	which is exactly the expression \eqref{eigenvalues of localization operators with radial weight}.
\end{proof}
In conclusion, we will consider two meaningful examples, namely when the weight $F$ is the characteristic function of a disk centred around the origin and when it is a Gaussian. In order to make computations easier we confine ourselves in the case $d=1$.
\begin{es}[Localization on a disk]\label{example localization on a disk}
	We study arguably the most simple case, namely when $F$ is the characteristic function of the disk $\mathdutchcal{B}_R = \{(x,\omega) \in \R^2 : x^2 + \omega^2 \leq R^2\}$:
	\begin{equation*}
			F(x,\omega) = \mathdutchcal{F}(r^2) = \begin{cases}
				1 \quad \mathrm{if\ } x^2 + \omega^2 = r^2 \leq R^2\\
				0 \quad \mathrm{otherwise}
			\end{cases} .
	\end{equation*}
	In order to highlight that $F$ is the characteristic function of $\mathdutchcal{B}_R$ we let $L_{\mathdutchcal{F},\varphi} = L_{\mathdutchcal{B}_R,\varphi}$. Noticing that $\mathdutchcal{F}(\frac{s}{\pi}) = \chi_{[0,\pi R^2]} (s)$, expression \eqref{eigenvalues of localization operators with radial weight} brings to:
	\begin{align*}
		\lambda_k(R) = \dfrac{1}{k!} \int_{0}^{+\infty} \chi_{[0,\pi R^2]}(s) s^k e^{-s} ds = \dfrac{1}{k!} \int_{0}^{\pi R^2} s^k e^{-s} ds = \gamma(k+1,\pi R^2),
	\end{align*}
	where $\gamma$ is the lower incomplete gamma function. An easy integration by parts, when $k \geq 1$, leads to:
	\begin{equation*}
		\int_{0}^{\pi R^2} s^k e^{-s} ds = -(\pi R^2)^k e^{-\pi R^2} + k \int_{0}^{\pi R^2} s^{k-1} e^{-s} ds.
	\end{equation*}
	Iterating this process gives us the following formula for the $k$-th eigenvalue:
	\begin{equation*}
		\lambda_k = 1 - e^{-\pi R^2} \sum_{j=0}^{k} \dfrac{(\pi R^2)^j}{j!}, \quad k=0,1,\ldots
	\end{equation*}
	Since $(\pi R^2)^j/j!$ is strictly positive, it follows immediately that the sequence of eigenvalues is strictly decreasing. Moreover, since $F$ is real-valued, from Proposition \ref{condition localization operator self-adjoint} we have that $L_{\mathdutchcal{B}_R,\varphi}$ is self-adjoint, as well as compact. Therefore, from Corollay \ref{norm is greatest eigenvalue} we conclude that:
	\begin{equation*}
		\| L_{\mathdutchcal{B}_R,\varphi} \| = |\lambda_0| = 1 - e^{-\pi R^2}.
	\end{equation*}
	Recalling the definition of the norm for operators between Hilbert spaces \ref{dual norm}, we obtain that, for every normalized $f \in L^2(\R^d)$:
	\begin{align}\label{bound norm localization on a disk}
			\lambda_0 &= \|L_{\mathdutchcal{B}_R,\varphi} \| \geq |\langle L_{\mathdutchcal{B}_R,\varphi} f, f \rangle | = \int_{\mathdutchcal{B}_R} |\V_{\varphi} f(x,\omega) |^2 dx d\omega \nonumber \\
			&\implies  \int_{\mathdutchcal{B}_R} |\V_{\varphi} f(x,\omega) |^2 dx d\omega\leq 1 - e^{-\pi R^2}.
	\end{align}
	We point out that the left-hand side of the last expression represents the energy of $\V_{\varphi} f$ concentrated on the disk $\mathdutchcal{B}_R$. 
\end{es}

\begin{es}[Localization with Gaussian weight]
	Another natural choice for the weight function $F$ is a Gaussian:
	\begin{equation*}
		F(x,\omega) = e^{-\alpha \pi (x^2 + \omega^2)},
	\end{equation*}
	where $\alpha > 0$ is a dilation parameter. In this case $\mathdutchcal{F}(\frac{s}{\pi}) = e^{-\alpha s}$, so from \eqref{eigenvalues of localization operators with radial weight} and integrating by parts $k+1$ times we obtain the eigenvalues of $L_{\mathdutchcal{F},\varphi}$:
	\begin{align*}
		\lambda_k = \dfrac{1}{k!} \int_0^{+\infty} s^k e^{-(1+\alpha)s}ds = (1+\alpha)^{-(k+1)}, \quad k=0,1,\ldots
	\end{align*}
	Like the previous case, eigenvalues are already ordered in decreasing order and $F$ is still real-valued, therefore
	\begin{equation*}
		\|L_{\mathdutchcal{F},\varphi}\| = 1+\alpha.
	\end{equation*}
\end{es}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Uncertainty principles}\label{chapter uncertainty principles}

Up to know we put some effort in constructing some tools that have the ability to concentrate a signal in the time-frequency domain. In the introduction of Section \ref{section STFT} we also pointed out that a characteristic function is not a ``good'' window for the STFT because, in light of the duality between regularity and decay, the Fourier transform of a not regular functions has a slow decay. Indeed, in Section \ref{section spherically symmetric weights}, we considered the STFT with a Gaussian window function, which has nice regularity and decay properties. However, a natural question arises: how good can we concentrate a signal? Is it possible to have a signal arbitrarily concentrated both in time and frequency? The answer to this questions is definitely no and it is given by \emph{uncertainty principles}, which are ubiquitous results in Fourier and time-frequency analysis. Uncertainty principles arise in different versions but the main underlying idea is the following:\\
\emph{a function cannot be too concentrated both in time and frequency}.

Even if not explicit, we already had a first glimpse of this phenomenon when we computed the Fourier transform of dilated Gaussian (Example \ref{Fourier transform of Gaussian proposition}). Indeed, given $\lambda > 0$ we recall that:
\begin{equation*}
	\F(e^{-\lambda \pi |\cdot|^2})(\omega) = \dfrac{1}{\lambda^{d/2}} e^{-\frac{1}{\lambda}}\pi |\omega|^2 \quad \forall\,\omega \in \R^d.
\end{equation*}
If we choose $\lambda$ to be very large, the Gaussian in the time domain $e^{-\lambda \pi |t|^2}$ will be strongly concentrated around the origin. However, in the corresponding Gaussian in the frequency domain the dilation parameter appears in the denominator of the exponent, so this will be poorly concentrated.

In this chapter we will present some uncertainty principles, both for the Fourier transform and the STFT and we will give a quantitative description of how good we can localize a signal.

\section{Heisenberg's uncertainty principle}\label{Heisenberg's uncertainty principle sections}
Arguably, the most famous uncertainty principle is the one named after Heisenberg (\cite{heisenberg}). Despite being a fascinating topic, we will not discuss all the implications that this uncertainty principles has in quantum mechanics. Therefore, our attention is driven to the mathematical formulation of the principle.

In literature there are several proofs of Heisenberg's uncertainty principle. The one that we will present was given (in the 1-dimensional case) by de Bruijn in \cite{de_bruijn} and involves, once again, Hermite functions. Before stating and proving Heisenberg's uncertainty principle we are going to need some lemmas.
\begin{lem}\label{Heisenberg's uncertainty principle lemma}
	Let $f \in L^2(\R)$. Then
	\begin{equation}\label{Heisenberg's uncertainty principle lemma formula}
		\int_{\R} t^2 |f(t)|^2dt + \int_{\R} \omega^2 |\hat{f}(\omega)|^2d\omega = \dfrac{1}{2\pi} \sum_{k=0}^{+\infty} (2k+1)|\langle f,H_k \rangle|^2,
	\end{equation}
	where $H_k$ is the $k$-th Hermite function. In particular we have
	\begin{equation}\label{Heisenberg's uncertainty principle lemma inequality}
		\int_{\R} t^2 |f(t)|^2dt + \int_{\R} \omega^2 |\hat{f}(\omega)|^2d\omega \geq \dfrac{\|f\|_2^2}{2\pi},
	\end{equation}
	with equality if and only if $f$ is a multiple of $H_0$.
\end{lem}
\begin{proof}
	Our aim is to exploit the fact that Hermite functions are an orthonormal basis of $L^2(\R)$ by computing $\langle tf, H_k\rangle$. Before going on we remark that the previous expression is not an $L^2$ scalar product but is a duality between a tempered distribution and a function in the Schwartz class. However, from the theory of tempered distributions we have that $\langle tf, H_k \rangle = \langle f, t H_f \rangle$ and the latter expression is indeed a scalar product in $L^2(\R^d)$. Moreover, from this observation, we see that $t H_k$ appears. Therefore, using \eqref{Hermite functions recursion} we have:
	\begin{equation*}
		\langle tf, H_k \rangle = \langle f, tH_k \rangle = \dfrac{1}{2\sqrt{\pi}} \left( \sqrt{k+1} \langle f, H_{k+1} \rangle+ \sqrt{k}\langle f,H_{k-1}\rangle \right).
	\end{equation*}
	To compute the similar quantity for $\hat{f}$ we also need to recall that Hermite functions are eigenfunction of the Fourier transform \eqref{Hermite functions are eigenfunctions of Fourier transform}:
	\begin{align*}
		\langle \omega \hat{f}, H_k \rangle &= \langle \hat{f}, \omega H_k \rangle = \dfrac{1}{2\sqrt{\pi}} \left( \sqrt{k+1} \langle \hat{f}, H_{k+1} \rangle + \sqrt{k}\langle \hat{f},H_{k-1}\rangle \right) \\
											\overset{\eqref{Hermite functions are eigenfunctions of Fourier transform} + \F\;\mathrm{unitary}}&{=} \dfrac{1}{2\sqrt{\pi}} \left( \sqrt{k+1} (-i)^{k+1}\langle f, H_{k+1} \rangle + \sqrt{k}(-i)^{k-1}\langle f,H_{k-1}\rangle \right) \\
											&= \dfrac{1}{2\sqrt{\pi}} (-i)^{k-1} \left( \sqrt{k}\langle f,H_{k-1}\rangle - \sqrt{k+1} \langle f, H_{k+1} \rangle  \right).
	\end{align*}
	Now, since $\{H_k\}_{k \in \N_0^d}$ is an orthonormal basis of $L^2(\R)$, we can use Parseval's identity:
	\begin{align*}
		\int_{\R} t^2 |f(t)|^2dt + \int_{\R} \omega^2 |\hat{f}(\omega)|^2d\omega &= \sum_{k=0}^{+\infty} \left( |\langle tf, H_k \rangle|^2 + |\langle \omega \hat{f}, H_k\rangle|^2 \right) \\
								 											     &= \dfrac{1}{2\pi}\sum_{k=0}^{+\infty} \left[ (k+1)|\langle f,H_{k+1}\rangle|^2 + k|\langle f,H_{k-1}\rangle| \right] \\
								 											     &= \dfrac{1}{2\pi}\sum_{k=0}^{+\infty} (2k+1)|\langle f,H_k\rangle|^2.
	\end{align*}	
	Since $(2k+1) \geq 1$, it is immediate to see that inequality \eqref{Heisenberg's uncertainty principle lemma inequality}  holds and that equality is achieved if and only if $\langle f,H_k \rangle = 0$ for every $k > 1$, which means exactly that $f$ is a multiple of $H_0$.
\end{proof}
In order to extend previous lemma to the multi-dimensional case we need the following result related to the Fourier transform of restrictions. Before stating the lemma we introduce the following notation:
\begin{equation*}
	\begin{gathered}
		t' = (t_2, \ldots, t_d) \in \R^{d-1},\quad \omega' = (\omega_2, \ldots, \omega_d) \in \R^{d-1},\\
		(\F_1 f) (\omega_1, t') = \F(f(\cdot, t'))(\omega_1),\quad (\F' f)(t_1, \omega') = \F (f(t_1, \cdot))(\omega').
	\end{gathered}
\end{equation*}
\begin{lem}\label{Fourier transform for restrictions}
	Let $ f \in L^2(\R^d)$. Then $\F_1 f (\omega_1, \cdot) \in L^2(\R^{d-1})$ for almost every $\omega_1 \in \R$ and $\F' (\F_1 f (\omega_1, \cdot))(\omega') = \F f(\omega)$.
\end{lem}
\begin{proof}
	Before starting, we point out that, since $f \in L^2(\R^d)$, $f(\cdot, t') \in L^2(\R)$ for almost every $t' \in \R^{d-1}$, therefore $\F_1 f(\cdot, t')$ is well-defined for almost every $t' \in \R^{d-1}$. Then we have:
	\begin{align*}
		\int_{\R^d} |\F_1 f (\omega_1, t')|^2 d\omega_1 dt' \overset{\mathrm{Tonelli}}&{=} \int_{\R^{d-1}} \int_{\R} |\F(f(\cdot,t'))(\omega_1)|^2 d\omega_1 dt' \\
																					  \overset{\mathrm{Plancherel}}&{=} \int_{\R^{d-1}} \int_{\R} |f(t_1,t')|^2 dt_1 dt' = \|f\|_2.
	\end{align*}
	which proves that $\F_1 f(\omega_1, \cdot)$ is in $L^2(\R^{d-1})$ for almost every $\omega_1 \in \R$.
	
	Now suppose that $f \in L^1(\R^d) \cap L^2(\R^d)$. Since $f$ is in $L^1(\R^d)$ we can use \eqref{Fourier transform formula} to make $\F_1 f$ explicit:
	\begin{equation*}
	\begin{gathered}
		\F_1 f(\omega_1, t') = \int_{\R} f(t_1,t') e^{-2 \pi i \omega_1 t_1} dt_1 \implies \\
		\F' (\F_1 f(\omega_1, \cdot))(\omega') = \int_{\R^{d-1}} \left(\int_{\R} f(t_1,t') e^{-2 \pi i \omega_1 t_1} dt_1\right) e^{-2 \pi i \omega' \cdot t'} dt' \\
												\overset{\mathrm{Fubini}}{=} \int_{\R^d} f(t_1,t') e^{-2 \pi i \omega_1 t_1} e^{-2 \pi i \omega' \cdot t'} dt_1 dt' = \F f(\omega).
	\end{gathered}
	\end{equation*}
	Through the density of $L^1(\R^d) \cap L^2(\R^d)$ in $L^2(\R^d)$ the last part of the statement follows.
\end{proof}
\begin{lem}\label{Heisenberg's uncertainty principle lemma d dimensions}
	Let $f \in L^2(\R^d)$. Then, for every $j = 1,\ldots,d$:
	\begin{equation}\label{Heisenberg's uncertainty principle lemma d dimensions formula}
		\int_{\R^d} t_j^2 |f(t)|^2 dt + \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega = \dfrac{1}{2\pi} \sum_{k \in \N_0^d} (2k_j + 1) |\langle f, H_k \rangle|^2.
	\end{equation}
	In particular we have:
	\begin{equation}\label{Heisenberg's uncertainty principle lemma d dimensions inequality}
		\int_{\R^d} t_j^2 |f(t)|^2dt + \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega \geq \dfrac{\|f\|_2^2}{2\pi},
	\end{equation}
	with equality if and only if $f$ is a multiple of $H_0$.
\end{lem}
\begin{proof}
	Without loss of generality we consider the case $j=1$. Moreover, we introduce the following notation:
	\begin{equation*}
		\langle f, g \rangle_1(t') = \langle f(\cdot, t'), g(\cdot,t') \rangle, \quad \langle f, g \rangle'(t_1) = \langle f(t_1, \cdot), g(t_1, \cdot) \rangle,
	\end{equation*}
	
	From Lemma \ref{Heisenberg's uncertainty principle lemma}, since $f (\cdot, t') \in L^2(\R)$ for a.e. $t' \in \R^{d-1}$, we have:
	\begin{equation*}
		\int_{\R} t_1^2 |f(t_1, t')|^2 dt_1 + \int_{\R} \omega_1^2 |\F_1 f(\omega_1, t')|^2 d\omega_1 = \dfrac{1}{2\pi} \sum_{k_1=0}^{+\infty} (2k_1 + 1) |\langle f, H_{k_1} \rangle_1|^2,
	\end{equation*}
	which holds for almost every $t' \in \R^{d-1}$. We can now integrate with respect to $t'$ every member:
	\begin{itemize}
		\item $\displaystyle \int_{\R^{d-1}} \int_{\R} t_1^2 |f(t_1, t')|^2 dt_1 dt' \overset{\mathrm{Tonelli}}{=} \int_{\R^d} t_1^2 |f(t)|^2 dt$;
		\item $\displaystyle \int_{\R^{d-1}} \int_{\R}  \omega_1^2|\F_1 f(\omega_1, t')|^2 d\omega_1 dt' \overset{\mathrm{Tonelli}}{=} \int_{\R} \omega_1^2 \int_{\R^{d-1}} |\F_1 f(\omega_1, t')|^2 dt' d\omega_1$.\vspace{0.1mm}\\
		
		From Lemma \ref{Fourier transform for restrictions} we know that $\F_1 f(\omega_1) \in L^2(\R^{d-1})$ for a.e. $\omega_1 \in \R$. Therefore, we can use Plancherel's theorem in the inner integral for a.e. $\omega_1 \in \R$ and obtain:
		\begin{align*}
			\int_{\R^{d-1}} \int_{\R}  \omega_1^2 |\F_1 f(\omega_1, t')|^2 d\omega_1 dt' &= \int_{\R} \omega_1^2\int_{\R^{d-1}} |\F' (\F_1 f(\omega_1, \cdot))(\omega')|^2 d\omega' d\omega_1 \\
																			  \overset{\ref{Fourier transform for restrictions}}&{=} \int_{\R} \int_{\R^{d-1}} \omega_1^2 |\F f(\omega_1,\omega')|^2 d\omega' d\omega_1 = \int_{\R^d} \omega_1^2 |\F (\omega)|^2 d\omega.
		\end{align*}
		\item For the last term we start pointing out that integral and series can be exchanged because every term is non-negative. Then, from Parseval's identity we have:
		\begin{align*}
			\int_{\R^{d-1}} |\langle f, H_{k_1} \rangle_1(t')|^2 dt' &= \sum_{k' \in \N_0^{d-1}} |\langle \langle f, H_{k_1} \rangle_{1}, H_{k'} \rangle'|^2 \\
											 &= \sum_{k' \in \N_0^{d-1}} \Big| \int_{\R^{d-1}} \left(\int_{\R} f(t_1,t') \overline{H_{k_1}(t_1)} dt_1 \right) \overline{H_{k'}(t')} dt' \Big|^2 \\
											 \overset{\mathrm{Fubini}}&{=}  \sum_{k' \in \N_0^{d-1}} \Big| \int_{\R^{d}} f(t) \overline{H_{(k,k')}(t)} dt \Big|^2 = \sum_{k' \in \N_0^{d-1}} |\langle f, H_{(k,k')} \rangle|^2,
		\end{align*}
		where we used the fact that multi-dimensional Hermite functions are just the tensor product of 1-dimensional ones. Plugging this result in the series leads to:
		\begin{align*}
			\int_{\R^{d-1}} \sum_{k_1=0}^{+\infty} (2k_1 + 1) |\langle f, H_{k_1} \rangle |^2 &= \sum_{k_1=0}^{+\infty} \sum_{k' \in \N_0^{d-1}} (2k_1+1) |\langle f, H_{(k,k')} \rangle|^2 \\
																							  &= \sum_{k \in \N_0^d} (2k_1+1)|\langle f, H_k \rangle|^2,
			\end{align*}
		and we notice that the rearrangement of the series is allowed since convergence is unconditional. Putting all these results together leads to \eqref{Heisenberg's uncertainty principle lemma d dimensions formula}.
	\end{itemize}
	The proof of the last part of the statement is exactly the same as the one in Lemma \ref{Heisenberg's uncertainty principle lemma}.
\end{proof}
We are now in the position to prove Heisenberg's uncertainty principle.
\begin{teo}\label{Heisenberg's uncertainty principle theorem}
	Let $f \in L^2(\R^d)$ and $a,b \in \R^d$. Then:
	\begin{equation}\label{Heisenberg's uncertainty principle formula}
		\left( \int_{\R^d} |t-a|^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} |\omega - b|^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \geq \dfrac{d\|f\|_2^2}{4\pi}.
	\end{equation}
	Moreover, equality is achieved if and only if $f(t) = c M_b T_a \varphi(\lambda t)$, where $\varphi$ is the normalized Gaussian given by \eqref{gaussian normalized}, $c \in \C$, $\lambda > 0$ and $a,b \in \R^d$.
\end{teo}

\begin{proof}
	Firsly, we notice that it is sufficient to prove the inequality when $a=b=0$, since the generic case can be recovered from this special one by means of a phase-space translation. Indeed, given $f \in L^2(\R^d)$ we can consider $g = M_{-b}T_{-a}f$ for which we have:
	\begin{align*}
		|f(t)|^2 &= |(T_a M_b g)(t)|^2 = |e^{2\pi i b \cdot (t-a)} g(t-a)|^2 = |g(t-a)|^2,\\
		|\hat{f}(\omega)|^2 &= |\F(T_a M_b g)(\omega)|^2 \overset{\ref{properties of translation modulation and dilation operators}}{=} |M_{-a} T_b \hat{g}(\omega)|^2 = |e^{-2\pi i a \cdot \omega} \hat{g}(\omega - b)|^2 = |\hat{g}(\omega-b)|^2,
	\end{align*}
	and $\|f\|_2 = \|g\|_2$. In light of this, we will consider $a=b=0$.
	
	We start proving that, for every component, the following holds:
	\begin{equation}\label{Heisenberg's uncertainty principle component formula}
		\left( \int_{\R^d} t_j^2\, |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} \omega_j^2\, |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \geq \dfrac{\|f\|_2^2}{4\pi}.
	\end{equation} We observe that in the left-hand side of \eqref{Heisenberg's uncertainty principle component formula}, apart from a square root, we have the product of two integrals, while in \eqref{Heisenberg's uncertainty principle lemma d dimensions inequality} we had an estimate for the sum of these. The transition from the latter to the former estimate can be done through a dilation argument. Precisely, given $f \in L^2(\R^d)$, we consider the following dilation:
	\begin{equation*}
		g(t) = \lambda^{-d/2}f(t/\lambda) \implies \hat{g}(\omega) = \lambda^{d/2} \F(D_{1/\lambda}g) \overset{\ref{properties of translation modulation and dilation operators}\ref{Fourier transform of dilation}}{=} \lambda^{d/2} \hat{f}(\lambda \omega).
	\end{equation*}
	We remark that this dilation is different from the one considered in Section \ref{Fourier transform and its properties section}, expression \eqref{dilation operator def}. Indeed, now we chose the dilation so that $\|g\|_2 = \|f\|_2$, while previously the dilation was chosen in order to preserve the $L^1$ norm. Putting $g$ in \eqref{Heisenberg's uncertainty principle lemma d dimensions inequality} provides us:
	\begin{align}\label{Heisenberg's uncertainty principle inequality 1}
		\dfrac{\|f\|_2^2}{2\pi} &\leq \int_{\R^d} t_j^2 |g(t)|^2dt + \int_{\R^d} \omega_j^2 |\hat{g}(\omega)|^2d\omega \\
								&= \dfrac{1}{\lambda^d} \int_{\R^d} t_j^2 |f(t/\lambda)|^2dt  + \lambda^d \int_{\R^d} \omega^2 |\hat{f}(\lambda \omega)|^2d\omega \nonumber \\
								&= \lambda^2 \int_{\R^d} t_j^2 |f(t)|^2dt + \dfrac{1}{\lambda^2} \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega. \nonumber
	\end{align}
	We can choose $\lambda$ in order to minimize the last expression. Thus, deriving with respect to $\lambda^2$ and putting the derivative to 0 we obtain that the minimum is achieved when 
	\begin{equation}\label{Heisenberg's uncertainty principle lambda}
		\lambda^2 = \left(\int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega\right)^{1/2} \left(\int_{\R^d} t_j^2 |f(t)|^2dt\right)^{-1/2}.
	\end{equation}
	If we substitute this $\lambda$ into the last expression we obtain exactly \eqref{Heisenberg's uncertainty principle component formula}. Moreover, equality is achieved if and only if it is achieved in \eqref{Heisenberg's uncertainty principle inequality 1}, but Lemma \ref{Heisenberg's uncertainty principle lemma d dimensions} then implies that $g(t) = c H_0$ for some $c \in \C$. Recalling the definition of $g$ we obtain $f(t) = c \lambda^{d/2} H_0(\lambda t)$. If we put the explicit expression of $f$ into \eqref{Heisenberg's uncertainty principle lambda} we will see that $\lambda$ can be chosen arbitrarily, indeed:
	\begin{align*}
		\lambda^2 &= \left(\int_{\R^d} \omega_j^2 |c|^2 \lambda^{-d} |\F(D_{\lambda} H_0)(\omega)|^2 d\omega\right)^{1/2} \left(\int_{\R^d} t_j^2 |c|^2 \lambda^d |H_0(\lambda t)|^2dt\right)^{-1/2} \\
				  \overset{\eqref{properties of translation modulation and dilation operators}\ref{Fourier transform of dilation} + \eqref{Hermite functions are eigenfunctions of Fourier transform}}&{=} \left(\int_{\R^d} \omega_j^2 \lambda^{-d} |H_0(\omega/\lambda)|^2 d\omega\right)^{1/2} \left(\int_{\R^d} t_j^2 \lambda^d |H_0(\lambda t)|^2dt\right)^{-1/2} \\
				  &\overset{\hspace{-3mm}\xi = \omega/\lambda}{\underset{\hspace{-3mm}s = \lambda t}{\hspace{-3mm}=}} \lambda^2 \left(\int_{\R^d} \xi_j^2 |H_0(\xi)|^2 d\xi \right)^{1/2} \left(\int_{\R^d} s_j^2  |H(s)|^2 ds \right)^{-1/2} = \lambda^2.
	\end{align*}
	We notice that this result is independent of $j$. So, to sum up, equality in \eqref{Heisenberg's uncertainty principle component formula} is achieved for every $j=1,\ldots,d$ if and only if $f(t) = c H_0(\lambda t)$ for some $c \in \C$ and $\lambda > 0$.
	
	Now that we have \eqref{Heisenberg's uncertainty principle component formula} we can prove \eqref{Heisenberg's uncertainty principle formula}, starting from:
	\begin{align*}
		&\left( \int_{\R^d} |t|^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} |\omega|^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \\
		=&\left( \sum_{j=1}^d \int_{\R^d} t_j^2 |f(t)|^2dt \right)^{1/2} \left( \sum_{j=1}^d \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2}.
	\end{align*}
	We notice that the last expression is the product of the Euclidean norm of vectors $( \| t_j f \|_2 )_{j=1}^d$ and $( \| \omega_j \hat{f} \|_2)_{j=1}^d$. Thus, from Cauchy-Schwarz inequality in $\R^d$, we obtain:
	\begin{align*}
		&\left( \int_{\R^d} |t|^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} |\omega|^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \\
		\geq &\sum_{j=1}^d \left( \int_{\R^d} t_j^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \overset{\eqref{Heisenberg's uncertainty principle component formula}}{\geq} \dfrac{d \|f\|_2^2}{4 \pi}.
	\end{align*}
	Finally, equality is achieved if and only if both inequalities in the last expression become equalities. From the first part of the proof we know that equality in the latter inequality is achieved if and only if $f(t) = c H_0(\lambda t)$ for some $c \in \C$ and $\lambda > 0$. Moreover, always from previous computations we saw that in this case $f$ satisfies \eqref{Heisenberg's uncertainty principle lambda} for every $j=1,\ldots,d$. This means exactly that vectors $(\|t_j f\|_2)_{j=1}^d$ and $(\|\omega_j \hat{f}\|_2)_{j=1}^d$ are parallel, therefore equality is achieved also when using Cauchy-Schwarz' inequality.
\end{proof}

We shall comment a mathematical interpretation of Heisenberg's uncertainty principle. This can be written in the following form:
\begin{equation*}
	\left(\int_{\R^d}|t-a|^2 \dfrac{|f(t)|^2}{\|f\|_2^2} dt\right)^{1/2} \left(\int_{\R^d} |\omega-b|^2 \dfrac{|\hat{f}(\omega)|^2}{\|\hat{f}\|_2^2}d\omega\right)^{1/2} \geq \dfrac{d}{4\pi},
\end{equation*}
so we may directly assume that $f$ is normalized. In such a case, $|f|^2$ can be seen as a probability distribution. If these integrals are finite for some $a$ and $b$, through the same argument of time-frequency shift we already used, it is easy that they are always finite. Then, from a formal point of view, we can take the derivative with respect $a$ and $b$, thus obtaining that their minimum is achieved when
\begin{equation*}
	a = \bar{t} = \int_{\R^d} t |f(t)|^2 dt, \quad b = \bar{\omega} = \int_{\R^d} \omega |\hat{f}(\omega)|^2 d\omega,
\end{equation*}
which are the mean of $|f|^2$ and $|\hat{f}|^2$, respectively. In this case, previous integrals represent the standard deviation of $|f|^2$ and $|\hat{f}|^2$, which we indicate with $\Delta_x f$ and $\Delta_{\omega} f$. From an heuristic perspective, it is fair to believe that a function $|f|^2$ is mostly concentrated around its mean and that its standard deviation is a measure of how spread it is. In light of these arguments, Heisenberg's uncertainty principle can written as:
\begin{equation*}
	\Delta_x f \cdot \Delta_{\omega} f \geq \dfrac{d}{4\pi},
\end{equation*}
which is a quantification of the main point of an uncertainty principle, namely that a function and its Fourier cannot be simultaneously concentrated.
\section{Donoho-Stark's uncertainty principle}\label{section Donoho-Stark's UP}
As we saw, Heisenberg's uncertainty principle measures the concentration of a function in terms of the variance. However, this is not the only way concentration can be stated. In this section we present an uncertainty principle about the so-called \emph{essential support} of a function which, roughly speaking, is the set where a function has most of its energy.
\begin{defi}\label{epsilon-concetrated def}
	A function $f \in L^2(\R^d)$ is $\mathbf{\varepsilon}$-\textbf{concetrated} on a measurable set $T \subseteq \R^d$ for some $\varepsilon \in [0,1]$ if
	\begin{equation*}
		\left(\int_{T^c} |f(t)|^2 dt \right)^{1/2} \leq \varepsilon \| f \|_2,
	\end{equation*}
	where $T^c = \R^d \setminus T$ denotes the complement set of $T$.
\end{defi}
If $\varepsilon \leq \frac{1}{2}$, this tells us that most of the energy of $f$ is inside $T$. Therefore, in such case we may call $T$ the \emph{essential support} of $f$.
\begin{teo}[Donoho-Stark's uncertainty principle]\label{Donoho-Stark's uncertainty principle theorem}
	Let $f \in L^2(\R^d) \setminus \{0\}$, suppose that $f$ is $\varepsilon_T$-concentrated on $T \subseteq \R^d$ while $\hat{f}$ is $\varepsilon_{\Omega}$-concentrated on $\Omega \subseteq \R^d$. Then
	\begin{equation}\label{Donoho-Stark's uncertainty principle formula}
		|T| \, |\Omega| \geq (1 - \varepsilon_T - \varepsilon_{\Omega})^2
	\end{equation}
\end{teo}
\begin{proof}
	The result is trivial if $T$ or $\Omega$ have infinite measure. Hence we will suppose that they both have finite measure.\\
	Concentration can be stated in an equivalent way through projection operators introduced in Section \ref{Localization with projections section}, indeed:
	\begin{align*}
		&\left( \int_{T^c} |f(t)|^2 dt\right)^{1/2} = \|f - \chi_T f\|_2  = \|f - P_T f\|_2 \leq \varepsilon_{T} \|f\|_2,\\
		&\left( \int_{\Omega^c} |\hat{f}(\omega)|^2 d\omega\right)^{1/2} = \|\hat{f} - \chi_{\Omega} \hat{f} \|_2 = \|f - \F^{-1} (\chi_{\Omega} \hat{f}) \|_2 = \|f - Q_{\Omega}f \|_2 \leq \varepsilon_{\Omega} \|f\|_2.
	\end{align*}
	In Section \ref{Localization with projections section} we also noticed that $\|Q_{\Omega}\| \leq 1$, hence
	\begin{align*}
		\|f - Q_{\Omega}P_Tf \|_2 &= \|f - Q_{\Omega}f + Q_{\Omega}f - Q_{\Omega}P_T f \|_2 \leq \|f-Q_{\Omega}f \|_2 + \|Q_{\Omega}(f - P_T f) \|_2 \\
								  &\leq \|f-Q_{\Omega}f \|_2 + \|f - P_T f \|_2 \leq (\varepsilon_{\Omega} + \varepsilon_T)\|f\|_2,
	\end{align*}
	and consequently
	\begin{align*}
		&\|f\|_2 = \|f - Q_{\Omega}P_T f + Q_{\Omega}P_T f \|_2 \leq \|f - Q_{\Omega}P_T f\|_2 + \| Q_{\Omega}P_T f  \|_2\\
		\implies &\|Q_{\Omega}P_T f \|_2 \geq \|f\|_2 - \|f - Q_{\Omega}P_T f\|_2 \geq (1-\varepsilon_{\Omega} - \varepsilon_T)\|f\|_2.
	\end{align*}
	Thanks to Proposition \ref{projection operators are Hilbert-Schmidt} we know that $\| Q_{\Omega}P_T\|_{\mathrm{HS}} = \sqrt{|T| \, |\Omega|}$ and from Theorem \ref{Hilbert-Schmidt operators are compact and bounded} we know that $\|Q_{\Omega}P_T\| \leq \| Q_{\Omega}P_T\|_{\mathrm{HS}}$, therefore
	\begin{equation*}
		(1-\varepsilon_{\Omega} - \varepsilon_T)\|f\|_2 \leq \|Q_{\Omega}P_T f \|_2 \leq \sqrt{|T| \, |\Omega|} \|f\|_2.
	\end{equation*}
\end{proof}
Taking $\varepsilon = 0$ in \eqref{Donoho-Stark's uncertainty principle formula} gives us the following corollary.
\begin{cor}
	Let $f \in L^2(\R^d) \setminus \{0\}$, $\mathrm{supp}f \subseteq T$, $\mathrm{supp}\hat{f} \subseteq \Omega$. Then $|T| |\Omega| \geq 1$.
\end{cor}
Loosely speaking, this result is telling us that $f$ and $\hat{f}$ cannot concentrate too much energy in a small subset of the phase space.
\section{Lieb's inequality}\label{section Lieb's UP}
 Up to now we presented two uncertainty principles related to the Fourier transform. However, uncertainty principles can be stated for every time of time-frequency analysis. In this and in the following section we present some uncertainty principles for the STFT.

We start considering a weak form and then we will show how Lieb's inequality \eqref{Lieb's inequality} provides an uncertainty principle. Like for the Donoho-Stark's uncertainty principle, the notion of concentration is measured in terms of essential support. 
\begin{prop}\label{weak uncertainty principle for STFT}
	Let $f,\phi \in L^2(\R^d)$ normalized, $\Omega \subseteq \R^{2d}$ and $\varepsilon \in  [0,1]$. Suppose that
	\begin{equation*}
		\int_\Omega |\V_{\phi}f(x,\omega)|^2 dxd\omega \geq 1- \varepsilon.
	\end{equation*}
	Then $|\Omega| \geq 1- \varepsilon$.
\end{prop}
\begin{proof}
	From \eqref{STFT is bounded} we see that $|\V_{\phi}f(x,\omega)| \leq 1$ for all $(x,\omega) \in \R^{2d}$, therefore
	\begin{equation}\label{weak uncertainty principle for STFT formula}
		1-\varepsilon \leq \int_\Omega |\V_{\phi}f(x,\omega)|^2 dxd\omega \leq \|\V_{\phi}f\|_{\infty}^2 |\Omega| \leq |\Omega|.
	\end{equation}
\end{proof}

\begin{teo}[Lieb's inequality]\label{Lieb's uncertainty principle}
	Suppose that $\|f\|_2 = \|\phi\|_2 = 1$. If $\Omega \subseteq \R^{2d}$ and $\varepsilon \in [0,1]$ are such that
	\begin{equation*}
		\int_\Omega |\V_{\phi}f(x,\omega)|^2 dxd\omega \geq 1- \varepsilon.
	\end{equation*}
	Then
	\begin{equation*}\label{Lieb's uncertainty principle formula}
		|\Omega| \geq \sup_{p>2} (1-\varepsilon)^{\frac{p}{p-2}} \left(\dfrac{p}{2}\right)^{\frac{2d}{p-2}}.
	\end{equation*}
\end{teo}
\begin{proof}
	If $|\Omega| = \infty$ the result is trivial hence we can suppose that $\Omega$ has finite measure. It is sufficient to use H\"older's inequality with exponents $p/2$ and $(p/2)' = p/(p-2)$:
	\begin{align*}
		1-\varepsilon &\leq \int_\Omega |\V_{\phi}f(x,\omega)|^2 dxd\omega= \int_{\R^{2d}} |\V_{\phi}f(x,\omega)|^2 \chi_\Omega(x,\omega)dxd\omega  \\
					  \overset{\mathrm{H\"older}}&{\leq} \left(\int_{\R^{2d}} |\V_{\phi}f(x,\omega)|^{2 \frac{p}{2}} dxd\omega\right)^{\frac{2}{p}} \left(\int_{\R^{2d}} \chi_\Omega(x,\omega)^{\frac{p}{p-2}}dxd\omega\right)^{\frac{p-2}{p}} \\
					  \overset{\eqref{Lieb's inequality formula}}&{\leq} \left(\dfrac{2}{p}\right)^{\frac{2d}{p}} \| f \|_2^2\, \|g\|_2^2\, |\Omega|^{\frac{p-2}{p}} = \left(\dfrac{2}{p}\right)^{\frac{2d}{p}} |\Omega|^{\frac{p-2}{p}}.
	\end{align*}
	We point out that the use of H\"older's inequality is justified because $\Omega$ has finite measure and, since $\V_{\phi}f \in L^q(\R^{2d})$ for every $q \geq 2$, $|\V_{\phi}f|^2 \in L^q(\R^{2d})$ for every $q \geq 1$. Because this result holds for every $p > 2$, we can take the supremum over all possible $p$, which leads to \eqref{Lieb's uncertainty principle formula}.
\end{proof}


\section{Faber-Krahn Inequality for the STFT}\label{section Faber-Krahn inequality fot STFT}
Lieb's uncertainty principle and Lieb's inequality are general results for the STFT because they hold for every possible window $\phi \in L^2(\R^d)$. One may think that for specific choices of the window it is possible to obtain improved results. In this last section we present a recent result, due to Nicola and Tilli and presented in \cite{nicolatilli_fk}, about the STFT with Gaussian window $\varphi$ given by \eqref{gaussian normalized}. In this work, they considered the following variational problem: 
\begin{equation}\label{variational problem Faber-Krahn STFT}
	\max_{f \in L^2(\R^d) \setminus \{0\}} \dfrac{\int_{\Omega} |\V_{\varphi} f(x,\omega)|^2 dx d\omega}{\| f\|_2^2},
\end{equation}
where $\Omega \subset \R^{2d}$ is a measurable set with prescribed measure $s > 0$. Therefore, we are asking for the maximal energy of the STFT that can be trapped into a set of a prescribed measure $s$ and, possibly, which functions achieve the maximum. The problem is completely solved and the solution is presented in the following theorem.
\begin{teo}[Theorem 4.1 \cite{nicolatilli_norm}]\label{faberkrahn theorem}
	For every $f \in L^2(\R^d)$ such that $\|f\|_{L^2} = 1$ and every measurable subset $\Omega \subset \R^{2d}$ with finite measure we have
	\begin{equation}\label{bound STFT nicola-tilli}
		\int_{\Omega}  |\V_{\varphi} f(x,\omega)|^2 \dxdo \leq G(|\Omega|),
	\end{equation}
	where $G(s)$ is given by
	\begin{equation}\label{G}
		G(s) \coloneqq \int_0^s e^{\left(-d!\tau\right)^{1/d}} d\tau.
	\end{equation}
	Moreover, equality occurs if and only if $f$ is a Gaussian of the kind
	\begin{equation}\label{translated Gaussian}
		f(x) = c e^{2 \pi i  x \cdot \omega_0} \varphi(x-x_0) = c\, \pi(x_0, \omega_0) \varphi (x), \quad x \in \R^d
	\end{equation}
	for some unimodular $c \in \C$ and some $(x_0,\omega_0) \in  \R^{2d}$ and $\Omega$ is equivalent, in measure, to a ball of centre $(x_0,\omega_0)$.
\end{teo}
The proof of this theorem is non trivial since it requires some tools from geometric measure theory, such as the coarea formula and the isoperimetric inequality. However, it is worth mentioning that the very first step of the proof is rephrasing the problem in the Fock space introduced in \ref{section Fock Space and Bargmann transform}. Indeed, recalling the relation between the STFT with Gaussian window and the Bargmann transform \eqref{connection between Bargmann transform and STFT} and that the latter is an isometry from $L^2(\R^d)$ into $\Fock^2(\C^d)$ we have:
\begin{equation*}
	\dfrac{\int_{\Omega} |\V_{\varphi} f(x,\omega)|^2 dx d\omega }{\|f\|_2^2} = \dfrac{\int_{\Omega'} |\Barg f (z)|^2 e^{-\pi |z|^2}dz }{\|\Barg f\|_{\Fock^2}^2},
\end{equation*}
where $\Omega' = \{(x, \omega) : (x, -\omega) \in \Omega\}$. Since the Bargmann transform is an unitary operator, variational problem \eqref{variational problem Faber-Krahn STFT} can be rephrased in the following way:
\begin{equation*}
	\max_{F \in \Fock^2(\C^d) \setminus \{0\}} \dfrac{\int_{\Omega} |F(z)|^2 e^{-\pi |z|^2}dz}{\|F\|_{\Fock^2}^2}.
\end{equation*}
While, at first sight, this might just seem a rewriting of the problem, actually the presence of the Bargmann transform is crucial. It is clear that, for $F \in \Fock^2(\C^d)$, the quantity $\int_{\Omega} |F(z)|^2 e^{-\pi |z|^2} dz$ is maximized when $\Omega$ is the a super-level set of $|F(z)|^2 e^{-\pi |z|^2}$. Thus, it is natural to study the integral of $|F(z)|^2 e^{-\pi |z|^2}$ over its super-level sets and this is where regularity of functions in the Fock space comes into play.

For the sake of completeness, we mention that the Theorem in \cite{nicolatilli_fk} is presented in a slightly different way, namely:
\begin{equation*}
	\int_{\Omega}  |\V f(x,\omega)|^2 dx d\omega \leq \dfrac{\gamma\left(d, \pi(|\Omega|/\omega_{2d})^{1/d}\right)}{(d-1)!},
\end{equation*}
where $\omega_{2d}$ is the volume of the unit ball in $\R^{2d}$ and $\gamma$ is the lower incomplete gamma function. Recalling the definition of $\gamma$:
\begin{equation*}
	\gamma\left(d, \pi(|\Omega|/\omega_{2d})^{1/d}\right) = \int_0^{\pi(|\Omega|/\omega_{2d})^{1/d}} t^{d-1} e^{-t}dt 
\end{equation*}
and since $\omega_{2d} = \pi^d / d!$, through the change of variable $t^d = d! \tau$  one obtains \eqref{bound STFT nicola-tilli}.
\begin{remark}
	We notice that the numerator of \eqref{variational problem Faber-Krahn STFT} can be written also in the following way:
	\begin{equation}\label{Faber-Krahn for STFT for localization operators}
		\int_{\Omega} |\V_{\varphi} f(x,\omega)|^2 dx d\omega = \langle \chi_{\Omega} \V_{\varphi} f, \V_{\varphi} f \rangle = \langle \V_{\varphi}^* \chi_{\Omega} \V_{\varphi} f,f \rangle = \langle L_{\Omega,\varphi} f,f \rangle,
	\end{equation}
	where $L_{\Omega,\varphi}$ is the localization operator with weight $\chi_{\Omega}$. Therefore, taking the maximum for all possible $f \in L^2(\R^d) \setminus\{0\}$ such that $\|f\|_2=1$ leads to a bound for the norm of $L_{\Omega,\varphi}$ (to obtain the norm we have to take the maximum of $\langle L_{\Omega, \varphi}f, g \rangle$ for all possible $f$ and $g$ normalized). However, we know \emph{a posteriori} that the maximum is attained when $\Omega$ is a ball and $f$ is a Gaussian, both with the same centre $(x_0, \omega_0) \in \R^{2d}$. We mention that results we obtain in Section \ref{section spherically symmetric weights} for localization operators with spherically symmetric weights can be obtained also under the action of a time-frequency shift. Indeed, this case is considered in \cite{daubechies} and, as expected, the eigenfunctions of these shifted localization operators are  time-frequency shifted Hermite functions. So, if $f$ is a Gaussian it is an eigenfunction of $L_{\Omega, \varphi}$ and, in the end, the maximum of \eqref{Faber-Krahn for STFT for localization operators} is not only a bound for the norm of $L_{\Omega, \varphi}$ but it is the actual norm.
\end{remark}

Once Theorem \ref{faberkrahn theorem} has been established, arguing like previous section we immediately obtain an uncertainty principle, which is sharp.
\begin{cor}\label{nicola-tilli's uncertainty principle cor}
	Let $f \in L^2(\R^d)$ with $\|f\|_2 = 1$, $\Omega \subset \R^{2d}$ measurable, $\varepsilon \in [0,1)$ and suppose that
	\begin{equation*}
		\int_{\Omega} |\V_{\varphi}(x,\omega)|^2 dx d\omega \geq 1 - \varepsilon.
	\end{equation*}
	Then
	\begin{equation}\label{nicola-tilli's uncertainty principle formula}
		|\Omega| \geq G^{-1} (1-\varepsilon).
	\end{equation}
\end{cor}
We point out that $G$ is invertible since it is monotonically strictly increasing. Moreover, its image is $[0,1)$, therefore its inverse $G^{-1} : [0,1) \rightarrow [0,+\infty)$ is itself monotonically increasing. This implies that, letting $\varepsilon \rightarrow 0$ in \eqref{nicola-tilli's uncertainty principle formula}, which means that $\Omega$ contains more and more energy, we have $|\Omega| \rightarrow +\infty$. If we compare this with Lieb's uncertainty principle we immediately realize how strong this result is, since letting $\varepsilon=0$ in \eqref{Lieb's uncertainty principle formula} yields to:
\begin{equation*}
	|\Omega| \geq \sup_{p > 2} \left(\dfrac{p}{2}\right)^{\frac{2d}{p-2}},
\end{equation*}
which is a finite number.

In conclusion, we consider the special case $d=1$, when $G^{-1}$ can be actually computed. Indeed, in this case we have:
\begin{equation*}
	G(s)  = 1 - e^{-s} \implies G^{-1}(s) = \log \left(\dfrac{1}{1 - G(s)}\right),
\end{equation*}
therefore \eqref{nicola-tilli's uncertainty principle formula} becomes
\begin{equation*}
	|\Omega| \geq \log\left(\dfrac{1}{\varepsilon}\right).
\end{equation*}
Even if we did not remark it, we already obtain this bound in the case $\Omega$ is a ball. Indeed, in \ref{example localization on a disk} we obtained the following expression \eqref{bound norm localization on a disk}:
\begin{equation*}
	\int_{\mathdutchcal{B}_R} |\V_{\phi} f (x,\omega)|^2 dx d\omega \leq 1 - e^{-\pi R^2}.
\end{equation*}
If we suppose $\int_{\mathdutchcal{B}_R} |\V_{\varphi} f(x,\omega) |^2 dx d\omega \geq 1- \varepsilon$ for some $\varepsilon \in [0,1)$, we obtain that:
\begin{equation*}
	1-\varepsilon \leq 1 - e^{-\pi R^2} \implies \pi R^2 \geq \log\left(\dfrac{1}{\varepsilon}\right).
\end{equation*}
where $\pi R^2$ is exactly the measure of $\mathdutchcal{B}_R$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Maximal norm of localization operators: recent results}\label{chapter recent results}

Theorem \ref{faberkrahn theorem} is not only remarkable by itself, but it turns out to be a powerful tool in the study of the maximal norm of localization operator when the window of the STFT is a normalized Gaussian. We already mentioned that \ref{faberkrahn theorem} can be rephrased into a result for the norm of localization operators of the kind $L_{\Omega, \varphi}$, where $\Omega \subset \R^{2d}$ is a measurable set of prescribed measure. The latter condition can be seen as a constraint for the $L^1(\R^{2d})$ norm of $\chi_{\Omega}$. In light of this observation, we may think to consider an analogous problem where $\chi_{\Omega}$ is replaced by a generic weight function $F$ that satisfies an integrability, and possibly boundedness, condition. This last chapter is devoted to the study of this problem. We start presenting a result from Nicola and Tilli \cite{nicolatilli_norm} where $F$ is chosen under an $L^p$ and $L^{\infty}$ constraint. Then, we consider a more generic case where the $L^{\infty}$ constraint is replaced by a $L^q$ one.

\section{Results from Nicola-Tilli}\label{section norm of localization operators}
In this section we show the results in \cite{nicolatilli_norm}. The aforementioned problem can be precisely stated as follows: find the optimal constant $C > 0$ such that:
\begin{equation}\label{sharp estimate Nicola-Tilli}
	\|L_{F, \varphi}\|_{L^2(\R^d) \rightarrow L^2(\R^d)} \leq C,
\end{equation}
where $F$ satisfies the following constraints:
\begin{equation}\label{constraints Nicola-Tilli}
	\|F\|_{\infty} \leq A \quad \text{and} \quad \|F\|_p \leq B.
\end{equation}
Clearly the constant $C$ will depend on $p$, $A$ and $B$. In \cite{nicolatilli_norm} this problem is completely solved: the constant $C$ is computed (explicitly in some cases), weight functions $F$ which achieve this bound are explicitly found and also function $f$ and $g$ such that $|\langle L_{F, \varphi} f, g \rangle| = \|L_{F, \varphi}\| = C$ are found. Before reporting the main Theorem of \cite{nicolatilli_norm}, we define the following number which will appear many times:
\begin{equation}\label{kappa_p}
	\kappa_p \coloneqq \dfrac{p-1}{p}.
\end{equation}
Moreover, for the sake of brevity, we denote the variable $(x,\omega) \in \R^{2d}$ as $z$ and therefore $dxd\omega$ as $dz$.
\begin{teo}\label{Nicola Tilli norm theorem}
	Assume $p \in [1,\infty)$, $A \in (0, \infty]$ and $B \in (0,\infty)$ with the additional condition that $A < \infty$ when $p=1$. Let $F$ satisfy the constraints in \eqref{constraints Nicola-Tilli}.
	\begin{enumerate}[label=(\roman*)]
		\item If $p=1$, then
		\begin{equation}\label{Nicola-Tilli bound p=1}
			\|L_{F, \varphi}\| \leq A\;G(B/A),
		\end{equation}
		and equality occurs if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$
		\begin{equation}\label{Nicola-Tilli maximal function p=1}
			F(z) = A e^{i\theta} \chi_{\mathdutchcal{B}} (z-z_0) \quad \forall z \in \R^{2d}
		\end{equation}
		where $\mathdutchcal{B} \subset \R^{2d}$ is the ball of measure $B/A$ centred at the origin.
		
		\item If $p>1$ and $B/A \leq \kappa_p^{d/p}$, then
		\begin{equation}\label{Nicola-Tilli bound p>1 first regime}
			\|L_{F, \varphi}\| \leq \kappa_p^{d \kappa_p}B,
		\end{equation}
		with equality if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$,
		\begin{equation}\label{Nicola-Tilli maximal function p>1 first regime}
			F(z) = e^{i \theta} \lambda e^{-\frac{\pi}{p-1}|z-z_0|^2} \quad \forall z \in \R^{2d},
		\end{equation}
		where $\lambda = \kappa_p^{-d/p}B$.\label{Nicola-Tilli norm theorem case 2}	
		\item If $p>1$ and $B/A > \kappa_p^{d/p}$, then
		\begin{equation}\label{Nicola-Tilli bound p>1 second regime}
			\|L_{F, \varphi}\| \leq \int_{0}^{A} G(u_{\lambda}(t))dt,
		\end{equation}
		where $u_{\lambda}(t) = \left[-\log\left(\left(t/\lambda\right)^{p-1}\right) \right]^d$ and $\lambda>A$ is uniquely determined by the condition $p\int_{0}^{A} t^{p-1}u_{\lambda}(t)dt = B^p$. Equality in \eqref{Nicola-Tilli bound p>1 second regime} is achieved if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$,
		\begin{equation}\label{Nicola-Tilli maximal function p>1 second regime}
			F(z) = e^{i\theta} \min \{ \lambda e^{-\frac{\pi}{p-1}|z-z_0|^2}, A \}
		\end{equation}
	\end{enumerate}
	Finally, in all the cases, condition $|\langle L_{F, \varphi} f,g \rangle| = \|L_{F, \varphi}\|$ holds for some, $f,g \in L^2(\R^d)$ such that $\|f\|_2 = \|g\|_2 = 1$, if and only if both $f$ and $g$ are of the kind \eqref{translated Gaussian}, possibly with different $c$'s, but with the same $(x_0,\omega_0) \in \R^{2d}$ which coincides with the centre of $F$.
\end{teo}
We will not give the proof of these results since some of its parts are similar to the one we will see in the following section. Moreover, we point out that the case $A=
\infty$ means we are dropping the $L^{\infty}$ constraint. 

We shall briefly comment this theorem. Except for the case $p=1$, when maximal weight functions are just characteristic functions of a ball, we see that two regime arise. In the first one, when $B/A \leq \kappa_p^{d/p}$ (which means that $A$ is ``sufficiently'' big compared to $B$), maximal weight functions are Gaussians. We already saw that Gaussian arise naturally has minimizers of uncertainty principles, so this result is not unexpected. However, in the second regime, when $B/A > \kappa_p^{d/p}$, maximal weight functions are not Gaussians but \emph{Gaussians truncated above}. As pointed out in \cite{nicolatilli_norm}, this seems to be a new phenomenon in time-frequency analysis.

\section{New results in presence of two $L^p$ constraints}
In this section we will deal with a generalized version of the problem considered in \cite{nicolatilli_norm}. Indeed, we want to find the optimal constant $C$ such that
\begin{equation*}
	\| L_{F, \varphi} \|_{L^2(\R^d) \rightarrow L^2(\R^d)} \leq C
\end{equation*}
under the following constraints on $F$:
\begin{equation}\label{constraints generic case}
	\|F \|_p \leq A \quad \text{and} \quad \|F\|_q \leq B,
\end{equation}
where $p,q \in (1,\infty)$ and $A,B \in (0,\infty)$. Before presenting the main theorem of this section we introduce the following notation
\begin{equation*}
	\Log(x) = \max\{-\log(x),0\}.
\end{equation*}
\begin{teo}\label{main theorem}
	Assume $p,q \in (1,+\infty)$, $A,B \in (0, +\infty)$ and suppose that $F$ satisfies constraints \eqref{constraints generic case}. Then
	\begin{enumerate}[label=(\roman*)]
		\item\label{main theorem i} If $B/A \geq \kappa_p^{d(\frac{1}{q}-\frac{1}{p})} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}$ $\left(\text{respectively\ } B/A \leq \kappa_q^{d(\frac{1}{q}-\frac{1}{p})} \left(\dfrac{p}{q}\right)^{\frac{d}{p}}\right)$, then:
		\begin{equation*}
			\|L_{F,\varphi}\| \leq \kappa_p^{d \kappa_p} A \quad (\text{resp.\ } \|L_{F,\varphi}\| \leq \kappa_q^{d \kappa_q} B),
		\end{equation*}
		with equality if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$,
		\begin{equation*}
			F(z) = e^{i \theta} \lambda e^{-\frac{\pi}{p-1}|z-z_0|^2} \quad (\text{resp.\ } F(z) = e^{i \theta} \lambda e^{-\frac{\pi}{q-1}|z-z_0|^2}),
		\end{equation*}
		where $\lambda = \kappa_p^{-d/p}A$ (resp. $\lambda = \kappa_q^{-d/q}B$).
		\item\label{main theorem ii} If $\kappa_q^{d(\frac{1}{q}-\frac{1}{p})} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < B/A < \kappa_p^{d(\frac{1}{q}-\frac{1}{p})} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}$, then
		\begin{equation}\label{norm estimate generic case}
			\|L_{F,\varphi}\| \leq \int_0^{+\infty} G(u(t)) dt,
		\end{equation}
		where $u(t) = \frac{1}{d!} \left[ \Log (\lambda_1 t^{p-1} + \lambda_2 t^{q-1})\right]^d$ and $\lambda_1, \lambda_2 > 0$ are uniquely determined by
		\begin{equation*}
			p \int_0^{+\infty} t^{p-1} u(t) dt = A^p, \quad q \int_0^{+\infty} t^{q-1} u(t) dt = B^q.
		\end{equation*}
		Moreover, letting $T > 0$ the unique value such that $\lambda_1 T^{p-1} + \lambda_2 T^{q-1} = 1$, the function $t \mapsto -\log(\lambda_1 t^{p-1} + \lambda_2 t^{q-1})$ defined on $(0,T]$ is invertible and we denote by $\psi : [0, +\infty) \rightarrow (0,T]$ its inverse. Then, equality in \eqref{norm estimate generic case} is achieved if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$, $F(z) = e^{i \theta} \psi(\pi |z-z_0|^2)$.
	\end{enumerate}
	Finally, in every case, equality in $|\langle L_{F, \varphi} f, g \rangle| = \|L_{F. \varphi}\|$ is achieved if and only if $f$ and $g$ are both of the kind \eqref{translated Gaussian}, possibly with different $c$'s but same $(x_0, \omega_0) \in \R^{2d}$.
\end{teo}
We point out that in, in the second regime, it is not possible to find an explicit expression for $C$ and $F$, although they can be computed numerically.

We split the proof of the theorem in several steps and we start proving the first statement, which explains how these different regimes arise.
\begin{proof}[Proof of Theorem \ref{main theorem}\ref{main theorem i}]
We consider just the first version, since the other one can be obtained swapping $p$ and $q$.

Theorem \ref{Nicola Tilli norm theorem}\ref{Nicola-Tilli norm theorem case 2} includes the case when $F$ satisfies just an $L^p$ constraint by taking $A$ ($L^{\infty}$ constraint) equal to $\infty$. In the current setting we have an $L^p$ and an $L^q$ bound, hence, thanks to \eqref{Nicola-Tilli bound p>1 first regime}, it is straightforward to see that
\begin{equation*}
	\| L_{F, \varphi}\| \leq \min\{\kappa_p^{d\kappa_p}A, \, \kappa_q^{d\kappa_q}B\}.
\end{equation*}
Suppose that the first term is smaller than the second, which means:
\begin{equation}\label{condition B/A first}
	\kappa_p^{d\kappa_p}A \leq \kappa_q^{d\kappa_q}B \iff \dfrac{B}{A} \geq \left(\dfrac{\kappa_p^{\kappa_p}}{\kappa_q^{\kappa_q}}\right)^d.
\end{equation}
Clearly, for $B$ sufficiently large we expect that the solution of current problem is the same as the one with just an $L^p$ constraint, namely the one given by \eqref{Nicola-Tilli maximal function p>1 first regime}. Therefore, we want to compare its $L^q$ norm with the bound given by $B$:
\begin{align*}
	\| F \|_q^q &= \int_{\R^{2d}} |F(z)|^q dz = \lambda^q \int_{\R^{2d}} e^{-\frac{q\pi}{p-1}|z-z_0|^2} dz \\
	\overset{z' = \left(\frac{q\pi}{p-1}\right)^{1/2}(z-z_0)}&{=} \lambda^q \left(\dfrac{p-1}{q\pi}\right)^d \int_{\R^{2d}} e^{-|z'|^2}dz' = \lambda^q \left(\dfrac{p-1}{q\pi}\right)^d \pi^d = \lambda^q \left(\dfrac{p-1}{q}\right)^d.
\end{align*}
Since we want $F$ to satisfy the $L^q$ constraint we should have
\begin{equation*}
	\lambda \left(\dfrac{p-1}{q}\right)^{d/q} \leq B \overset{\lambda = \kappa_p^{-d/p}A}{\implies} \left(\dfrac{p}{p-1}\right)^{d/p} \left(\dfrac{p-1}{q}\right)^{d/q}A \leq B,
\end{equation*}
which is equivalent to
\begin{equation}\label{condition B/A second}
	\dfrac{B}{A} \geq \kappa_p^{d\left(\frac1q - \frac1p\right)}\left(\dfrac{p}{q}\right)^{\frac{d}{q}}.
\end{equation}
If this condition is met, the solution with just the $L^p$ constraint is a solution also for the problem with two constraints. Moreover, from \ref{Nicola Tilli norm theorem} follows also the last part of the statement regarding those $f$ and $g$ that achieve equality in $|\langle L_{F,\varphi} f, g \rangle = \|L_{F,\varphi}\|$.
\end{proof}
If condition \eqref{condition B/A second} were less restrictive than condition \eqref{condition B/A first} we would have completely solved the problem. Unfortunately, this is not the case. Indeed it is always true, regardless of $p$ and $q$, that
\begin{equation}\label{curious inequality between conjugate exponents original}
	\kappa_p^{d\left(\frac1q - \frac1p\right)}\left(\dfrac{p}{q}\right)^{\frac{d}{q}} \geq \left(\dfrac{\kappa_p^{\kappa_p}}{\kappa_q^{\kappa_q}}\right)^d.
\end{equation}
The proof of this inequality can be found in \ref{curious inequality between conjugate exponents}.


To sum up, if $B/A \geq \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)}\left(\frac{p}{q}\right)^{\frac{d}{q}}$ or $B/A \leq \kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)}\left(\frac{p}{q}\right)^{\frac{d}{p}}$, the problem is already solved and the solution is given by Theorem \ref{Nicola Tilli norm theorem}. Therefore, from now on, we will considerc the intermediate case, that is:
\begin{equation}\label{intermediate regime}
	\kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < \dfrac{B}{A} < \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{q}},
\end{equation}
which corresponds to the statement of \ref{main theorem}\ref{main theorem ii}. We notice that the condition is well-posed, since it is actually true that
\begin{align}\label{another inequality between conjugate exponents original}
	\kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}
\end{align}
whenever $p \neq q$ (proof is in \ref{another inequality between conjugate exponents}).

The proof of this part of Theorem \ref{main theorem} is much more complex than the first one. The starting point is a Theorem from \cite{nicolatilli_norm} which gives a bound for $\| L_{F, \varphi} \|$ in terms of the distribution function of $|F|$.
\begin{teo}\label{norm limitation}
	Assume $F \in L^p(\R^{2d})$ for some $p \in [1,+\infty)$ and let $\mu(t) = |\{|F|>t\}|$ be the distribution function of $|F|$. Then
	\begin{equation}\label{norm limitation formula}
		\| L_{F, \varphi} \| \leq \int_0^{\infty} G(\mu(t))dt.
	\end{equation}
	Equality occurs if and only if $F(z)=e^{i\theta}\rho(|z-z_0|)$ for some $\theta \in \R$, $z_0 \in \R^{2d}$ and some nonincreasing function $\rho : [0,+\infty) \rightarrow [0,+\infty)$. In this case, it holds $|\langle L_{F,\varphi}f,g \rangle| = \|L_{F,\varphi}\|$ for some normalized Gaussians $f$ and $g$ of the kind \eqref{translated Gaussian}, possibly with different $c$'s but with same centre $(x_0,\omega_0) \in \R^{2d}$.
\end{teo}
\begin{proof}
	Let $f,g \in L^2(\R^d)$ such that $\| f \|_2 = \| g \|_2 = 1$.
	Since we are in a Hilbert space $\| L_{F, \varphi} \|$ can be computed as the supremum of $|\langle L_{F, \varphi} f,g \rangle|$ over all normalized $f$ and $g$. Therefore we are interested in estimating the previous scalar product:
	\begin{equation}\label{first inequality}
	\begin{aligned}
		| \langle L_{F, \varphi} f, g \rangle |  &= | \L_{F, \varphi}(f,g) | \leq \int_{\R^{2d}} |F(z)| \cdot |\V_{\varphi} f(z)| \cdot | \V_{\varphi} g(z) | dz  \\
									  \overset{\text{C-S}}&{\leq} \left( \int_{\R^{2d}} |F(z)| \cdot |\V_{\varphi} f(z)|^2 dz \right)^{1/2} \left( \int_{\R^{2d}} |F(z)| \cdot |\V_{\varphi} g(z)|^2 dz \right)^{1/2}.
	\end{aligned}
	\end{equation}
	Since the result is symmetric in $f$ and $g$ we can study just one of the terms. Letting $m = \esssup |F(z)|$ and assuming $m>0$ (otherwise every result is trivial) we can use the ``layer cake'' representation \cite[][Theorem 1.13]{liebloss}
	\begin{equation*}
		|F(z)| = \int_0^m \chi_{\{|F|>t\}}(z)dt 
	\end{equation*}
	in order to find
	\begin{align*}
		\int_{\R^{2d}} |F(z)| \cdot |\V_{\varphi} f(z)|^2 dz &= \int_{\R^{2d}} \left( \int_0^m \chi_{\{|F|>t\}}(z)dt \right) |\V_{\varphi} f(z)|^2 dz \\
											 \overset{\text{Tonelli}}&{=} \int_0^m \left( \int_{\R^{2d}} \chi_{\{|F|>t\}}(z) |\V_{\varphi} f(z)|^2 dz \right)dt \\
											 &= \int_0^m \left( \int_{\{|F|>t\}} |\V_{\varphi} f(z)|^2 dz \right)dt.
	\end{align*}
	We notice that the quantity in the inner integral is exactly the one in Theorem \ref{faberkrahn theorem}, hence
	\begin{equation}\label{limitation with G}
		\int_{\R^{2d}} |F(z)| \cdot |\V_{\varphi} f(z)|^2 dz \leq \int_0^m G\left(|\{|F|>t\}|\right)dt = \int_0^m G(\mu(t)) dt.
	\end{equation}
	We point out that since $\mu(t) = 0$ for $t > m$ and that $G(0)=0$, the previous expression is equivalent to \eqref{norm limitation formula}.
	
	Because $p<\infty$, from Proposition \ref{F in L^p L_F compact} we know that $L_F$ is a compact operator, therefore there exist normalized $f$ and $g$ which achieve equality in the supremum of the norm, namely $|\langle L_{F, \varphi} f, g \rangle| = \| L_{F, \varphi} \|$. Therefore, equality in \eqref{norm limitation formula} occurs if and only if all the previous inequalities become equalities. Equality in \eqref{limitation with G} occurs if and only if
	\begin{equation}\label{first equality}
		\int_{\{|F|>t\}} |\V_{\varphi} f(z)|^2 dz = G(\mu(t))
	\end{equation}
	for a.e. $t \in (0,m)$. Now, fix $t_0 \in (0,m)$ such that equality holds. From Theorem \ref{faberkrahn theorem} we can infer that $\{|F|>t_0\}$ is (equivalent to) a ball centred in $z_0 = (x_0,\omega_0)$ and that $f$ is a Gaussian of the kind \eqref{translated Gaussian} with the same centre $z_0$. Now that the centre of $f$ is fixed, still from Theorem \ref{faberkrahn theorem}, we obtain that equality in \eqref{first equality} a.e implies that also the other levels sets $\{|F|>t\}$ are equivalent to balls centred at the same $z_0$. Finally, we can extend the result to every $t \in (0,m)$ because $\{|F|>t\} = \bigcup_{s > t} \{|F|>s\}$. Since Theorem \ref{faberkrahn theorem} is a ``if and only if'', these conditions on $F$ and $f$ are also sufficient to guarantee equality in \eqref{limitation with G}. Clearly, the same result holds for $g$ which has to be a Gaussian, possibly with different coefficient $c$ but the same centre $z_0$.
	
	In the end, since all the super-level sets of $|F|$ are balls we conclude that it is spherically symmetric and radially decreasing, as claimed in the theorem's statement.
	
	Conditions for $f$ and $g$ imply that $\V_{\varphi} g  = e^{i \alpha} \V_{\varphi} f$ for some $\alpha \in \R$. This provides equality in \eqref{first inequality} when using Cauchy-Schwarz' inequality. Lastly we shall prove that also the first inequality in \eqref{first inequality}, that is
	\begin{equation*}
		\left\vert \int_{\R^{2d}} F(z) \V_{\varphi} f(z) \overline{\V_{\varphi} g(z)} dz \right\vert \leq \int_{\R^{2d}} |F(z)| \cdot | \V_{\varphi} f(z)| \cdot |\V_{\varphi} g(z)| dz
	\end{equation*}
	becomes an equality. With the additional information that $\V_{\varphi} g  = e^{i \alpha} \V_{\varphi} f$ this is equivalent to prove that:
	\begin{equation}\label{norm limitation equality 1}
		\left\vert \int_{\R^{2d}} F(z) |\V_{\varphi} f(z)|^2 dz \right\vert = \int_{\R^{2d}} |F(z)| \cdot | \V_{\varphi} f(z)|^2 dz.
	\end{equation}
	The integral on the left-hand side is just a complex number, therefore we have:
	\begin{equation*}
		\int_{\R^{2d}} F(z) |\V_{\varphi} f(z)|^2 dz = e^{i \theta} \left\vert \int_{\R^{2d}} F(z) |\V_{\varphi} f(z)|^2 dz \right\vert,
	\end{equation*}
	so \eqref{norm limitation equality 1} becomes
	\begin{equation*}
		\int_{\R^{2d}} e^{-i \theta} F(z) |\V_{\varphi} f(z)|^2 dz = \int_{\R^{2d}} |F(z)| \cdot | \V_{\varphi} f(z)|^2 dz.
	\end{equation*}
	The proof would be complete if $|\V_{\varphi} f(z)|^2$ was always positive, since this would imply $F(z) = e^{i \theta} |F(z)|$.
	In the proof of Theorem \ref{eigenvalues and eigenvectors localization operator spherically symmetric weight} we computed the STFT of Hermite functions. Since $\varphi = H_0$, letting $k = 0 \in \N_0^d$ into \eqref{STFT of Hermite functions} leads to:
	\begin{equation*}
		\V_{\varphi} \varphi (x, \omega) = e^{- \pi i \omega \cdot x} e^{- \pi (|x|^2 + |\omega|^2)/2}. 
	\end{equation*}
	In order to compute the STFT of $f = c \pi(x_0, \omega_0) \varphi$, all we have to do is to understand how $\V_{\varphi}$ interacts with a time-frequency shift:
	\begin{equation*}
		\V_{\varphi} f (x, \omega) = \langle c \pi(x_0, \omega_0) \varphi, \pi(x, \omega) \varphi \rangle = c \langle \varphi, T_{-x_0} M_{\omega - \omega_0} T_{x} \varphi \rangle.
	\end{equation*}
	Then:
	\begin{align*}
		T_{-x_0} M_{\omega - \omega_0} T_{x} \varphi(t) = e^{2 \pi i (\omega - \omega_0) \cdot (t + x_0)} f (t + x_0 - x) = e^{2 \pi i (\omega - \omega_0) \cdot x_0} M_{\omega - \omega_0} T_{x - x_0} \varphi(t),
	\end{align*}
	so that we obtain
	\begin{equation*}
		\V_{\varphi} f (x, \omega) = c e^{2 \pi i (\omega - \omega_0) \cdot x_0 } \langle \varphi, M_{\omega-\omega_0} T_{x-x_0} \varphi \rangle = c e^{2 \pi i (\omega - \omega_0) \cdot x_0 } \V_{\varphi} \varphi(x - x_0, \omega - \omega_0).
	\end{equation*}
	In the end, taking the modulus of both side we conclude that $|\V_{\varphi} f (x, \omega)|^2$ is always strictly positive, which concludes the proof.
\end{proof}

In light of the previous Theorem, it is natural to seek for a sharp upper bound for the right-hand side of \eqref{norm limitation formula}. Since this involves the distribution function $|F|$, we shall search this bound between all the possible distribution functions. In order to do so, we need to rephrase constraints \eqref{constraints generic case} in terms of $\mu$. This can be easily done thanks to a more general version of the ``layer cake'' representation (see \cite[][Theorem 1.13]{liebloss} or \cite[][Proposition 1.1.4]{grafakos}):
\begin{equation*}
	\|F\|_p^p = p \int_0^{\infty} t^{p-1}|\{|F|>t\}|dt.
\end{equation*}
Hence, constraints \eqref{constraints generic case} become
\begin{equation}\label{constraints generic case distribution function}
	p \int_0^{\infty} t^{p-1} u(t)dt \leq A^p \quad \text{and} \quad q \int_0^{\infty} t^{q-1} u(t)dt \leq B^q
\end{equation}
and we can define the proper space of possible distribution functions
\begin{equation}\label{distribution function space}
	\mathcal{C} = \{u : (0,+\infty) \rightarrow [0,+\infty) \text{ such that } u \text{ is decreasing and satisfies } \eqref{constraints generic case distribution function}\}.
\end{equation}
We have reached the point where our original question is rephrased in the following variational problem:
\begin{equation}\label{nonstandard variational problem formulation}
	\sup_{v \in \mathcal{C}} I(v) \quad \text{where} \quad I(v) \coloneqq \int_0^{+\infty} G(v(t))dt.
\end{equation}
Firstly, we shall prove existence of maximizers.
\begin{prop}\label{existence of maximizer}
	The supremum in \eqref{nonstandard variational problem formulation} is finite and it is attained by at least one function $u \in \mathcal{C}$. Moreover, every extremal function $u$ achieves equality in at least one of the constraints \eqref{constraints generic case distribution function}.
\end{prop}
\begin{proof}
	Considering, for example, the first constraint in \eqref{constraints generic case distribution function}, we see that
	\begin{equation*}
		t^p u(t) = p \int_0^t \tau^{p-1} u(t) d\tau \overset{u \text{ decreasing}}{\leq} p \int_0^t \tau^{p-1} u(\tau) d\tau \leq A^p,
	\end{equation*}
	hence functions in $\mathcal{C}$ are pointwise bounded by $A^p/t^p$. It is straightforward to verify that $G$ in \eqref{G} is increasing, that $G(s) \leq s$ and that $G(s) \leq 1$. Using these properties we have:
	\begin{align*}
		I(u) &= \int_0^{+\infty} G(u(t))dt = \int_0^1 G(u(t))dt + \int_1^{+\infty} G(u(t))dt \overset{G(s) \leq 1}{\leq} 1 + \int_1^{+\infty} G(u(t))dt \\
		     \overset{G \text{ increasing}}&{\leq} 1 + \int_1^{+\infty} G(A^p/t^p)dt \overset{G(s) \leq s}{\leq} 1 + \int_1^{+\infty} \dfrac{A^p}{t^p}dt < \infty,
	\end{align*}
	therefore the supremum in \eqref{nonstandard variational problem formulation} is finite.
	
	Let $\{u_n\}_{n \in \N} \subset \mathcal{C}$ be a maximizing sequence. Since every $u_n$ is pointwise bounded by $A^p/t^p$, thanks to Helly's selection theorem \ref{Helly's selection theorem} we can say that, up to a subsequence, $u_n$ converges pointwise to a decreasing function $u$. Moreover, $u$ is still in $\mathcal{C}$, indeed:
	\begin{equation*}
		\int_0^{+\infty} t^{p-1} u(t) = \int_0^{+\infty} \lim_{n \rightarrow \infty} t^{p-1} u_n(t) dt \overset{\text{Fatou's lemma}}{\leq} \liminf_{n \rightarrow \infty} \int_0^{+\infty} t^{p-1} u_n(t) dt \leq \dfrac{A^p}{p},
	\end{equation*}
	and clearly the same holds for $q$ instead of $p$.
	
	Now we have to prove that $u$ is actually achieving the supremum. We already saw that the following holds:
	\begin{equation*}
		|G(u_n(t))| \leq \chi_{(0,1)}(t) + \dfrac{A^p}{t^p} \chi_{(1,+\infty)}(t)
	\end{equation*}
	and that the left-hand side is a function in $L^1(0,+\infty)$. This allows us to use dominated convergence theorem to conclude that 
	\begin{equation*}
		I(u) = \int_0^{+\infty} G(u(t)) = \lim_{n \rightarrow \infty} \int_0^{\infty} G(u_n(t))dt = \lim_{n \rightarrow \infty} I(u_n) = \sup_{v \in \mathcal{C}} I(v).
	\end{equation*}

	Lastly, we need to show that $u$ achieves equality at least in one of the constraints \eqref{constraints generic case distribution function}. Suppose that this is not true. If we let $u_{\varepsilon}(t) = (1+\varepsilon) u(t)$, then for $\varepsilon > 0$ sufficiently small constraints are still satisfied and since $G$ is strictly increasing $I(u_{\varepsilon}) > I(u)$, which contradicts the hypothesis that $u$ is a maximizer.
\end{proof}
In order to do some ``meaningful'' calculus of variations we need to enlarge $\mathcal{C}$, because the monotonicity assumption is quite strict. We will show that removing this hypothesis leaves the supremum unchanged and that maximizers are indeed monotonic.
\begin{prop}\label{monotonicity of maximizer}
	Let $\allowdisplaybreaks[1]\mathcal{C}' = \{u : (0,+\infty) \rightarrow [0,+\infty)$ such that  $u$ is measurable and satisfies \eqref{constraints generic case distribution function}$\}$. Then
	\begin{equation}
		\sup_{v \in \mathcal{C}} I(v) = \sup_{v \in \mathcal{C}'} I(v).
	\end{equation}
	In particular, any function $u \in \mathcal{C}$ achieving the supremum on the left-hand side also achieves it on the right-hand side.
\end{prop}
\begin{proof}
	Let $u \in \mathcal{C}'$. We define its \emph{decreasing rearrangement} as:
	\begin{equation}
		u^*(s) = \sup\{t \geq 0 : |\{u>t\}|>s\},
	\end{equation}
	with the convention that $\sup \emptyset = 0$. It is clear from the definition that $u^*$ is a non-increasing function. Moreover, one can see (\cite[][Section 10.12]{hardy_littlewood_polya}, \cite[][Proposition 1.4.5]{grafakos}) that $u^*$ is right-continuous and that $u$ and $u^*$ are \emph{equi-measurable}, which means that they have the same distribution function. Moreover, we already pointed out that constraints \eqref{constraints generic case distribution function} imply that $u$ is pointwise bounded by $A^p/t^p$, therefore $u^*$ takes only finite values. Our aim is to show that $u^* \in \mathcal{C}$. Letting $\nu$ be the Radon measure with density $t^{p-1}$, we start proving that $\nu(\{u>s\}) \geq \nu(\{u^* > s\})$, indeed:
	\begin{align*}
		\nu(\{u>s\}) &= \int_{\{u>s\}} t^{p-1} dt \overset{t^{p-1}\; \mathrm{increasing}}{\geq} \int_0^{|\{u>s\}|} t^{p-1} dt  \\
					 \overset{\mathrm{equi-measurability}}&{=} \int_0^{|\{u^*>s\}|} t^{p-1} dt \overset{u^*\; \mathrm{decreasing}}{\underset{\mathrm{right-continuous}}{=}} \int_{\{u^*>s\}} t^{p-1} dt = \nu(\{u^* > s\}).
	\end{align*}
	Then, using one more time the ``layer cake'' representation:
	\begin{align*}
		\int_{0}^{+\infty} t^{p-1} u(t) dt &= \int_{0}^{+\infty} u(t) d\nu(t) = \int_{0}^{+\infty} \nu(\{u>s\}) ds \geq \\
										   &= \int_{0}^{+\infty} \nu(\{u^* > s\}) ds = \int_{0}^{+\infty} u^*(t) d\nu(t) = \int_{0}^{+\infty} t^{p-1} u^*(t) dt.
	\end{align*}
	If we swap $p$ with $q$ we conclude that $u^* \in \mathcal{C}$. Moreover, always from equi-measurability, we have:
	\begin{align*}
		I(u) &= \int_{0}^{+\infty} G(u(t)) dt = \int_{0}^{+\infty} \int_0^{u(t)} e^{-(d!\tau)^{1/d}} d\tau dt = \int_{0}^{+\infty} \int_{0}^{+\infty} \chi_{\{u>\tau\}}(t) e^{-(d!\tau)^{1/d}} d\tau dt \\
			 \overset{\mathrm{Tonelli}}&{=}  \int_{0}^{+\infty} |\{u > \tau\}| e^{-(d!\tau)^{1/d}} d\tau = \int_{0}^{+\infty} |\{u^* > \tau\}| e^{-(d!\tau)^{1/d}} d\tau = I(u^*).
	\end{align*}
	Taking the supremum over all possible $u \in \mathcal{C'}$ we have:
	\begin{align*}
		\sup_{v \in \mathcal{C}'} I(v)= \sup_{v \in \mathcal{C}'} I(v^*) \leq \sup_{v \in \mathcal{C}} I(v).
	\end{align*}
	Inequality $\sup_{v \in \mathcal{C}'} I(v) \geq \sup_{v \in \mathcal{C}} I(v)$ is trivial since $\mathcal{C}' \supset \mathcal{C}$.
	
\end{proof}
We are now in the position to find maximizers of \eqref{nonstandard variational problem formulation}.
\begin{teo}\label{nonstandard variational problem solution theorem}
	There exist a unique function $u \in \mathcal{C}$ achieving the supremum in \eqref{nonstandard variational problem formulation} that is:
	\begin{equation}\label{nonstandard variational problem solution formula}
		u(t) = \dfrac{1}{d!} \left[\Log\left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right)\right]^d, \quad t>0
	\end{equation}
	where $\lambda_1, \lambda_2$ are both positive and uniquely determined by
	\begin{equation*}
		p\int_0^{+\infty} t^{p-1}u(t)dt = A^p, \quad q\int_0^{+\infty} t^{q-1}u(t)dt = B^q.
	\end{equation*}
\end{teo}
\begin{proof}
We will split the proof in several parts. Firstly we will show that maximizers are given by \eqref{nonstandard variational problem solution formula}. Then we will show that multipliers $\lambda_1$ and $\lambda_2$ are both strictly positive and unique.\\
\textbf{Expression of maximizers}

Let $M = \sup\{t \in (0,+\infty)\ : u(t) > 0\}$. From Proposition \ref{existence of maximizer} we know that $u$ has to achieve at least one of the constraints, therefore $M>0$. Consider now a closed interval $[a,b] \subset (0,M)$ and a function $\eta \in L^{\infty}(0,M)$ supported in $[a,b]$. Without loss of generality we can suppose that $\eta$ is orthogonal, in the $L^2$ sense, to $t^{p-1}$ and $t^{q-1}$, explicitly
\begin{equation}\label{variations are orthogonal to constraints}
	\int_a^b t^{p-1} \eta(t) dt=0, \quad \int_a^b t^{q-1} \eta(t) dt=0.
\end{equation}
On $[a,b]$ we have that $u(t) \geq u(b) > 0$, hence, for $|\varepsilon|$ sufficiently small, $u+\varepsilon\eta$ is still a nonnegative function which satisfies \eqref{constraints generic case distribution function}, therefore $u+\varepsilon\eta \in \mathcal{C}'$. Since we are supposing that $u$ is a maximizer, the function $\varepsilon \mapsto I(u+\varepsilon\eta)$ has a maximum for $\varepsilon = 0$. Given that $\eta$ is supported in a compact interval we can differentiate under the integral sign and obtain
\begin{equation*}
	0 = \dfrac{d}{d\varepsilon}I(u+\varepsilon\eta) \lvert_{\varepsilon=0} = \int_a^b G'(u(t))\eta(t)dt.
\end{equation*}
We would like to extend this result to every $\eta$ in $L^2(a,b)$ satisfying \eqref{variations are orthogonal to constraints}. Since $L^{\infty}(a,b)$ is dense in $L^2(a,b)$, there exist a sequence $\{\eta_k\}_{k \in \N} \subset L^{\infty}(a,b)$ such that $\eta_k \rightarrow \eta$ in $L^2(a,b)$. We can consider the projection operator $P$ such that, given $\psi \in L^2(a,b)$, $P\psi$ is the orthogonal projection of $\psi$ onto $ X = \mathrm{span}\{t^{p-1},t^{q-1}\}^{\perp} \subset L^2(a,b)$. Considering $P$ is continuous we have that $P\eta_k \rightarrow P\eta = \eta$, hence, since $P\eta_k \in L^{\infty}(a,b)$:
\begin{equation*}
	0 = \int_a^b G'(u(t)) P\eta_k(t) dt = \langle G'(u), P\eta_n \rangle_{L^2(a,b)} \rightarrow \langle G'(u), \eta \rangle_{L^2(a,b)} = \int_a^b G'(u(t)) \eta(t) dt
\end{equation*}
namely
\begin{equation}\label{orthogonality of G'}
	\int_a^b G'(u(t)) \eta(t) dt = 0. 
\end{equation}
Since \eqref{orthogonality of G'} holds for every $\eta \in X$ it must be that 
\begin{equation*}
	G'(u) \in X^{\perp} = \left(\mathrm{span}\{t^{p-1},t^{q-1}\}^{\perp}\right)^{\perp} = \mathrm{span}\{t^{p-1},t^{q-1}\} \quad \text{in } (a,b).
\end{equation*}
Letting $a \rightarrow 0^+$ and $b \rightarrow M^-$ we then obtain
\begin{equation}\label{expression G'(u)}
	G'(u(t)) = \lambda_1 t^{p-1} + \lambda_2 t^{q-1} \quad \text{for a.e. } t \in (0,M)
\end{equation}
for some multipliers $\lambda_1,\lambda_2 \in \R$. Since $u$ is decreasing actually \eqref{expression G'(u)} holds for every $t \in (0,M)$. Finally, recalling the expression of \eqref{G} we see that $G'(s) = e^{-(d!s)^{1/d}}$. Since $u$ is monotonically decreasing we can invert \eqref{expression G'(u)} thus obtaining the explicit expression of maximizers:
\begin{equation}\label{expression u}
	u(t) = \begin{cases}
		\dfrac{1}{d!} \left[-\log\left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) \right]^d & t \in (0,M)\\
		0 & t \in (M,+\infty)
	\end{cases}
\end{equation}
We remark that a priori it was possible that $M=+\infty$, but from the explicit expression of maximizers we see that this is not possible since $u$ has to be nonnegative.\\
 \textbf{Maximizers achieve equality in both constraints and multipliers are non-zero}

The argument we used to determine the expression of maximizers enables us to say that these have to achieve equality in both constraints in \eqref{constraints generic case distribution function}. Indeed, if, for example, we had that $q \int_0^{\infty} t^{q-1}u(t)dt < B^q$, the second condition of orthogonality in \eqref{variations are orthogonal to constraints} could be removed, because for sufficiently small $\varepsilon$ a variation non-orthogonal to $t^{q-1}$ would be admissible. This would provide us the solution of the same variational problem but without the $L^q$ constraint. Since we are working in the intermediate case, we know that actually this solution does not satisfy the $L^q$ constraint, hence we conclude that $u$ has to achieve equality in both constraints. With the very same reasoning we can say that neither $\lambda_1$ nor $\lambda_2$ can be 0.\\
 \textbf{Multipliers are positive}

Suppose that one of the multipliers, for example $\lambda_2$, is negative. Consider an interval $[a,b] \subset (0,M)$ and a variation $\eta \in L^2(0,M)$ supported in $[a,b]$ and such that $\int_{a}^{b}t^{q-1}\eta(t)dt <0$. Then, $\eta$ is admissible and the directional derivative of $G$ at $u$ along $\eta$ is:
\begin{equation*}
	\int_{a}^{b} G'(u(t))\eta(t)dt = \int_{a}^{b} (\lambda_1 t^{p-1} + \lambda_2 t^{q-1})\eta(t)dt = \lambda_2 \int_{a}^{b} t^{q-1} \eta(t)dt > 0,
\end{equation*}
which contradicts the fact that $u$ is a maximizer.\\
 \textbf{$u$ is continuous}

Now that we now that both multipliers are positive we can prove that $u$ is continuous, which is equivalent to say that $M=T$, where $T$ is the unique positive number such that $\lambda_1 T^{p-1} + \lambda_2 T^{q-1} = 1$ (uniqueness of $T$ follows from the positivity of multipliers).

We start supposing that $M < T$, which means that $\lim_{t \rightarrow M^-} u(t) > 0$. Consider the following variation 
\begin{equation*}
	\eta(t) = \left\{
	\begin{aligned}
		-1 + \alpha \frac{t}{M} + \beta,\quad & t \in (M-M\delta,M)\\
		1,\quad					   & t \in (M,M+M\delta)\\
		0,\quad				   & \text{otherwise}
	\end{aligned}\right.
\end{equation*}
where $\delta>0$ is small enough so that $M-M\delta >0$ and $M+M\delta < T$, while $\alpha$ and $\beta$ are constants, depending on $\delta$, to be determined. Since we want this to be an admissible variation, we impose that $\eta$ is orthogonal to $t^{p-1}$ and $t^{q-1}$. For example, the first condition is:
\begin{align*}
	0 &= \int_{M-M\delta}^{M+M\delta} t^{p-1} \eta(t) dt = -\int_{M-M\delta}^M t^{p-1}dt + \int_{M-M\delta}^M t^{p-1}\left(\alpha \frac{t}{M} + \beta\right) dt + \int_M^{M+M\delta} t^{p-1}dt \\
	  \overset{\tau=t/M}&{=}M^p \int_{1-\delta}^1 \tau^{p-1}(\alpha \tau + \beta) d\tau - M^p \int_{1-\delta}^1 \tau^{p-1} d\tau + M^p \int_1^{1+\delta} \tau^{p-1} d\tau
\end{align*}
therefore, dividing by $\delta$:
\begin{align*}
	  \fint_{1-\delta}^1 \tau^{p-1}(\alpha \tau + \beta) d\tau = \alpha \fint_{1-\delta}^1 \tau^{p} d\tau + \beta \fint_{1-\delta}^1 \tau^{p-1} d\tau = \fint_{1-\delta}^1 \tau^{p-1} d\tau - \fint_1^{1+\delta} \tau^{p-1} d\tau.
\end{align*}
The equation stemming from the orthogonality with $t^{q-1}$ is analogous. Therefore, we obtained a nonhomogeneous linear system for $\alpha$ and $\beta$:
\begin{equation}\label{system continuity}
	\begin{pmatrix}
		 \fint_{1-\delta}^1 \tau^{p} d\tau &   \fint_{1-\delta}^1 \tau^{p-1} d\tau\\
		 \fint_{1-\delta}^1 \tau^{q} d\tau &   \fint_{1-\delta}^1 \tau^{q-1} d\tau
	\end{pmatrix}
	\begin{pmatrix}
		\alpha\\
		%\phantom{a} \\
		%\phantom{a} \\
		\beta
	\end{pmatrix}=
	\begin{pmatrix}
		\fint_{1-\delta}^1 \tau^{p-1} d\tau - \fint_1^{1+\delta} \tau^{p-1} d\tau\\
		\fint_{1-\delta}^1 \tau^{q-1} d\tau - \fint_1^{1+\delta} \tau^{q-1} d\tau
	\end{pmatrix}.
\end{equation}
This system has a unique solution if and only if the determinant of the matrix is not 0. We can show this directly:
\begin{align*}
	& \fint_{1-\delta}^1 \tau^{p} d\tau \fint_{1-\delta}^1 \tau^{q-1} d\tau - \fint_{1-\delta}^1 \tau^{q} d\tau \fint_{1-\delta}^1 \tau^{p-1} d\tau=\\
	&= \dfrac{1}{\delta^2} \int_{(1-\delta,1)^2} \left(\tau^p \sigma^{q-1} - \tau^{p-1}\sigma^q\right) d\tau d\sigma = \dfrac{1}{\delta^2} \int_{(1-\delta,1)^2} \tau^{p-1} \sigma^{q-1} \left( \tau - \sigma \right) d\tau d\sigma = \\
	&= \dfrac{1}{\delta^2} \left( \int_{Q_1} \tau^{p-1} \sigma^{q-1} \left( \tau - \sigma \right) d\tau d\sigma + \int_{Q_2} \tau^{p-1} \sigma^{q-1} \left( \tau - \sigma \right) d\tau d\sigma \right),
\end{align*}
where $Q_1=(1-\delta,1)^2 \cap \{\tau > \sigma\}$ and $Q_2=(1-\delta,1)^2 \cap \{\tau < \sigma\}$. In the second integral we can consider the change of variable that swaps $\tau$ and $\sigma$. In this case, the new domain is $Q_1$, hence:
\begin{align*}
	&\fint_{1-\delta}^1 \tau^{p} d\tau \fint_{1-\delta}^1 \tau^{q-1} d\tau - \fint_{1-\delta}^1 \tau^{q} d\tau \fint_{1-\delta}^1 \tau^{p-1} d\tau=\\
	&= \dfrac{1}{\delta^2} \int_{Q_1} \left(\tau^{p-1}\sigma^{q-1} - \tau^{q-1}\sigma^{p-1}\right) \left(\tau - \sigma\right) d\tau d\sigma.
\end{align*}
In $Q_1$ we have that $\tau - \sigma > 0$ and the sign of $\tau^{p-1}\sigma^{q-1} - \tau^{q-1}\sigma^{p-1}$ is constant, indeed:
\begin{equation*}
	\tau^{p-1}\sigma^{q-1} - \tau^{q-1}\sigma^{p-1} > 0 \iff \left(\dfrac{\tau}{\sigma}\right)^{p-q} > 1 \overset{\tau > \sigma}{\iff} p>q.
\end{equation*}
Therefore the determinant of the matrix is always not 0.

Now that we have an admissible variation, we can compute the directional derivative of $G$ along $\eta$. Since $u$ is supposed to be a maximizer, this derivative has to be nonpositive, therefore:
\begin{align*}
	0 &\geq \int_{M-M\delta}^{M+M\delta} G'(u(t))\eta(t)dt = -\int_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt\, +\\
	  &+ \int_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right)\left(\alpha \dfrac{t}{M}+\beta\right) dt + \int_M^{M+M\delta}dt = \\
	  &= -\int_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt + \lambda_1 M^p \int_{1-\delta}^1 t^{p-1}(\alpha t + \beta) dt +\\
	  &+ \lambda_2 M^q \int_{1-\delta}^1 t^{q-1}(\alpha t + \beta) dt + M\delta.
\end{align*}
Dividing by $M\delta$ and rearranging we obtain:
\begin{equation}\label{inequality continuity}
	\begin{aligned}
		\fint_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt \geq  1 &+ \lambda_1 M^{p-1} \fint_{1-\delta}^1 t^{p-1}(\alpha t + \beta) dt\\ &+ \lambda_2 M^{q-1} \fint_{1-\delta}^1 t^{q-1}(\alpha t + \beta) dt.
	\end{aligned}
\end{equation}
We notice that the last two terms are exactly the ones that appear in the orthogonality condition, therefore, to understand their behavior as $\delta$ approaches 0, we need to study the right-hand side of the system \eqref{system continuity}. If we expand the first component in the right-hand side of \eqref{system continuity} in its Taylor series with respect to $\delta$ we have:
\begin{align*}
	\left(1-\dfrac{p-1}{2}\delta + o(\delta) \right) - \left(1+\dfrac{p-1}{2}\delta + o(\delta) \right) = -(p-1)\delta + o(\delta)
\end{align*}
and similarly for the other component. If we let $\delta \rightarrow 0^+$ in \eqref{inequality continuity} we obtain
\begin{align*}
	\lambda_1 M^{p-1} + \lambda_2 M^{q-1} &= \lim_{\delta \rightarrow 0^+} \fint_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt \\
										  &\geq 1 + \lim_{\delta \rightarrow 0^+} \left[ \lambda_1 M^{p-1} \fint_{1-\delta}^1 t^{p-1}(\alpha t + \beta) dt + \lambda_2 M^{q-1} \fint_{1-\delta}^1 t^{q-1}(\alpha t + \beta) \right] \\
										  &= 1 + \lambda_1 M^{p-1} \lim_{\delta \rightarrow 0^+} \left[ -(p-1)\delta + o(\delta) \right] + \lambda_2 M^{q-1} \lim_{\delta \rightarrow 0^+} \left[ -(q-1)\delta + o(\delta) \right]\\
										  &= 1.
\end{align*}
The function $\lambda_1 t^{p-1}  + \lambda_2 t^{q-1}$ is strictly increasing because $\lambda_1$ and $\lambda_2$ are both positive, therefore this implies that $M \geq T$, which is absurd because we supposed that $M<T$. This allows us to write $u$ as in \eqref{nonstandard variational problem solution formula}.\\
 \textbf{Uniqueness of multipliers}

Lastly we shall prove that multipliers $\lambda_1, \lambda_2$, and hence maximizer, are unique. For this proof it is convenient to express $u$ in a slightly different way:
\begin{equation*}
	u(t) = \dfrac{1}{d!}\left[ \Log\left((c_1t)^{p-1} + (c_2t)^{q-1}\right) \right]^d.
\end{equation*} 
To emphasize that $u$ is parametrized by $c_1, c_2$ we write $u(t;c_1,c_2)$. If we let $u$ into \eqref{constraints generic case distribution function} we obtain the following functions of $c_1$ and $c_2$:
\begin{equation*}
	f(c_1,c_2) = p\ \int_0^T t^{p-1}u(t;c_1,c_2)dt, \quad g(c_1,c_2) = q\ \int_0^T t^{q-1}u(t;c_1,c_2)dt.
\end{equation*}
We want to highlight that, even if it is not explicit, also $T$ depends on $c_1$ and $c_2$. Nevertheless, these functions are differentiable since both $T$ and $u$ are differentiable with respect to $(c_1,c_2)$ and $t^{p-1}u$, $t^{q-1}u$ and their derivatives are bounded in $(0,T)$. 
Our maximizer $u$ satisfies the constraints only if $f(c_1,c_2)=A^p, \, g(c_1,c_2) = B^q$. Therefore, to prove uniqueness of the maximizer we need to show that level sets $\{f=A^p\}$ and $\{g=B^q\}$ intersect only in one point.

First of all we need to study endpoints, namely when one of $c_1$ or $c_2$ is 0. For example, if $c_2=0$:
\begin{align*}
	f(c_1,0) &= p\int_0^{1/{c_1}} t^{p-1}\dfrac{1}{d!}\left[-\log(c_1t)^{p-1}\right]^d dt \overset{\tau = c_1t}{=} \\
			 &= \dfrac{p(p-1)^d}{c_1^p d!}\int_0^1 \tau^{p-1}\left[-\log(\tau)\right]^d d\tau = \dfrac{\kappa_p^d}{c_1^p} = A^p \implies c_{1,f} = \dfrac{\kappa_p^{d/p}}{A}.
\end{align*}
The same can be done for $g$ and setting $c_1=0$ instead of $c_2 = 0$. Thus, we obtain four points:
\begin{equation*}
	c_{1,f} = \dfrac{\kappa_p^{d/p}}{A},\ c_{1,g} = \left(\dfrac{p-1}{q}\right)^{d/q}\dfrac1{B},\ c_{2,f} = \left(\dfrac{q-1}{p}\right)^{d/p}\dfrac1{A},\ c_{2,g} = \dfrac{\kappa_q^{d/q}}{B}.
\end{equation*}
In the regime we are considering one has that $c_{1,f} < c_{1,g}$ and $c_{2,f} > c_{2,g}$, indeed:
\begin{align*}
	&c_{1,f} < c_{1,g} \iff \dfrac{\kappa_p^{d/p}}{A} < \left(\dfrac{p-1}{q}\right)^{d/q}\dfrac1{B} \iff \dfrac{B}{A} < \kappa_p^{d\left( \frac1{q}-\frac1{p}\right)}\left(\dfrac{p}{q}\right)^{d/q},\\
	&c_{2,f} > c_{2,g} \iff \left(\dfrac{q-1}{p}\right)^{d/p}\dfrac1{A} > \dfrac{\kappa_q^{d/q}}{B} \iff \dfrac{B}{A} > \kappa_q^{d\left( \frac1{q}-\frac1{p}\right)}\left(\dfrac{p}{q}\right)^{d/p},
\end{align*}
which are exactly conditions in \eqref{intermediate regime}.
Because of this arrangement of these points we expect there is an intersection between level sets. Firstly we notice that, for every value $c_1 \in (0,c_{1,f})$, there exists a unique value of $c_2$ for which $f(c_1,c_2) = A^p$. Indeed, from previous computations we notice that $f(c_1,0)$ is a decreasing function hence $f(c_1,0) > A^p$, while $\lim_{c_2 \rightarrow +\infty} f(c_1,c_2) = 0$, therefore from the intermediate value theorem it follows that $f(c_1,c_2) = A^p$ for some $c_2$. The uniqueness of this value follows from strict monotonicity of $f(c_1,\cdot)$, indeed:
\begin{equation}\label{df/dc1}
	\pfrac{f}{c_1}(c_1,c_2) = -\dfrac{p(p-1)}{(d-1)!}c_1^{p-2}\ \int_0^T \dfrac{t^{2(p-1)}}{(c_1t)^{p-1}+(c_2t)^{q-1}} \left[ -\log\left((c_1t)^{p-1} + (c_2t)^{q-1}\right) \right]^{d-1}dt,
\end{equation}
is always strictly negative. We point out that the term $\frac{\partial T}{\partial c_1}(c_1,c_2)u(T;c_1,c_2)$, that should appear since $T$ depends on $c_1$, is 0 because $u$ is 0 in $T$. The same is true for $g$, therefore on the interval $(0,c_{1,f})$ the level sets of $f$ and $g$ can be seen as the graph of two functions $\varphi, \gamma$. Since $f$ and $g$ are both differentiable, from the implicit function theorem we have that $\varphi$ and  $\gamma$ are differentiable with respect to $c_1$.

After defining $\varphi$ and $\gamma$ we want to prove that $(\varphi-\gamma)' < 0$. Again by the implicit function theorem we have
\begin{equation*}
	\begin{split}
		\dfrac{d}{d c_1}(\varphi - \gamma)(c_1) = -\dfrac{\pfrac{f}{c_1}(c_1,\varphi(c_1))}{\pfrac{f}{c_2}(c_1,\varphi(c_1))}  + 
		\dfrac{\pfrac{g}{c_1}(c_1,\gamma(c_1))}{\pfrac{g}{c_2}(c_1,\gamma(c_1))} < 0 \iff \\
		\mathcal{I}(c_1) = \pfrac{f}{c_1}(c_1,\varphi(c_1)) \pfrac{g}{c_2}(c_1,\gamma(c_1)) - \pfrac{f}{c_2}(c_1,\varphi(c_1)) \pfrac{g}{c_1}(c_1,\gamma(c_1)) > 0
	\end{split}
\end{equation*}
As for \eqref{df/dc1} the other derivatives are computed. To simplify the notation we define 
\begin{equation*}
	h(t;c_1,c_2) = \frac{1}{(d-1)!}\frac{1}{(c_1t)^{p-1}+(c_2t)^{q-1}}\left[ -\log\left((c_1t)^{p-1} + (c_2t)^{q-1}\right) \right]^{d-1}.
\end{equation*}
From Fubini's theorem, we can write the product of the integrals as a double integral:
\begin{equation*}
	\begin{split}
		\mathcal{I}(c_1) &=p(p-1)q(q-1)c_1^{p-2}\gamma(c_1)^{q-2} \int_{[0,T]^2} h(t;c_1,\varphi(c_2)) h(s;c_1,\gamma(c_2)) t^{2(p-1)} s^{2(q-1)} dtds \\ &-p(q-1)q(p-1)c_1^{p-2}\varphi(c_1)^{q-2} \int_{[0,T]^2} h(t;c_1,\varphi(c_2)) h(s;c_1,\gamma(c_2)) t^{p+q-2} s^{p+q-2} dtds.
	\end{split}
\end{equation*}
When level sets intersect we have $\varphi(c_1)=\gamma(c_1)$. In this situation we can factorize the terms outside the integral and notice that the sign of $\mathcal{I}$ depends only on the sign of
\begin{equation*}
	\begin{split}
		&\int_{[0,T]^2} h(t;c_1,\varphi(c_1)) h(s;c_1,\gamma(c_1))\left( t^{2(p-1)} s^{2(q-1)} - t^{p+q-2} s^{p+q-2} \right)dtds \\
		=&\int_{[0,T]^2} h(t;c_1,\varphi(c_1)) h(s;c_1,\gamma(c_1)) t^{p-2}s^{q-2}\left( t^p s^q - t^q s^p \right)dtds.\\
	\end{split}
\end{equation*}
In order to simplify the notation once again, we set $H(t,s;c_1) = h(t;c_1,\varphi(c_1)) h(s;c_1,\gamma(c_1))$. Let $T_1 = [0,T]^2 \cap \{t>s\}$ and $T_2 = [0,T]^2 \cap \{t<s\}$. We can split the above integral in two parts:
\begin{equation*}
	\begin{split}
		\int_{T_1} H(t,s;c_1)t^{p-2}s^{q-2}\left( t^p s^q - t^q s^p \right)dtds + \int_{T_2} H(t,s;c_1)t^{p-2}s^{q-2}\left( t^p s^q - t^q s^p \right)dtds.
	\end{split}
\end{equation*}
Then, considering the change of variables that swaps $t$ and $s$, the domain of integration becomes $T_1$ and since $H$ is symmetric in $t$ and $s$, we have that the previous quantity is equal to
\begin{equation*}
	\begin{split}
		&\int_{T_1} H(t,s;c_1)\left( t^{p-2} s^{q-2} - t^{q-2} s^{p-2} \right) \left( t^p s^q - t^q s^p \right)dtds =\\
		&= \int_{T_1} H(t,s;c_1)\dfrac{1}{t^2 s^2}\left( t^p s^q - t^q s^p \right)^2 dtds,
	\end{split}
\end{equation*}
which is strictly positive.We are now in the position to prove the uniqueness of multipliers.

First of all, since $(\varphi-\gamma)'<0$ whenever $\varphi(c_1) = \gamma(c_1)$, we point out that for every point of intersection there exist $\delta>0$ such that $\varphi(t) > \gamma(t)$ for $t \in (c_1 - \delta, c_1)$ and $\varphi(t) < \gamma(t)$ for $t \in (c_1, c_1 + \delta)$.\\
Define $c_1^* \coloneqq \sup \{c_1 \in [0,c_{1,f}] : \forall t \in [0,c_1] \ \varphi(t) \geq \gamma(t)\}$. This is an intersection point between $\varphi$ and $\gamma$ (if $\varphi(c_1^*) > \gamma(c_1^*)$ due to continuity there would be $\varepsilon > 0$ such that $\varphi(c_1^*+\varepsilon) > \gamma(c_1^*+\varepsilon)$ which contradicts the definition of $c_1^*$) and it is the first one, because we saw that after every intersection point there is an interval where $\varphi < \gamma$. Lastly, since $\varphi(0) > \gamma(0)$ and $\varphi(c_{1,f}) = 0 < \gamma(c_1,f)$, we have that $0 < c_1^* < c_{1,f}$.\\
Suppose now that there is a second point of intersection $\tilde{c}_1$ after the first one. Since immediately after $c_1^*$ we have that $\varphi$ becomes smaller than $\gamma$, this second point of intersection is given by $\tilde{c}_1 = \sup \{c_1 \in [c_1^*,c_{1,f}] : \forall t \in [c_1^*,c_1] \ \varphi(t) \leq \gamma(t)\}$. Considering that this is an intersection point, there exists an interval before $\tilde{c}_1$ where $\varphi$ is strictly greater than $\gamma$ which is absurd, hence $c_1^*$ is the only intersection point between $\varphi$ and $\gamma$.\\
Therefore, the pair $(c_1^*, \varphi(c_1^*) = c_2^*)$ is the unique pair of multipliers for which 
\begin{equation*}
	p\int_0^T t^{p-1} u(t;c_1^*,c_2^*)dt = A^p, \quad q\int_0^T t^{q-1} u(t;c_1^*,c_2^*)dt = B^q
\end{equation*}
and, in the end, $u(t;c_1^*,c_2^*)$ is the unique maximizer for \eqref{nonstandard variational problem formulation}.
\end{proof}
We are now in the position to prove the second part of Theorem \eqref{main theorem}.
\begin{proof}[Proof of Theorem \ref{main theorem}\ref{main theorem ii}]
	We recall that from Theorem \ref{norm limitation} we have
	\begin{equation*}
		\|L_{F, \varphi} \| \leq \int_{0}^{+\infty} G(\mu(t))dt,
	\end{equation*}
	where $\mu(t) = |\{|F|>t\}|$ is the distribution function of $F$ and equality is achieved if and only if $F(z) = e^{i \theta} \rho(|z-z_0|)$ for some $\theta \in \R$, some $z_0 \in \R^{2d}$ and some non-increasing function $\rho : [0, +\infty) \rightarrow [0, +\infty)$. Then, Theorem \ref{nonstandard variational problem solution theorem} gives us a bound on the right-hand side, namely that:
	\begin{equation*}
		\int_0^{+\infty} G(\mu(t))dt \leq \int_0^{+\infty} G(u(t)) dt,
	\end{equation*}
	where $u$ is given by \eqref{nonstandard variational problem solution formula}. This is sufficient to prove \eqref{norm estimate generic case}. Then, from $u$ we can reconstruct $\rho$. If $F(z) = e^{i \theta} \rho (|z-z_0|)$, then its super-level sets are balls with centre $z_0$. Since $u$ is the distribution function of $F$, for $t \in (0,T)$ and some radius $r>0$ we have:
	\begin{equation*}
		\dfrac{\pi^d r^{2d}}{d!} = u(t) = \dfrac{1}{d!}\left[ - \log(\lambda_1 t^{p-1} + \lambda_2 t^{q-1})\right]^d,
	\end{equation*}
	where the left-hand side is the measure of a 2$d$-dimensional ball of radius $r$. If we simplify this expression we obtain:
	\begin{equation*}
		\pi r^2 = -\log(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}) \implies t = \psi(\pi r^2).
	\end{equation*}
	However, it is clear that $\rho(r) = t$, therefore $\rho(r) = \psi(\pi r^2)$.
\end{proof}
\appendix
\chapter{Unconditional convergence}
In some cases we had to deal with series over multiple indices. Formally, the possibility of manipulating these series is related to the property of \emph{unconditional convergence}.
\begin{defi}\label{unconditional convergence def}
	Let $\{x_j\}_{j \in \mathcal{J}}$ be a countable subset of a Banach space $X$. The series $\sum_{j \in \mathcal{J}} x_j$ is said to \textbf{converge unconditionally} to some $x \in X$ if, for every $\varepsilon > 0$, there exist a finite subset $J_0$ of $\mathcal{J}$ such that
	\begin{equation*}
		\| x - \sum_{j \in J} x_j \| \leq \varepsilon
	\end{equation*}
	for every finite set $J \supseteq J_0$.
\end{defi}

The notion of unconditional convergence is of crucial importance in cases where an exchange between an operator and a series or a certain order of summation is required. Here we present two results that are useful in such situations.

\begin{prop}\label{exchange of series and operator}
	Let $A \in \B(X,Y)$. If $\sum_{j \in \mathcal{J}} x_j$ converges unconditionally to $x$ in $X$, then $\sum_{j \in \mathcal{J}} Ax_j$ converges unconditionally to $Ax$ in $Y$.
\end{prop}
\begin{proof}
	Let $\varepsilon > 0$. By definition, there exist $J_0 \subseteq \mathcal{J}$ finite such that $\| x - \sum_{j \in J} x_j \|_X \leq \varepsilon$ for every finite set $J \supseteq J_0$. Then:
	\begin{align*}
		\|Ax - \sum_{j \in J} Ax_j \|_Y = \|A(x - \sum_{j \in J} x_j) \|_Y \leq \|A\| \|x - \sum_{j \in J} x_j \|_X < \|A\| \varepsilon,
	\end{align*}
	thus the series $\sum_{j \in \mathcal{J}} Ax_j$ converges unconditionally to $Ax$.
\end{proof}

\begin{prop}\label{exchange order double series}
	Suppose that $\sum_{(n,m) \in \N^2} x_{n,m}$ converges unconditionally to $x \in X$. Then the inner partial sum $s_{n,M} = \sum_{m=1}^{M} x_{n,m}$ converges to some $y_n \in X$ for every $n \in \N$ and $x = \sum_{n \in \N} y_n$ with unconditional convergence. Similarly, $\sum_{n=1}^{N} x_{n,m}$ converges to some $z_m \in X$ for every $m \in \N$ and $x = \sum_{m \in \N} z_m$.
\end{prop}
\begin{proof}
	Since $\sum_{(n,m) \in \N^2} x_{n,m}$ is unconditionally convergent to $x$, given $\varepsilon > 0$, by definition there exist $J_0 \subset \N^2$ such that $\|x - \sum_{(n,m) \in J} x_{n,m}\| < \varepsilon$ for every finite set $J$ containing $J_0$. Without loss of generality, we can suppose that $J_0$ is of the form $J_0 = \{(n,m) \in \N^2 : n \leq N_0,\, m \leq M_0\}$ for some $N_0,M_0 \in \N$. In such a way we have:
	\begin{equation}\label{exchange order double series estimate 1}
		\| x - \sum_{n=1}^N \sum_{m=1}^M x_{n,m} \| < \varepsilon
	\end{equation}
	for every $N \geq N_0$ and $M \geq M_0$. Consider now $I \subset \N$ finite and $M_1,M_2 \in \N$ such that $M_0 \leq M_1 < M_2$. Then:
	\begin{align*}
		\|\sum_{n \in I} s_{n,M_2} - \sum_{n \in I} s_{n,M_1}\| &= \| \sum_{n \in I} \sum_{m=1}^{M_2} x_{n,m} - \sum_{n \in I} \sum_{m=1}^{M_1} x_{n,m} \| = \| \sum_{n \in I} \sum_{m=M_1+1}^{M_2} x_{n,m}\| =\\
									  &= \| \sum_{n \in I} \sum_{m=M_1+1}^{M_2} x_{n,m} + \sum_{(n,m) \in J_0} x_{n,m} -\sum_{(n,m) \in J_0} x_{n,m} \|
	\end{align*}
	Letting $J = J_0 \cup (I \times \{M_1+1,\ldots,M_2\}) \subset \N^2$ and using triangular inequality we obtain:
	\begin{equation}\label{exchange order double series estimate 2}
		\|\sum_{n \in I} (s_{n,M_2} - s_{n,M_1})\| \leq \|x - \sum_{(n,m) \in J} x_{n,m} \| + \|x - \sum_{(n,m) \in J_0} x_{n,m} \| < 2\varepsilon
	\end{equation}
	because $J \supseteq J_0$. This proves that, for every $n \in \N$ and for every $I \subset \N$ finite, the sequence $\{ \sum_{n \in I} s_{n,M}\}_{M \in \N}$ is a Cauchy sequence in $X$ which is a Banach space, therefore it is convergent. In particular, taking $I = \{n\}$, we obtain that the sequence $\{s_{n,M}\}_{M \in \N}$ converges to some $y_n \in X$ for every $n \in \N$. Moreover, since $I$ is finite, we have that $\sum_{n \in I} s_{n,M} \overset{M \rightarrow +\infty}{\longrightarrow} \sum_{n \in I} y_n$.
	
	Now we have to show that $\sum_{n \in \N} y_n$ converges unconditionally to $x$. Consider $I \subset \N$ finite such that $\{1,\ldots,N_0\} \subseteq I$. First of all we notice that:
	\begin{equation*}
		\lim_{M_2 \rightarrow +\infty} \left( \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} s_{n,M_2}\right) = \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} y_n.
	\end{equation*}
	Therefore, taking the $\limsup_{M_2 \geq M_1}$ in \eqref{exchange order double series estimate 3} we obtain
	\begin{equation}\label{exchange order double series estimate 3}
		\| \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} y_n \| = \limsup_{M_2 \geq M_1} \|  \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} s_{n,M_2} \| \leq 2\varepsilon.
	\end{equation}
	Then we have:
	\begin{align*}
		\|x - \sum_{n \in I} y_n \| \leq \| x - \sum_{n \in I} s_{n,M_1} \| + \| \sum_{n \in I} s_{n,M_1} -  \sum_{n \in I} y_n \| \overset{\eqref{exchange order double series estimate 1} + \eqref{exchange order double series estimate 3}}{<} 3\varepsilon.
	\end{align*}

	The last part of the statement easily follows swapping $n$ and $m$ in previous computations.
\end{proof}


\chapter{Calculations}

\section{The constant in Lieb's inequality}\label{constant in Lieb's inequality calculation}
In the last part of the proof of Lieb's inequality \eqref{Lieb's inequality} we used the following equality $A_{p'}^d A_{2/p'}^{2d/p'}A_{(p/p')'}^{d/p'}=(2/p)^{d/p}$ without proving it. We recall that the Babenko-Bechner constant $A_p$ is given by:
\begin{equation*}
	A_p = \left(\dfrac{p^{1/p}}{p'^{1/p'}}\right)^{1/2}.
\end{equation*}
Since there is an exponent $1/2$ in the Babenko-Bechner constant it is better to compute the square of $A_{p'}^d A_{2/p'}^{2d/p'}A_{(p/p')'}^{d/p'}$. For the sake of clarity we are going to compute every single term and then we are going to multiply them.
\begin{itemize}
	\item $\displaystyle A_{p'}^2 = \dfrac{p'^{1/p'}}{p^{1/p}}$;
	\item In order to compute $A_{2/p'}^{2\cdot 2/p'}$ we start computing $(2/p')'$:
	\begin{equation*}
		\left(\dfrac{2}{p'}\right)' = \dfrac{2/p'}{2/p'-1} = \dfrac{2}{2-p'},
	\end{equation*}
	therefore
	\begin{align*}
		A_{2/p'}^{2\cdot 2/p'} &= \left[  \left(\dfrac{2}{p'}\right)^{p'/2} \left(\dfrac{2-p'}{2}\right)^{(2-p')/2}\right]^{2/p'} = \dfrac{2}{p'} \left(\dfrac{2-p'}{2}\right)^{(2-p')/(p')} = \\
							   &= \dfrac{2^{2(1-1/p')}}{p'}(2-p')^{2/p'-1} = \dfrac{2^{2/p}}{p'} (2-p')^{1/p'-1/p};
	\end{align*}
	\item Like the previous case, we start computing $(p/p')'$:
	\begin{equation*}
		\left(\dfrac{p}{p'}\right)' = \dfrac{p/p'}{p/p'-1} = \dfrac{p}{p-p'},
	\end{equation*}
	hence
	\begin{align*}
		A_{(p/p')'}^{2/p'} &= \left[ \left(\dfrac{p}{p-p'}\right)^{(p-p')/p} \left(\dfrac{p'}{p}\right)^{p'/p}\right]^{1/p'} = \left(\dfrac{p}{p-p'}\right)^{1/p' - 1/p} \left(\dfrac{p'}{p}\right)^{1/p} = \\
						   &= \left(1-\dfrac{p'}{p}\right)^{1/p-1/p'} \left(\dfrac{p'}{p}\right)^{1/p} = (2-p')^{1/p-1/p'}\left(\dfrac{p'}{p}\right)^{1/p}.
	\end{align*}
	We are now ready to calculate the product of the three constant:
	\begin{align*}
		A_{p'}^2 A_{2/p'}^{2 \cdot 2/p'} A_{(p/p')'}^{2/p'} &= \dfrac{p'^{1/p'}}{p^{1/p}} \dfrac{2^{2/p}}{p'} (2-p')^{1/p'-1/p} (2-p')^{1/p-1/p'}\left(\dfrac{p'}{p}\right)^{1/p} = \\
															&= 2^{2/p} p^{-2/p}p'^{1/p'+1/p-1} = \left(\dfrac{2}{p}\right)^{2/p}
	\end{align*}
	which is the desired result.
\end{itemize}



\section{A curious inequality between conjugate exponents}\label{curious inequality between conjugate exponents}
We consider inequality \eqref{curious inequality between conjugate exponents original}, which we rewrite for the sake of clarity:
\begin{equation*}
	\kappa_p^{d\left(\frac1q - \frac1p\right)}\left(\dfrac{p}{q}\right)^{\frac{d}{q}} \geq \left(\dfrac{\kappa_p^{\kappa_p}}{\kappa_q^{\kappa_q}}\right)^d
\end{equation*}
Firstly we want to restate in a more concise way. Recalling that $\kappa_p =\frac{1}{p'}$, where $p'$ is the conjugate exponent of $p$, we have:
\begin{align*}
	\left(\dfrac{1}{p'}\right)^{\frac{1}{q}-\frac{1}{p}}\left(\dfrac{p}{q}\right)^{\frac{1}{q}} \geq \left(\dfrac{1}{p'}\right)^{\frac{1}{p'}} \left(\dfrac{1}{q'}\right)^{-\frac{1}{q'}}\iff \left(\dfrac{1}{p'}\right)^{\frac{1}{q}-\frac{1}{p} - \frac{1}{p'}}\left(\dfrac{1}{q'}\right)^{\frac{1}{q'}} \left(\dfrac{p}{q}\right)^{\frac{1}{q}} \geq 1
\end{align*}
but, since $\frac{1}{p}+\frac{1}{p'}=1$ and $\frac{1}{q} - 1 = \frac{1}{q'}$, in conclusion we have:
\begin{equation*}
	\left(\dfrac{p'}{q'}\right)^{\frac1{q'}} \left(\dfrac{p}{q}\right)^{\frac{1}{q}} \geq 1
\end{equation*}
In order to prove that this inequality holds for every pair of $p,q>1$ we consider the left-hand side as function of $x=\frac{1}{p}$ and $y=\frac{1}{q}$ (therefore $\frac{1}{p'} = 1-x$ and $\frac{1}{q'} = 1-y$). If we take the logarithm of this quantity we want to show that:
	\begin{equation*}
		f(x,y) = (1-y)\left[\log(1-y) - \log(1-x)\right] + y\left[\log(y) - \log(x)\right] \geq 0.
	\end{equation*}
	The partial derivative of $f$ with respect to $x$ is:
	\begin{equation*}
		\pdfrac{f}{x}(x,y) = \dfrac{1-y}{1-x} - \dfrac{y}{x} = \dfrac{x-y}{x(1-x)}.
	\end{equation*}
	Since $x \in (0,1)$, $\pfrac{f}{x}(x,y)$ is negative for $x<y$ and positive for $x>y$, so $f$ has a minimum for $x=y$ where $f(x,x) = 0$.

\section{Another inequality between conjugate exponents}\label{another inequality between conjugate exponents}
We want to prove that inequality \eqref{another inequality between conjugate exponents original} holds, namely that:
\begin{align*}
	\kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}
\end{align*}
whenever $p \neq q$. As for the previous section, since $\kappa_p = \frac{1}{p'}$ and $\kappa_q = \frac{1}{q'}$ we can write the inequality in a more concise way:
\begin{align*}
	\left(\dfrac{1}{q'}\right)^{\frac{1}{q}-\frac{1}{p}} \left(\dfrac{p}{q}\right)^{\frac{1}{p}} < \left(\dfrac{1}{p'}\right)^{\frac{1}{q}-\frac{1}{p}} \left(\dfrac{p}{q}\right)^{\frac{1}{q}} \iff \left(\dfrac{q\phantom{'} \, p'}{q' \, p}\right)^{\frac{1}{q}-\frac{1}{p}} < 1 \iff \left(\dfrac{q-1}{p-1}\right)^{\frac{1}{q}-\frac{1}{p}} < 1.
\end{align*}
If $q>p$ the base is greater than 1 while the exponent is less than 1, whereas if $q<p$ the converse happens, which proves that inequality holds whenever $p \neq q$.

\chapter{Helly's selection theorem}
In this chapter we prove Helly's selection theorem, which was used in the proof of Proposition \ref{existence of maximizer}.
\begin{teo}\label{Helly's selection theorem}
	Let $\{f_n\}_{n \in \N}$ be a sequence of monotonically increasing functions on $\R$ and suppose that the sequence is uniformly bounded, namely that $f_n(x) \in [a, b]$ for every $n \in \N$, every $x \in \R$ and some $a<b$ in $\R$. Then, there exist a monotonically increasing function $f$ and a subsequence $\{f_{n_k}\}_{k \in \N}$ such that $f_{n_k}$ is pointwise convergent to $f$.
\end{teo}
\begin{proof}
	We begin by constructing $f$ over rational numbers. Let $\{q_1, q_2, \ldots\}$ be an enumeration of $\Q$.
	
	Since $\{f_n(q_1)\}_{n \in \N}$ is a bounded sequence in $\R$, it follows from Bolzano-Weiestrass' theorem that there exist $S_1 \subseteq \N$ countable such that $\{f_n(q_1)\}_{n \in S_1}$ is convergent to some $y_1 \in [a,b]$. Then, if we consider the sequence $\{f_n(q_2)\}_{n \in S_1}$, we see that this is a bounded sequence in $\R$ and again, from Bolzano-Weiestrass' theorem it follows that there exist $S_2 \subseteq S_1$ countable such that $\{f_n(q_2)\}_{n \in S_2}$ converges to some $y_2 \in [a,b]$. Repeating this argument lead to a family of countable subsets of $\N$ such that $S_1 \supseteq S_2 \supseteq \cdots$ such that $\{f_n(q_i)\}_{n \in S_i}$ converges to $y_i$.\\
	Consider now the set $S \subseteq \N$ built in the following way: the first element of $S$ is the first element of $S_1$, the second element of $S$ is the second element of $S_2$ and so on. Then, if we consider $q_i \in \Q$, we have that, up to the first $i-1$ terms, $\{f_n(q_i)\}_{n \in S}$ is a subsequence of $\{f_n(q_i)\}_{n \in S_i}$, therefore $\{f_n(q_i)\}_{n \in S}$ converges to $y_i$. Moreover, we notice that if $q_i < q_j$ then, by the monotonicity of $f_n$, we have $f_n(q_i) \leq f_n(q_j)$ for every $n \in S$. Taking the limit for $n \rightarrow +\infty$ we conclude that $y_i \leq y_j$.
	
	Up to now we built a subsequence of $\{f_n(q_1)\}_{n \in \N}$ that is pointwise convergent over rational numbers. Through the density of $\Q$ in $\R$ it is immediate to create a monotonically increasing function $g$ over $\R$. Indeed, if $q \in \Q$ we simply let $g(q) = y_q$, while if $r \in \R \setminus \Q$ we let $g(r) = \sup_{\substack{q \in \Q \\ q \leq r}} g(q)$. We point out that the supremum is finite because $y_q$ are bounded from above. Moreover, it is clear from the definition that $g$ is monotonically increasing.
	
	The proof is still not complete because, up to now, we only know that the subsequence $\{f_n\}_{n \in S}$ is pointwise converging to $g$ only over rational numbers. However, we can show that if $g$ is continuous in $r \in \R$, then $\lim_{\substack{n \rightarrow +\infty \\ n \in S}} f_n(x) = g(x)$. If $g$ is continuous in $r$, given $\varepsilon > 0$ there exist $\delta > 0$ such that $|g(y) - g(x)| < \varepsilon$ if $|y-x| < \delta$. In particular, we can pick $q_1, q_2 \in \Q$ such that $r-\delta < q_1 < r < q_2 < r+\delta$. Since $g$ is monotonically increasing this implies that $0 \leq g(r) - g(q_1) < \varepsilon$ and $0 \leq g(q_2) - g(r) < \varepsilon$. Moreover, for $n$ sufficiently large, we have that $|f_n(q_1) - g(q_1)| < \varepsilon$ and $|f_n(q_2) - g(q_2)| < \varepsilon$. These, together with the fact that $f_n$ are monotonically increasing leads to:
	\begin{equation*}
		\begin{gathered}
			f_n(q_1) \leq f(x) \leq f_n(q_2) \implies \\
			-2 \varepsilon < f_n(q_1) - g(q_1) + g(q_1) - g(x) \leq f_n(x) - g(x) \leq f_n(q_2) - g(q_2) + g(q_2) - g(x) < 2 \varepsilon,
		\end{gathered}
	\end{equation*}
	which means that $f_n(x)$ converges to $g(x)$.
	
	In order to conclude, we have to deal with the points where $g$ is not continuous. It is well known that a monotonic function on an open interval is continuous except possibly on a countable subset $J \subset \R$ (see \cite[][Section 6.1 Theorem 1]{royden} or \cite[][Theorem 4.30]{rudin_principles}). Since $J$ is at most countable we con consider an enumeration $\{j_1, j_2, \ldots\}$. If we repeat the same argument we used at the beginning of the proof, we see that there exist a countable subset $P \subseteq S$ such that $\{f_n\}_{n \in P}$ is pointwise converging to $g$ in $\R \setminus J$ and to some values in $J$. Taking $f$ as the limit function of $\{f_n\}_{n \in P}$ the proof is complete.
\end{proof}
\nocite{*}
\printbibliography


	

\end{document}32
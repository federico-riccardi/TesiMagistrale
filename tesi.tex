\documentclass[corpo=11pt, stile=classica, tipotesi=custom,
greek, evenboxes, english]{toptesi}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{hyperref}
\hypersetup{%
	pdfpagemode={UseOutlines},
	bookmarksopen,
	pdfstartview={FitH},
	colorlinks,
	linkcolor={blue},
	citecolor={blue},
	urlcolor={blue}
}

\usepackage{geometry} %for the margins
\newcommand\fillin[1][4cm]{\makebox[#1]{\dotfill}} %for the dotted line in the frontispiace

\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1} } %to vetical align numbers in tables, along the decimal dot

\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathrsfs}
%\usepackage{dutchcal}

\DeclareMathAlphabet{\mathdutchcal}{U}{dutchcal}{m}{n}
\SetMathAlphabet{\mathdutchcal}{bold}{U}{dutchcal}{b}{n}
\DeclareMathAlphabet{\mathdutchbcal}{U}{dutchcal}{b}{n}

\usepackage{esint}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{systeme}
\usepackage{mathtools}
\hypersetup{colorlinks, linkcolor=blue, citecolor=blue}
\usepackage{cancel}
\usepackage{color}
\usepackage[
backend=bibtex,
style=numeric,
sorting=nyt
]{biblatex}
\addbibresource{bibliografia.bib}


\numberwithin{equation}{chapter}
\newtheorem{teo}{Theorem}[chapter] %in questo modo la numerazione ricomincia da capo ad ogni nuovo capitolo
\newtheorem{defi}[teo]{Definition}
\newtheorem{lem}[teo]{Lemma}
\newtheorem{cor}[teo]{Corollary}
\newtheorem{prop}[teo]{Proposition}
\theoremstyle{definition}
\newtheorem{es}[teo]{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\R}{\mathbb{R}} %scorciatoia per R reali
\newcommand{\N}{\mathbb{N}} %scorciatoia per N naturali
\newcommand{\V}{\mathcal{V}} %STFT
\newcommand{\F}{\mathscr{F}} %Fourier transform
\newcommand{\Fock}{\mathcal{F}} %Fock space
\newcommand{\C}{\mathbb{C}} %Complex numbers
\newcommand{\B}{\mathscr{B}} %Bounded linear operators
\newcommand{\Barg}{\mathcal{B}} %Bargmann transform
\renewcommand{\L}{\mathscr{L}} %sesquilinear form localization operator
\newcommand{\dxdo}{dxd\omega}
\newcommand{\notazione}{\underline{\textbf{Remark Notazionale}}}
\newcommand{\Log}{\ensuremath{\mathrm{Log}_-}}
\newcommand{\finire}{\fbox{\LARGE DA FINIRE}}
\newcommand{\pdfrac}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator*{\esssup}{ess\,sup}

\begin{document}
\english
	
\input{./title.tex}

\tableofcontents

\ringraziamenti

\sommario

\chapter{Introduction}
{\color{blue} Con il blu indico frasi da mettere a posto o dove manca una citazione.}\\
{\color{red} Con il rosso indico frasi da controllare se sono corrette.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preliminaries}\label{chapter preliminaries}
In this first chapter we briefly recall some basics definition and results about functional analysis and Fourier transform. In section \ref{section basics of functional analysis} basic concepts about operators between Banach spaces are presented. In section \ref{section Fourier transform and its properties} Fourier transform is defined and essential properties are given.
\section{Basics of Functional Analysis}\label{section basics of functional analysis}
In this section we focus our attention on linear operators between Banach spaces. Across the section a generic Banach space will be denoted as $X$ (or $Y$) endowed with the norm $\| \cdot \|_X$. In case we will deal with an Hilbert space we will denote it with $H$ (or $K$) and its inner product as $\langle \cdot, \cdot \rangle_H$. {\color{red}Pedex in the norm and in the scalar product may be dropped in case there is no ambiguity}. {\color{red} Moreover, all the theory will be presented assuming we are dealing with infinite dimensional spaces but, if not otherwise stated, everything can be adapted almost directly for finite dimensional spaces.}

A generic linear operator between two Banach spaces $X$ and $Y$ will be denoted as $T : X \rightarrow Y$. As a standard notation, the image of $x \in X$ through $T$ will be indicated as $T(x)$, or equivalently as $Tx$.
\begin{defi}\label{bounded operator}
	A linear operator $T : X \rightarrow Y$ is \textbf{bounded} if there exist $C>0$ such that
	\begin{equation}\label{boundedness property}
		\| Tx \|_Y \leq C \| x \|_X \quad \forall x \in X
	\end{equation}
\end{defi}
For linear operator boundedness is strictly related to continuity as the subsequent theorem states.
\begin{teo}\label{equivalente boundedness continuity}
	For a linear operator $T$ the following statements are equivalent:
	\begin{itemize}
		\item $T$ is continuous
		\item $T$ is bounded.
	\end{itemize}
\end{teo}
We denote the set of linear bounded (continuous) operators from $X$ to $Y$ as $\B(X,Y)$, while if $X=Y$ we will just write $\B(X)$.\\
For the sake of completeness we mention that actually, for linear operators, boundedness is equivalent to uniform continuity.

After this we define the \emph{norm} of an operator
\begin{defi}\label{norm operator}
	Given a linear bounded operator $T$ we define its \textbf{norm} as the following number:
	\begin{equation*}
		\|T\| \coloneqq \inf\{C>0 : \| Tx \|_Y \leq C \| x \|_X \  \forall x \in X\} = \sup \left\{ \dfrac{\| Tx \|_Y}{\| x \|_X} : x \in X \setminus \{0\}\right\}
	\end{equation*}
\end{defi}
The proof of the equivalence between two definition is straightforward. We see that the norm of an operator is the best constant for which boundedness property \eqref{boundedness property} holds. {\color{blue} Sometimes, in order to emphasize the spaces between which $T$ operates, we may write the norm of $T$ as $\| T \|_{X \rightarrow Y}$.}

In the following we will mostly deal with $X$ and $Y$ being $L^2(\R^d)$, which is an Hilbert space. For operators between Hilbert spaces we can give the norm of an operator by means of the dual norm:
\begin{equation}\label{dual norm}
	\| T \|_H = \sup\{|\langle Tx, y \rangle_H| : x,y \in H,\,\ \|x\|_H = \|y\|_H = 1 \}
\end{equation}

An important class of operators is the class of \emph{compact operators}.
\begin{defi}\label{compact operator}
	A linear bounded operator $T$ is \textbf{compact} if for every bounded sequence $\{x_n\}_{n \in \N} \subset X$ the sequence of the images $\{Tx_n\}_{n \in \N} \subset Y$ has a converging subsequence.
\end{defi}
The property of compactness can be stated in multiple ways \textbf{SERVE SCRIVERLE?}.

Now we suppose $H$ and $K$ to be Hilbert spaces. Given $T \in \B(H,K)$ there exist a unique $T^* \in \B(K,H)$ such that:
\begin{equation*}
	\langle Tx, y \rangle_H = \langle x, T^* y \rangle_K \quad \forall x \in H,\,y \in K
\end{equation*}
$T^*$ is called the \textbf{adjoint} operator of $T$. In the particular case in which $T : H \rightarrow H$, if $T=T^*$, we say that $T$ is \textbf{self-adjoint}.

From now on we suppose that $X$ is over the field $\C$ and that $T \in \B(X)$.
\begin{defi}\label{spectrum def}
	The set $\sigma(T) = \{\lambda \in \C : T - \lambda I \text{ is not invertible}\}$ is called  the \textbf{spectrum} of $T$.
\end{defi}
For operators between finite-dimensional spaces (matrices) the spectrum is made up of \emph{eigenvalues}, those $\lambda \in \C$ such that $T-\lambda I$ is not injective. On the other hand, when dealing with infinite-dimensional spaces, this is no more true. Eigenvalues are in the so called \emph{point spectrum}, which in general is just a part of the whole spectrum.

If an operator is compact or self-adjoint its spectrum has some additional properties.
\begin{teo}[Fredholm's alternative]\label{Fredholm alternative}
	Let $T \in \B(X)$ be a compact operator. Then for $T-I$ one and only one of the following happens:
	\begin{itemize}
		\item $T$ is invertible
		\item $T$ is not injective
	\end{itemize}
\end{teo}
Therefore for compact operators, all the values in the spectrum, except at most for 0, are eigenvalues.\\
Another fundamental result arises if we study the spectrum of compact and self-adjoint operators.
\begin{teo}\label{self-adjoint compact operators are diagonalizable}
	Let $H$ be a separable Hilbert space and $T \in \B(H)$ a compact and self-adjoint operator. Then there exist an orthonormal basis $\{\psi_n\}_{n \in \N}$ with eigenvalues $\{\lambda_n\}_{n \in \N}$ of $H$ composed of eigenvectors of $T$. Moreover $\lim_{n \rightarrow +\infty} \lambda_n = 0$.
\end{teo}
Hence self-adjoint compact operators can always be diagonalized in some suitable basis (\cite{brezis}). 

From this Theorem immediately follows the next corollary, which relates the eigenvalues of a compact self-adjoint operator with its norm.
\begin{cor}\label{norm is greatest eigenvalue}
	Let $T \in \B(H)$ be a self-adjoint compact operator on a separable Hilbert space $H$ and suppose its eigenvalues are ordered in such a way that $|\lambda_1| \geq |\lambda_2| \geq \cdots$. Then $\|T\| = |\lambda_1|$.
\end{cor}

If an operator $T$ is compact but not self-adjoint we can easily construct an operator, related to $T$, that is compact and also self-adjoint, namely considering $T^*T$.
\begin{cor}\label{canonical form for compact operators corollary}
	Let $T \in \B(H)$ be a compact operator. There there exist orthonormal sets $\{\psi_n\}_{n \in \N}$ and $\{\phi\}_{n \in \N}$ and non-negative real numbers $\{\mu_n\}_{n \in \N}$ with $\lim_{n \rightarrow +\infty} \mu_n = 0$ so that
	\begin{equation}\label{canonical form for compact operators formula}
		T = \sum_{n=1}^{+\infty} \mu_n (\cdot , \psi_n)\phi_n
	\end{equation}
	where the series converges in norm. These $\mu_n$ are called \emph{singular values} of $T$ and are the square root of the eigenvalues of $T^*T$ or, equivalently, the eigenvalues of $|T|$.
\end{cor}

\subsection{Trace class and Hilbert-Schmidt operators}
Now we are going to consider two important classes of operators: \emph{trace-class} operators and \emph{Hilbert-Schmidt} operators.

The trace of an operator can be defined as it is for matrices.
\begin{defi}\label{non-negative operator}
	Let $H$ be an Hilbert space. An operator $T \in \B(H)$ is said \textbf{non-negative} if
	\begin{equation}\label{non-negative operator formula}
		\langle T x, x \rangle \geq 0 \quad \forall x \in H
	\end{equation}
\end{defi}
Condition \eqref{non-negative operator formula} is sufficient to show that, on Hilbert spaces over the field of complex numbers, non-negative operators are automatically self-adjoint. This result is an immediate corollary of the following form of the polarization identity.
\begin{prop}\label{polarization identity for sesquilinear forms}
	Let $\mathcal{S} : H \times H \rightarrow \C$ be a sesquilinear form over the complex Hilbert space $H$. Then, for every $x,y \in H$:
	\begin{equation}\label{polarization identity for sesquilinear forms formula}
		\mathcal{S}(x,y) = \dfrac{1}{4}\sum_{k=0}^3 i^k \mathcal{S}(x + i^k y, x + i^k y)
	\end{equation}
\end{prop}
\begin{proof}
	The proof follows from a direct computation of the right-hand side
	\begin{align*}
		&\sum_{k=0}^3 i^k \mathcal{S}(x + i^k y, x + i^k y) = \mathcal{S}(x,x)\sum_{k=0}^{3}i^k + \mathcal{S}(x,y) \sum_{k=0}^{3} i^k i^{-k} + \mathcal{S}(y,x) \sum_{k=0}^{3} i^k i^k+\\
		&+ \mathcal{S}(y,y)\sum_{k=0}^{3} i^k i^k i^{-k} = 4 \mathcal{S}(x,y) \implies \mathcal{S}(x,y) = \dfrac{1}{4}\sum_{k=0}^3 i^k \mathcal{S}(x + i^k y, x + i^k y).
	\end{align*}
\end{proof}
\begin{prop}\label{positive operators are self-adjoint}
	Let $T \in \B(H)$ be a non-negative operator over a complex Hilbert space $H$. Then $T$ is self-adjoint.
\end{prop}
\begin{proof}
	First of all we notice that if $T$ is non-negative, the quantity $\langle Tx,x \rangle$ is real, therefore $\langle Tx,x \rangle = \overline{\langle Tx,x \rangle} = \langle x,Tx \rangle$. Letting $\mathcal{T}(\cdot,\cdot) = \langle T \cdot, \cdot \rangle$, and using the polarization identity \eqref{polarization identity for sesquilinear forms formula}:
	\begin{align}
		\overline{\mathcal{T}(y,x)} &= \dfrac{1}{4}\sum_{k=0}^{3}  \overline{i^{k}\mathcal{T}(y + i^k x, y + i^k x)} \overset{T\ \mathrm{positive}}{=} \dfrac{1}{4}\sum_{k=0}^{3} i^{-k} \mathcal{T}(y + i^k x, y + i^k x) =\nonumber\\
									&= \dfrac{1}{4}\sum_{k=0}^{3} i^{-k} \mathcal{T}\left(i^k(x + i^{-k}y), i^k(x + i^{-k}y)\right) = \\
									&= \dfrac{1}{4}\sum_{k=0}^{3} i^{-k} \mathcal{T}(x + i^{-k}, x + i^{-k}y) = \mathcal{T}(x,y) \label{polarization identity}
	\end{align}
\end{proof}
{\color{blue}Non-negative operators are somewhat ``special'', since their behaviour resembles, in some sense, the one of complex numbers. In particular, one can define the \emph{square root} of non-negative operator. Before doing so, we need the following Lemma.}
\begin{lem}\label{power series of square root lemma}
	The power series of $\sqrt{1-z}$ about zero converges absolutely for all complex numbers such that $|z| \leq 1$.
\end{lem}
\begin{proof}
	The power series of $\sqrt{1-z}$ about the origin is given by
	\begin{equation*}
		\sqrt{1-z} = \sum_{n=0}^{+\infty}c_n z^n = 1 + \sum_{n=1}^{+\infty} (-1)^n\binom{1/2}{n} z^n
	\end{equation*}
	where the binomial $\binom{r}{n}$ is defined by $\frac{r(r-1)\cdots(r-n+1)}{n!}$ for every $r \in \R$ and every $n \in \N$, while if $n=0$ we have $\binom{r}{0}=1$. Since $\sqrt{1-z}$ is analytic for $|z|<1$, the series here converges absolutely, so now we have to consider the case $|z|=1$. We notice that $c_n < 0$ for every $n \geq 1$ because when $n$ is even $\binom{1/2}{n}$ is negative, while if $n$ is odd $\binom{1/2}{n}$ is positive, therefore
	\begin{align*}
		1-\sqrt{1-x} = \sum_{n=1}^{+\infty} (-c_n) x^n
	\end{align*}
	is a positive series. Using this fact, given $N \in \N$, we have
	\begin{align*}
		\sum_{n=0}^{N} |c_n| &= 1 + \sum_{n=1}^{N} (-c_n) = 1 + \lim_{x \rightarrow 1^-} \sum_{n=1}^{N} (-c_n)x^n \overset{\mathrm{positive\ series}}{\leq} \\
							 &\leq 1 + \lim_{x \rightarrow 1^-} \sum_{n=1}^{+\infty} (-c_n)x^n = 1 + \lim_{x \rightarrow 1^-}(1-\sqrt{1-x}) = 2.
	\end{align*}
	Since this holds for every $N$, taking the limit $N \rightarrow +\infty$ we obtain $\sum_{n=1}^{+\infty} |c_n| < \infty$, which means exactly that the power series is absolutely convergent.
\end{proof}
\begin{teo}\label{existence of square root}
	Let $T \in \B(X)$ be a non-negative operator. Then, there exist a unique non-negative operator $S \in \B(X)$ such that $S^2 = T$. Moreover $S$ commutes with all bounded operators commuting with $T$.
	
	We call $S$ the \textbf{square root} of $T$ and we denote it by $S = \sqrt{T}$.
\end{teo}
\begin{proof}
	If $T=0$ we let $\sqrt{T}=0$, otherwise we define $B = I - \|T\|^{-1}T$, where $I$ is the identity operator.
	Since $T$ is non-negative, for every $x \in X$ such that $\|x\|_X = 1$, we have
	\begin{align*}
		\langle Bx,x \rangle = \langle (I-\|T\|^{-1}T)x, x\rangle = \|x\|_X^2 - \|T\|^{-1}\langle Tx,x \rangle \leq \|x\|_X^2 = 1
	\end{align*}
	which implies, {\color{red} using polarization identity}, that $\|B\| \leq 1$. Thanks to Lemma \ref{power series of square root lemma}, this means that the series $\sum_{n=0}^{+\infty}c_n B^n$ is absolutely convergent, therefore convergent, in $\B(X)$ to an operator we indicate with $B_{1/2}$. We define
	\begin{equation}\label{square root of an operator}
		S = \|T\|^{1/2} B_{1/2}
	\end{equation}
	and we want to show that $S$ is non-negative and satisfies $S^2 = T$. We start proving that $B_{1/2}$, hence $S$, is non-negative. Taking $x \in X$, we have
	\begin{align*}
		\langle B_{1/2}x,x \rangle &= \langle (I + \sum_{n=1}^{+\infty} c_n B^n),x \rangle = \|x\|_X^2 + \sum_{n=1}^{+\infty} c_n\langle B^n x,x \rangle \geq \|x\|_X^2 + \sum_{n=1}^{+\infty} c_n \|B\|^n \|x\|_X^2 \geq\\
								   &\geq \|x\|_X^2 + \sum_{n=1}^{+\infty} c_n \|x\|_X^2 = 0
	\end{align*}
	where we used the fact that $c_n$ are negative, $\|B\| \leq 1$ and that $1 + \sum_{n=1}^{\infty}c_n = \sqrt{1-x}\rvert_{x=1} = 0$.
	
	Now we shall prove that $S^2 = T$, which means $\|T\|(B_{1/2})^2 = T$:
	\begin{align*}
		(B_{1/2})^2 = \left(\sum_{n=0}^{+\infty}c_n B^n\right) \left(\sum_{m=0}^{+\infty}c_m B^m\right) = \sum_{n=0}^{+\infty} \left(\sum_{m=0}^{n} c_m c_{n-m}\right) B^n = \sum_{n=0}^{+\infty} d_n B^n
	\end{align*}
	where the rearrangement is justified since all series are absolutely converging. In order to compute $d_n = \sum_{m=0}^{n} c_m c_{n-m}$, we notice the following:
	\begin{align*}
			1-x &= \sqrt{1-x} \sqrt{1-x} = \left(\sum_{n=0}^{+\infty}c_n x^n\right) \left(\sum_{m=0}^{+\infty}c_m x^m\right) = \sum_{n=0}^{+\infty} \left(\sum_{m=0}^{n} c_m c_{n-m}\right) x^n 
	\end{align*}
	which implies that $d_0=1$, $d_1 = -1$ and $d_n = 0$ for $n \geq 2$, therefore $B_{1/2} = I - B$ and, in the end:
	\begin{align*}
		S^2 = \|T\|(B_{1/2})^2 = \|T\|(I - B) = \|T\|(I - I + \|T\|^{-1}T) = T.
	\end{align*}
	Lastly, since the series that defines $B_{1/2}$, hence $S$, is absolutely convergent, it commutes with every bounded operator commuting with $T$.

	Up to now we only proved the existence of a square root for $T$, in particular the one given by the expression \eqref{square root of an operator}. To prove uniqueness of the square root we start supposing $S_0$ is another non-negative operator in $\B(X)$ such that $S_0^2 = T$. We notice that $S_0$ commutes with $T$, indeed $S_0 T = S_0 S_0^2 = S_0^2 S_0 = T S_0$. Therefore $S_0$ commutes also with $S$, thus we have:
	\begin{align*}
		(S-S_0)^2S + (S-S_0)^2S_0 &= (S-S_0)[(S-S_0)S + (S-S_0)S_0] =\\
								  &= (S-S_0)(S^2 - S_0S + SS_0 -S_0^2) =\\
								  &= (S-S_0)(S^2 - S_0^2) = (S-S_0)(T-T)=0.
	\end{align*}
	But $(S-S_0)^2 S$ and $(S-S_0)^2 S_0$ are non-negative operator, so they must vanish, in particular also their difference $(S-S_0)^2 S - (S-S_0)^ S_0 = (S-S_0)^3$ is zero. This implies $(S-S_0)^4 = (S-S_0) (S-S_0)^3 = 0$, but since both $S$ and $S_0$ are self-adjoint, for every $x \in H$ we have:
	\begin{align*}
		0 = \langle (S-S_0)^4 x,x \rangle = \langle (S-S_0)^2 u, (S-S_0)^2 u \rangle =  \|(S-S_0)^2 x\|^2 \implies (S-S_0)^2 = 0
	\end{align*}
	and with the same argument we conclude that also $S-S_0 = 0$.
\end{proof}
\begin{remark}
	From expression \eqref{square root of an operator} it is clear that if $T$ is compact then also $\sqrt{T}$, since it is a limit of compact operators.
\end{remark}
\begin{defi}\label{absolute value of an operator definition}
	Given an operator $T \in \B(X)$ we define its \textbf{absolute value} as
	\begin{equation}\label{absolute value of an operator formula}
		|T| \coloneqq \sqrt{T^*T}
	\end{equation}
\end{defi}
\begin{defi}\label{partial isometry}
	A linear operator $U \in \B(X)$ is a \textbf{partial isometry} if it is an isometry over $(\mathrm{Ker}\,U)^{\perp}$, i.e. $\|Ux\|_X = \|x\|_X$ for every $x \in (\mathrm{Ker}\,U)^{\perp}$
\end{defi}
\begin{prop}\label{U^* is a partial isometry}
	Let $U \in \B(H)$ be a partial isometry. Then also $U^*$ is a partial isometry.
\end{prop}
\begin{proof}
	{\color{red} Since $U$ is a partial isometry, from polarization identity it follows that $\langle Ux,Uy \rangle = \langle x,y \rangle$ for every $x,y \in (\mathrm{Ker}(U))^{\perp}$. Clearly the same equality holds if $x \in (\mathrm{Ker}(U))^{\perp}$ while $y \in \mathrm{Ker}(U)$, so $U^*U$ is the identity over $(\mathrm{Ker}(U))^{\perp}$. This implies that $U^*$ is an isometry onto $\overline{\mathrm{Im}(U)} = (\mathrm{Ker}(U^*))^{\perp}$, hence it is a partial isometry too.}
\end{proof}
\begin{teo}\label{polar decomposition theorem}
	Given $T \in \B(X)$ there exist unique a partial isometry $U$ such that
	\begin{equation}\label{polar decomposition formula}
		T = U|T|,
	\end{equation}
	which is uniquely determined by the condition $\mathrm{Ker}\,U = \mathrm{Ker}\,T$.
\end{teo}
\begin{proof}
	We start defining $\tilde{U} : \mathrm{Im}|T| \rightarrow \mathrm{Im}T$. Every $\phi \in \mathrm{Im}|T|$ can be written as $\phi = |T|\psi$ for some $\psi \in H$ and we define $\tilde{U}(\phi) = \tilde{U}(|T|\psi) \coloneqq T\psi$. We notice that	
	\begin{align*}
		\|\phi\|^2 &= \||T|\psi\|^2 = \langle |T|\psi, |T|\psi \rangle  = \langle |T|^2 \psi, \psi \rangle = \langle T^*T \psi, \psi \rangle = \|T \psi \|^2 = \|\tilde{U}\phi\|.
	\end{align*}
	This computation ensures us that the definition of $\tilde{U}$ is consistent (if $\phi = |T|\psi_1 = |T|\psi_2$ for some $\psi_1,\psi_2 \in H$, then $|T|(\psi_1 - \psi_2)=0$, but $\| |T|(\psi_1 - \psi_2)\| = \| T(\psi_1 - \psi_2)\|$, $T\psi_1 = T\psi_2$) and it implies that $\tilde{U}$ is an isometry over $\mathrm{Im}(|T|)$, thus it can be uniquely extended to an isometry of $\overline{\mathrm{Im}(|T|)}$ over $\overline{\mathrm{Im}T}$. The definition of the map $U$ is straightforward extending $\tilde{U}$ to all $H$ defining it 0 on $(\mathrm{Im}(|T|))^{\perp}$. Since $|T|$ is self-adjoint $(\mathrm{Im}(|T|))^{\perp} = \mathrm{Ker}(|T|)$. Furthermore, since $\| |T|\psi\| = \| T\psi\|$, $|T|\psi=0$ if and only if $T\psi=0$, therefore $\mathrm{Ker}|T| = \mathrm{Ker}T$ which implies, in the end, that $\mathrm{Ker}U = \mathrm{Ker}T$.
	
	Lastly we shall prove that $U$ is unique. Suppose $V$ is another partial isometry such that $T = V|T|$ and $\mathrm{Ker}V = \mathrm{Ker}T$. First condition implies that $T = U|T| = V|T|$, so $U$ and $V$ coincide over $\mathrm{Im}|T|$ (and by continuity over its closure), while second condition implies that $\mathrm{Ker}T = \mathrm{Ker}U = \mathrm{Ker}T$, so $U$ and $V$ coincide over $\mathrm{Ker}|T| = (\mathrm{Im}|T|)^{\perp}$, therefore $U$ and $V$ coincide over $H = \overline{\mathrm{Im}|T|} \oplus \mathrm{Ker}|T|$.
\end{proof}

\begin{defi}\label{trace def}
	Let $X$ be a separable Hilbert space with orthonormal basis $\{e_n\}_{n \in \N}$. Given $T \in \B(X)$ a non-negative operator we define the \textbf{trace} of $T$ as
	\begin{equation}\label{trace expression}
		\mathrm{tr}(T) = \sum_{n=1}^{\infty} \langle T e_n, e_n \rangle
	\end{equation}
\end{defi}
Since $T$ is non-negative the sum in \eqref{trace expression} is well defined and, in principle, it could depend upon the basis $\{e_n\}_{n \in \N}$. The following theorem states that actually the definition is well posed, i.e. the trace does not depend on the base.
\begin{prop}\label{trace is well-defined}
	The definition of $\mathrm{tr}$ given by \eqref{trace expression} is independent of the basis.
\end{prop}
\begin{proof}
	Let $T \in \B(H)$ be a non-negative operator and $\{\phi_n\}_{n \in \N}$ and $\{\psi_n\}_{n \in \N}$ be two orthonormal basis of $H$. We have
	\begin{align*}
		\sum_{n=1}^{+\infty} \langle T\phi_n,\phi_n \rangle &= \sum_{n=1}^{+\infty} \|\sqrt{T}\phi_n\|^2 = \sum_{n=1}^{+\infty} \left(\sum_{m=1}^{+\infty} |\langle \sqrt{T}\phi_n, \psi_m \rangle|^2 \right)=\\
															&= \sum_{m=1}^{+\infty} \left(\sum_{n=1}^{+\infty} |\langle \sqrt{T}\psi_m, \phi_n \rangle|^2 \right) = \sum_{m=1}^{+\infty} \|\sqrt{T}\psi_m\|^2 = \\
															&= \sum_{m=1}^{+\infty} \langle T \psi_m, \psi_m \rangle
	\end{align*}
	where we used the fact that $\sqrt{T}$ is self-adjoint, while the exchange of series is allowed because all terms are non-negative.
\end{proof}
\begin{defi}\label{Hilbert-Schmidt operator def}
	An operator $T \in \B(X)$ is called \textbf{Hilbert-Schmidt} if and only if $\mathrm{tr}(T^*T) < \infty$. We define the \textbf{Hilbert-Schmidt norm} of an operator as $\|T\|_{\mathrm{HS}} = \sqrt{\mathrm{tr}(T^*T)}$.
\end{defi}
From the definition of the trace we can see that, given an orthonormal basis $\{e_n\}_{n=1}^{+\infty}$,
\begin{equation}\label{Hilbert-Schmidt norm expression}
	\|T\|_{\mathrm{HS}}^2 = \mathrm{tr}(T^*T) = \sum_{n=1}^{+\infty} \langle T^*Te_n,e_n \rangle = \sum_{n=1}^{+\infty} \langle Te_n,Te_n \rangle = \sum_{n=1}^{+\infty} \|Te_n\|^2
\end{equation}
so we can say, in an equivalent way, that an operator is Hilbert-Schmidt if and only if $\sum_{n=1}^{+\infty} \|Te_n\|^2 < +\infty$. Thanks to Proposition \ref{trace is well-defined} we immediately see that the Hilbert-Schmidt norm is independent on the choice of the basis.

We are now going to show some properties of Hilbert-Schmidt and trace-class operators.
\begin{prop}\label{|T| and T^* are Hilbert-Schmidt}
	Let $T \in \B(H)$ be a Hilbert-Schmidt operator. Then also $|T|$ and $T^*$ are Hilbert-Schmidt operators and 
	\begin{equation}\label{Hilbert-Schmidt norm of |T| and T^*}
		\| |T| \|_{\mathrm{HS}} = \| T^* \|_{\mathrm{HS}} = \| T \|_{\mathrm{HS}}.
	\end{equation}
\end{prop}
\begin{proof}
	We already know that, for every $x \in H$, $\|Tx\|^2 = \||T|x\|^2$, therefore from \eqref{Hilbert-Schmidt norm expression} we immediately see that $ \| |T| \|_{\mathrm{HS}} = \| T \|_{\mathrm{HS}}$.
	
	Consider now an orthonormal basis $\{e_n\}_{n \in \N}$ of $H$. From \eqref{Hilbert-Schmidt norm expression}:
	\begin{align*}
		\|T\|_{\mathrm{HS}}^2 &= \sum_{n=1}^{+\infty} \|Te_n\|^2 = \sum_{n=1}^{+\infty} \left(\sum_{m=1}^{+\infty} |\langle Te_n,e_m \rangle|^2\right) = \sum_{m=1}^{+\infty} \left(\sum_{n=1}^{+\infty} |\langle e_n, T^*e_m \rangle|^2\right) =\\
							&= \sum_{m=1}^{+\infty} \|T^* e_m\|^2 = \|T^*\|_{\mathrm{HS}}^2
	\end{align*}
	where exchange of series is allowed since all terms are non-negative.
\end{proof}
\begin{teo}\label{Hilbert-Schmidt operators are compact and bounded}
	Let $T \in \B(H)$ be a Hilbert-Schmidt operator. Then $\|T\| \leq \|T\|_{\mathrm{HS}}$ and $T$ is compact. Moreover, a compact operator $T$ is Hilbert-Schmidt if and only if $\sum_{n=1}^{+\infty} \mu_n^2$, where $\{\mu_n\}_{n=1}^{+\infty}$ are the singular values of $T$.
\end{teo}
\begin{proof}
	Let $\{e_n\}_{n \in \N}$ be an orthonormal basis of $H$ and let $x \in H$. We have:
	\begin{align*}
		\|Tx\|^2 &= \sum_{n=1}^{+\infty} |\langle Tx,e_n \rangle|^2 = \sum_{n=1}^{+\infty} |\langle x,T^*e_n \rangle|^2 \leq \|x\|^2 \sum_{n=1}^{+\infty} \|T^*e_n\|^2 =\\
				 &= \|x\|^2 \|T^*\|_{\mathrm{HS}}^2 \overset{\eqref{Hilbert-Schmidt norm of |T| and T^*}}{=} \|x\|^2 \|T\|_{\mathrm{HS}}^2
	\end{align*}
	Taking the supremum over all $x \in H$ gives us that $\|T\| \leq \|T\|_{\mathrm{HS}}$.
	
	To prove that $T$ is compact, consider the following sequence of operators $T_N = \sum_{n=1}^{N} \langle \cdot , e_n \rangle Te_n$. All $T_N$ have finite rank, so they are compact and $T_N \rightarrow T$ in $\B(H)$, indeed:
	\begin{align*}
		\|T-T_N\| \leq \|T-T_N\|_{\mathrm{HS}}^2 = \sum_{n=N+1}^{+\infty} \|T e_n\|^2 \rightarrow 0 \quad \mathrm{as\ } N \rightarrow +\infty
	\end{align*}
	therefore, since $T$ is limit of compact operators, it is compact.
	
	Lastly, if $T$ is a compact operator also $T^*T$ is compact as well as self-adjoint, hence we can consider as orthonormal basis of $H$ the one given by its eigenvectors $\{e_n\}_{n \in \N}$. Recalling Corollary \ref{canonical form for compact operators corollary}, we know that $\mu_n^2$ are exactly the eigenvalues of $T^*T$.  For such basis we have:
	\begin{align*}
		\|T\|_{\mathrm{HS}}^2 &= \sum_{n=1}^{+\infty} \|Te_n\|^2 = \sum_{n=1}^{+\infty} \langle Te_n,Te_n \rangle = \sum_{n=1}^{+\infty} \langle T^*T e_n,e_n \rangle = \sum_{n=1}^{+\infty} \mu_n^2
	\end{align*}
\end{proof}
\begin{prop}\label{Hilbert-Schmidt operator are an ideal}
	Let $T \in \B(H)$ be a Hilbert-Schmidt operator and $S \in \B(H)$. Then $TS$ and $ST$ are both Hilbert-Schmidt operators.
\end{prop}
\begin{proof}
	Given an orthonormal basis $\{e_n\}_{n \in \N}$ of $H$ we have:
	\begin{align*}
		\|ST\|_{\mathrm{HS}}^2 = \sum_{n=1}^{+\infty} \|ST e_n\|^2 \leq \|S\|^2 \sum_{n=1}^{+\infty} \|T e_n\|^2 = \|S\|^2 \|T\|_{\mathrm{HS}}^2
	\end{align*}
	so $ST$ is Hilbert-Schmidt. Moreover, from Proposition \ref{Hilbert-Schmidt norm of |T| and T^*} follows:
	\begin{align*}
		\| TS \|_{\mathrm{HS}} = \| (TS)^* \|_{\mathrm{HS}} = \| S^* T^* \|_{\mathrm{HS}} \leq \|S^*\| \|  T^* \|_{\mathrm{HS}} = \|S \| \| T \|_{\mathrm{HS}}
	\end{align*}
\end{proof}
We can now turn back to trace-class operator. It is evident that trace-class operators and Hilbert-Schmidt operators are strictly related. Indeed, as seen in the proof of Proposition \ref{trace is well-defined}, we have
\begin{equation*}
	\mathrm{tr}|T| = \sum_{n=1}^{+\infty} \langle |T|e_n, e_n \rangle = \sum_{n=1}^{+\infty} \|\sqrt{|T|}e_n\|^2 = \| T \|_{\mathrm{HS}}^2,
\end{equation*}
so, an operator is trace-class if and only if $\sqrt{|T|}$ is Hilbert-Schmidt. By virtue of this link we can exploit properties of Hilbert-Schmidt operators in order to obtain informations about trace-class operators. First of all, we are going to prove that expression \eqref{trace expression} is well-defined for every trace-class operator and not only for non-negative ones.
\begin{teo}
	Let $T \in \B(H)$ be a trace-class operator and $\{e_n\}_{n \in \N}$ an orthonormal basis of $H$. Then $\sum_{n=1}^{+\infty} \langle Te_n,e_n \rangle$ converges absolutely and the limit is independent of the basis.
\end{teo}
\begin{proof}
	We start proving that the series converges absolutely. Letting $T = U |T|$ be the polar decomposition of $T$, we want to show that:
	\begin{equation}\label{absoulte convergence trace intermediate}
		\sum_{n=1}^{+\infty} |\langle Te_n,e_n  \rangle| = \sum_{n=1}^{+\infty} |\langle U\sqrt{|T|}\sqrt{|T|}e_n,e_n \rangle| = \sum_{n=1}^{+\infty} |\langle \sqrt{|T|}e_n,\sqrt{|T|}U^*e_n\rangle| < +\infty.
	\end{equation}
	From Cauchy-Schwarz' inequality, for every term we have that $|\langle \sqrt{|T|}e_n,\sqrt{|T|}U^*e_n\rangle| \leq \|\sqrt{|T|}e_n \|\,\| \sqrt{|T|}U^*e_n\|$.
	Since $T$ is trace-class, $\sqrt{|T|}$ is Hilbert-Schmidt and, thanks to Proposition \ref{Hilbert-Schmidt operator are an ideal}, also $\sqrt{|T|}U^*$ is a Hilbert-Schmidt operator, therefore both $\{\|\sqrt{|T|}e_n\|\}_{n \in \N}$ and $\{\|\sqrt{|T|}U^* e_n\|\}_{n \in \N}$ are in $\ell^2(\N)$. This allows us to use the Cauchy-Schwarz inequality in the last expression of \eqref{absoulte convergence trace intermediate}, thus obtaining:
	\begin{align*}
		\sum_{n=1}^{+\infty} |\langle Te_n,e_n  \rangle| &= \sum_{n=1}^{+\infty} |\langle \sqrt{|T|}e_n,\sqrt{|T|}U^*e_n\rangle| \leq \sum_{n=1}^{+\infty} \| \sqrt{|T|}e_n\|\, \|\sqrt{|T|}U^*e_n \| \overset{\mathrm{C-S}}{\leq}\\
		&\leq \left(\sum_{n=1}^{+\infty} \| \sqrt{|T|}e_n\|^2 \right)^{1/2} \left(\sum_{n=1}^{+\infty} \| \sqrt{|T|}U^*e_n\|^2 \right)^{1/2} \leq \|\sqrt{|T|}\|_{\mathrm{HS}}^2.
	\end{align*}
	The proof of the independence of the basis is exactly the same as the one for Proposition \ref{trace is well-defined}, because now the exchange of series is allowed since the series is convergent.
\end{proof}
Another immediate corollary of Proposition \ref{Hilbert-Schmidt operator are an ideal} is the following proposition
\begin{prop}\label{trace-class operators are Hilbert-Schmidt}
	Let $T \in \B(H)$ be a trace-class operator. Then $T$ is also Hilbert-Schmidt.
\end{prop}
\begin{proof}
	Let $T = U |T|$ be the polar decomposition of $T$. Since $T$ is trace-class $\sqrt{|T|}$ is an Hilbert-Schmidt operator. Therefore, from Proposition \ref{Hilbert-Schmidt operator are an ideal} follows that $T = U \sqrt{|T|} \sqrt{|T|}$ is an Hilbert-Schmidt operator.
\end{proof}

\begin{teo}\label{trace-class operators are compact and bounded}
	Let $T \in \B(H)$. Then $T$ is trace-class if and only if it is compact and $\sum_{n=1}^{+\infty} \mu_n$, where $\{\mu_n\}_{n \in \N}$ are the singular values of $T$. Moreover, if $T$ is trace-class then $\mathrm{tr}|T| = \sum_{n=1}^{+\infty} \mu_n$ and additionally, if it is also self-adjoint, $\mathrm{tr}T = \sum_{n=1}^{+\infty} \lambda_n$ where $\{\lambda_n\}_{n \in \N}$. Finally, we have $\|T\| \leq \mathrm{tr}|T|$.
\end{teo}
\begin{proof}
	Let $T = U|T|$ be the polar decomposition of $T$. If $T$ is trace-class $\sqrt{|T|}$ is Hilbert-Schmidt, but from Theorem \ref{Hilbert-Schmidt operators are compact and bounded} we have that $\sqrt{|T|}$ is compact, therefore also $T = U \sqrt{|T|}\sqrt{|T|}$ and $|T|$ are compact. Not only $|T|$ is compact, but it is also self-adjoint and its eigenvalues are exactly $\{\mu_n\}_{n \in \N}$. Letting $\{e_n\}_{n \in \N}$ be an orthonormal basis made up of eigenvectors of $|T|$, we have
	\begin{align*}
		\mathrm{tr}|T| = \sum_{n=1}^{+\infty} \langle |T|e_n,e_n \rangle = \sum_{n=1}^{+\infty} \mu_n.
	\end{align*}
	Conversely, if $T$ is a compact operator the previous formula still holds and shows that $T$ is also trace-class. With the same reasoning, if $T$ is self-adjoint and trace-class, we can write the trace with respect to the basis made up its eigenvectors thus obtaining $\mathrm{tr}T = \sum_{n=1}^{+\infty} \lambda_n$.
	
	For the last part of the Theorem, we notice that $|T|$ is a compact self-adjoint non-negative operator, therefore, assuming its eigenvalues are decreasingly ordered, from Corollary \ref{norm is greatest eigenvalue} we have that $\| |T| \| = \mu_1$, hence:
	\begin{align*}
		\| T \| = \| U |T| \| \leq \| |T| \| = \mu_1 \leq \sum_{n=1}^{+\infty} \mu_n = \mathrm{tr}|T|.
	\end{align*} 	
\end{proof}

In the special case $H = L^2(\R^d)$ we are able to give a characterization of Hilbert-Schmidt operators.
\begin{teo}\label{representation of Hilbert-Schmidt integral operator}
	An operator $T \in \B(L^2(\R^d))$ is Hilbert-Schmidt if only if there exist a function $K_T \in L^2(\R^d \times \R^d)$, called integral kernel, such that
	\begin{equation}\label{Hilbert-Schmidt integral operator}
		(Tf)(x) = \int_{\R^d} K_T(x,y)f(y)dy \quad \forall f \in L^2(\R^d).
	\end{equation}
	Moreover $\|T\|_{\mathrm{HS}} = \|K_T\|_2$.
\end{teo}
\begin{proof}
	We start proving that, given $K \in L^2(\R^{2d})$, the corresponding integral operator defined by \eqref{Hilbert-Schmidt integral operator} is continuos. Denoting with $T_K$ such operator, for every $f \in L^2(\R^d)$, we have:
	\begin{align*}
		\| T_K f \|_{L^2(\R^d)}^2 &= \int_{\R^{d}} |T_K f(x)|^2 dx = \int_{\R^d} \Big|\int_{\R^{d}} K(x,y) f(y) dy\Big|^2 dx \leq\\
						&\leq \int_{\R^d} \left(\int_{\R^{d}} |K(x,y)| |f(y)| dy\right)^2 dx.
	\end{align*}
	From Fubini's theorem we have that $|K(x,\cdot)| \in L^2(\R^d)$ for almost every $x \in \R^d$. Therefore, we can apply Cauchy-Schwartz's inequality in the inner integral, thus obtaining:
	\begin{equation}\label{representation of Hilbert-Schmidt integral operator bound 1}
		\| T_K f \|_{L^2(\R^d)}^2 \leq \|f\|_{L^2(\R^d)}^2 \int_{\R^d} \int_{\R^{d}} |K(x,y)|^2 dy dx = \| K \|_{L^2(\R^{2d})}^2 \|f\|_{L^2(\R^d)}
	\end{equation}
	which shows that $T_K$ is a bounded operator. Moreover, $T_K$ is clearly linear, therefore $T_K \in \B(L^2(\R^d))$. Consider now an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$. Moreover, always from \eqref{representation of Hilbert-Schmidt integral operator bound 1}, it follows that, for every $f \in L^2(\R^d)$, the operator $K \in L^2(\R^{2d}) \mapsto A_f(K) = T_K f \in L^2(\R^d)$ is linear and continuous.
	
	We can now suppose to have an Hilbert-Schmidt operator $T \in \B(H)$ and an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$. We want to show that \eqref{Hilbert-Schmidt integral operator} holds with the following kernel:
	\begin{equation}\label{integral kernel formula}
		K_T(x,y) = \sum_{(m,n) \in \N^2} \langle Te_n,e_m \rangle_{L^2(\R^d)} e_m(x)\overline{e_n(y)}.
	\end{equation}
	Since $\{e_m \otimes \overline{e_n}\}_{(m,n) \in \N^2}$ is an orthonormal basis of $L^2(\R^{2d})$, the convergence is unconditional. Therefore, for every $f \in L^2(\R^d)$
	\begin{align*}
		\int_{\R^d} K_T(x,y)f(y)dy &= A_f(K_T)(x) = A_f \Big( \sum_{(m,n) \in \N^2} \langle Te_n,e_m \rangle e_m \otimes\overline{e_n} \Big)(x) \overset{\ref{exchange of series and operator}}{=}\\
								   &= \sum_{(m,n) \in \N^2} A_f \big(\langle Te_n,e_m \rangle e_m \otimes\overline{e_n}\big)(x) \overset{\ref{exchange order double series}}{=}\\
								   &= \sum_{m=1}^{+\infty} \sum_{n=1}^{+\infty} \langle Te_n,e_m \rangle A_f(e_m \otimes e_n)(x) =\\
								   &= \sum_{m=1}^{+\infty} \sum_{n=1}^{+\infty} \langle Te_n,e_m \rangle \int_{\R^d} e_m(x)\overline{e_n(y)}f(y) dy =\\
								   &= \sum_{m=1}^{+\infty} \sum_{n=1}^{+\infty} \langle T(\langle f,e_n\rangle e_n), e_m \rangle e_m(x) =\\
								   &= \sum_{m=1}^{+\infty} \langle Tf, e_m \rangle e_m(x) = (Tf)(x), 
	\end{align*} 
	where the use of \ref{exchange of series and operator} is justified because, $\sum_{(m,n) \in \N^2} \langle Te_n,e_m \rangle e_m \otimes\overline{e_n}$ converges unconditional, while the use of \ref{exchange order double series} is justified because $A_f$ is continuous, thus \eqref{exchange of series and operator} implies that $\sum_{(m,n) \in \N^2} A_f \big(\langle Te_n,e_m \rangle e_m \otimes\overline{e_n}\big)$ converges unconditionally. Finally, we have:
	\begin{equation}\label{equivalence norm L2 and Hilbert-Schmidt norm}
		\begin{aligned}
			\|K_T\|_{L^2(\R^{2d})}^2 &= \sum_{(n,m) \in \N^2} |\langle Te_n,e_m \rangle|^2 = \sum_{n=1}^{+\infty} \sum_{m=1}^{+\infty} |\langle Te_n,e_m \rangle|^2 =\\
			&= \sum_{n=1}^{+\infty} \|T e_n\|^2_{L^2(\R^d)} = \|T\|_{\mathrm{HS}}^2.
		\end{aligned}		
	\end{equation}
	Conversely, if we have $K \in L^2(\R^{2d})$ and we define $T_K$ by \eqref{Hilbert-Schmidt integral operator}, we have to show that this operator is Hilbert-Schmidt, but this is straightforward since  \eqref{equivalence norm L2 and Hilbert-Schmidt norm} still holds.
\end{proof}
In light of this theorem, operators defined by \eqref{Hilbert-Schmidt integral operator} are called \emph{Hilbert-Schmidt integral operators}.

\begin{prop}\label{condition integral operator self-adjoint}
	Let $T$ be an Hilbert-Schmidt integral operator over $L^2(\R^d)$ with integral kernel $K \in L^2(\R^{2d})$. Then its adjoin operator is given by
	\begin{equation}\label{Hilbert-Schmidt integral operator adjoint}
		T^*f(x) = \int_{\R^d} \overline{K(y,x)} f(y)dy.
	\end{equation}
	Therefore $T$ is self-adjoint if and only if $K(x,y) = \overline{K(y,x)}$.
\end{prop}
\begin{proof}
	Let $f,g \in L^2(\R^d)$. We start showing that $K(x,y)f(y)\overline{g(x)} \in L^1(\R^{2d})$:
	\begin{align*}
		&\int_{\R^{2d}} |K(x,y)| |f(y)| |g(x)| dxdy \overset{\mathrm{Tonelli}}{=} \int_{\R^d} \left(\int_{\R^d} |K(x,y)||f(y)| dy\right) |g(x)| dx = \\
		&= \int_{\R^d} (T_{|K|} |f|)(y) |g(y)| \overset{\mathrm{C-S}}{\leq} \|T_{|K|}\|_2 \|g\|_2 \leq \|K\|_2 \| f \|_2 \|g\|_2 < +\infty,
	\end{align*}
	where $T_{|K|}$ denotes the Hilbert-Schmidt integral operator with kernel $|K|$ and $\|T_{|K|}|f|\|_2 \leq \|K\|_2 \| f \|_2$ because $T_{|K|}$ is Hilbert-Schmidt, therefore $\|T_{|K|}\| \leq \|T_{|K|}\|_{\mathrm{HS}} = \| K \|_2$. We are now in the position to use Fubini's theorem:
	\begin{align*}
		\langle Tf,g \rangle &= \int_{\R^d} \left(\int_{\R^d} K(x,y)f(y)dy\right)\overline{g(x)}dx = \int_{\R^d} \left(\int_{\R^d} K(x,y)\overline{g(x)}dx\right) f(y)dy =\\
							 &= \int_{\R^d} \overline{\left(\int_{\R^d} \overline{K(x,y)} g(x)dx\right)} f(y)dy = \langle f,T^*g \rangle
	\end{align*}
	where the expression of $T^*$ is exactly the one in \eqref{Hilbert-Schmidt integral operator adjoint}.
\end{proof}


\section{Fourier Transform and its properties}\label{Fourier transform and its properties section}
\begin{defi}\label{Fourier transform def}
	Let $f \in L^1(\R^d)$. We define the \textbf{Fourier transform} of $f$ the function
	\begin{equation}\label{Fourier transform formula}
		\F f(\omega) = \hat{f}(\omega) \coloneqq \int_{\R^d} e^{-2 \pi i \omega \cdot t} f(t) dt
	\end{equation}
\end{defi}
It is straightforward to see that the definition is well-posed and that $\F f \in L^{\infty}(\R^d)$ with $\|\F f\|_{\infty} \leq \| f \|_1$. Therefore $\F$ can be seen as a linear operator between $L^1(\R^d)$ and $L^{\infty}(\R^d)$ with $\| \F \| = 1$ (from the previous inequality actually we saw that $\| \F \| \leq 1$ but if we take $f \geq 0$ a.e. we have that $\hat{f}(0) = \| f \|_1$ that gives us the equality).\\
The Fourier transform of an $L^1(\R^d)$ is not only bounded, as stated by the \emph{Riemann-Lebesgue lemma}.
\begin{teo}[Riemann-Lebesgue lemma]\label{Riemann-Lebesgue lemma}
	Let $f \in L^1(\R^d)$. Therefore $\hat{f} \in C_0(\R^d) = \{f : \R^d \rightarrow \C \text{ continuous such that } \lim_{|t| \rightarrow \infty} |f(t)|=0\}$.
\end{teo}
\begin{defi}\label{inverse Fourier transform def}
	Let $f \in L^1(\R^d)$. We define the \textbf{inverse Fourier transform} of the function $f$
	\begin{equation}\label{inverse Fourier transform formula}
		\F^{-1} f(t) = \check{f}(t) \coloneqq \int_{\R^d} e^{2 \pi i \omega \cdot t} f(\omega) d\omega
	\end{equation}
\end{defi}
The inverse Fourier transform is denoted with $\F^{-1}$ because it is actually the inverse operator of the Fourier transform as stated by the \emph{inversion theorem}.
\begin{teo}[Inversion theorem]\label{inversion theorem}
	Let $f \in L^1(\R^d)$ and suppose that also $\hat{f} \in L^1(\R^d)$. Then
	\begin{equation*}
		f(t) = \F^{-1} \circ \F f(t) = \int_{\R^d} \hat{f}(\omega) e^{2 \pi i \omega \cdot t}d\omega
	\end{equation*}
\end{teo}

The Fourier transform is intimately related to regularity and decay properties. The duality between these two is stated in the following theorems.
\begin{teo}\label{duality decay-regularity theorem}
	Let $f \in L^1(\R^d)$. If $|t|^k f \in L^1(\R^d)$ for some $k \in \N$ then $\hat{f} \in C_0^k(\R^d)$ and the following holds for every $\alpha \in \N^d$ with $|\alpha| \leq k$:
	\begin{equation}\label{derivative of transform}
		\F \left((-2 \pi i t)^{\alpha} f \right) (\omega) = \partial^{\alpha} \F f(\omega).
	\end{equation}
\end{teo}
\begin{teo}\label{duality regularity-decay theorem}
	Let $f \in C^k(\R^d)$ for some $k \in \N$. If $f, \partial^{\alpha}f \in L^1(\R^d)$ for every $\alpha \in \N^d$ with $|\alpha| \leq k$ then
	\begin{equation}\label{transform of derivative}
		\F \left(\partial^{\alpha} f\right) (\omega) = \left(2 \pi i \omega\right)^{\alpha} \F f(\omega)
	\end{equation}
	In particular this implies that $\hat{f}(\omega) = o\left(|\omega|^{-k} \right) $ as $| \omega | \rightarrow \infty$.
\end{teo}
{\color{blue}To sum up, previous theorems state a duality between regularity and decay: if a function is smooth then its Fourier transform decays rapidly and vice versa.}


If $f$ is in $L^2(\R^d)$, the integral in \eqref{Fourier transform formula} in general will not converge. Nevertheless we can define the Fourier transform of an $L^2$ function through a density argument. For example, one can use $L^2(\R^d) \cap L^1(\R^d)$, which is a dense subspace of $L^2(\R^d)$. On this space one can show that the Fourier transform is an isometry with respect to the $L^2$ norm and therefore it extends to an isometry on the whole $L^2(\R^d)$. This is stated by the \emph{Plancherel theorem}.
\begin{teo}[Plancherel theorem]\label{Plancherel theorem}
	If $f \in L^1(\R^d) \cap L^2(\R^d)$ then $\| f \|_2 = \| \hat{f} \|_2$.
\end{teo}
Thanks to the polarization identity this implies that $\F$ preserves the inner product in $L^2(\R^d)$:
\begin{equation}\label{Parseval formula}
	\langle f,g \rangle_{L^2(\R^d)} = \langle \hat{f}, \hat{g} \rangle_{L^2(\R^d)} \quad \forall f,g \in L^2(\R^d)
\end{equation}
therefore the Fourier transform $\F$ is a unitary operator on $L^2(\R^d)$. Result \eqref{Parseval formula} is called \emph{Parseval formula}.

Lastly, we introduce some fundamental operators in Fourier analysis. Given $x,\xi \in \R^d$ and $\lambda > 0$ we define the \emph{time-shift} (or translation) operator $T_x$
\begin{equation}\label{time-shift operator def}
	T_x f(t) = f(t-x) \quad \forall\, t \in \R^d,
\end{equation}
the \emph{modulation} operator $M_{\xi}$
\begin{equation}\label{modulation operator def}
	M_{\xi} f(t) = e^{2 \pi i \xi \cdot t} f(t) \quad \forall\, t \in \R^d
\end{equation}
and the \emph{dilation} operator $D_{\lambda}$
\begin{equation}\label{dilation operator def}
	D_{\lambda}f(t) = \lambda^d f(\lambda t) \quad \forall\, t \in \R^d.
\end{equation}
Moreover, time-shift and modulation operators can be combined into a \emph{time-frequency shift} operator
\begin{equation}\label{time-frequency shift def}
	\pi(x,\xi) f(t) = M_{\xi} T_x f(t) \quad \forall\, t \in \R^d.
\end{equation}
It is easy to check that all these operator are isometric (with respect to the $L^1$ norm) isomorphism. We show how these operator act under the Fourier transform.
\begin{prop}\label{properties of translation modulation and dilation operators}
	Let $f \in L^1(\R^d)$. Then the following holds:
	\begin{enumerate}[label=(\roman*)]
		\item $\displaystyle \F(T_xf)(\omega) = M_{-x}\hat{f}(\omega)$;\label{Fourier transform of translation}
		\item $\displaystyle \F(M_{\xi} f)(\omega) = T_{\xi} \hat{f}(\omega)$;\label{Fourier transform of modulation}
		\item $\displaystyle \F(D_{\lambda}f)(\omega) = \hat{f}\left(\frac{\omega}{\lambda}\right)$.\label{Fourier transform of dilation}
	\end{enumerate}
\end{prop}
\begin{proof}
	\phantom{a}\\
	\begin{enumerate}[label=(\roman*)]
		\item $\begin{aligned}[t]
			 \F(T_xf)(\omega) &= \int_{\R^d} f(t-x) e^{-2\pi i \omega \cdot t}dt \overset{s = t-x}{=} \int_{\R^d} f(s) e^{-2\pi i \omega \cdot (s+x)}ds = \\
			                  &= e^{-2\pi i \omega \cdot x} \int_{\R^d} f(s) e^{-2\pi i \omega \cdot s}ds = M_{-x}\hat{f}(\omega) 
		\end{aligned}$
		\item $\begin{aligned}
			\F(M_\xi f)(\omega) = \int_{\R^d} e^{2 \pi i \xi \cdot t} f(t) e^{-2 \pi i \omega \cdot t}dt = \int_{\R^d} f(t) e^{-2\pi i (\omega - \xi) \cdot t} dt = T_{\xi}\hat{f}(\omega)
		\end{aligned}$
	\item $\begin{aligned}
		\F(D_{\lambda}f)(\omega) = \int_{\R^d} \lambda^d f(\lambda t) e^{-2\pi i \omega \cdot t}dt \overset{s=\lambda t}{=} \int_{\R^d} f(s) e^{-2 \pi i \omega \cdot \frac{s}{\lambda}}ds = \hat{f}\left(\frac{\omega}{\lambda}\right)
	\end{aligned}$
	\end{enumerate}	
\end{proof}
We point out that the second property, namely that $\F(M_{\xi}f) = T_{\xi}\hat{f}$, is shedding light on the role of modulation operator:\\
while $T_x$ acts as a translation in the time domain, $M_{\xi}$ is a translation in the frequency domain. Therefore the time-frequency shift operator $\pi(x,\xi)$ is indeed a shift operator because it acts as a translation in the joint time-frequency domain.

Thanks to these properties and Theorems \ref{duality decay-regularity theorem} and \ref{duality regularity-decay theorem} we can state compute the Fourier transform of Gaussians.
\begin{prop}\label{Fourier transform of Gaussian proposition}
	The Fourier transform maps Gaussian into Gaussians. More precisely, for $\lambda > 0$, we have:
	\begin{equation}\label{Fourier transform of Gaussian formula}
		\F(e^{-\lambda \pi |\cdot|^2})(\omega) = \dfrac{1}{\lambda^{d/2}} e^{-\frac{1}{\lambda}}\pi |\omega|^2 \quad \forall\,\omega \in \R^d.
	\end{equation}
\end{prop}
\begin{proof}
	We start considering the 1-dimensional case and for the ease of notation we let $\varphi_{\lambda}(t) = e^{-\lambda \pi t^2}$ for $t \in \R$.
	
	First of all we consider $\varphi = \varphi_1$, for which we have:
	\begin{equation*}
		\frac{d \varphi}{dt}(t) = -2 \pi t \varphi(t)
	\end{equation*}
	If we take the Fourier transform of both members, using \eqref{transform of derivative} in the former and \eqref{derivative of transform} in the latter we obtain:
	\begin{equation*}
		2 \pi i \omega \hat{\varphi}(\omega) = -i \dfrac{d\hat{\varphi}}{d\omega}(\omega) \implies \dfrac{d\hat{\varphi}}{d\omega}(\omega) = - 2 \pi \omega \hat{\varphi}(\omega)
	\end{equation*}
	which is exactly the same equation satisfied by $\varphi$, therefore $\hat{\varphi}(\omega) = C e^{-\pi \omega^2}$ for some $C \in \R$. Since $\hat{\varphi}(0) = \int_\R \varphi(t)dt = \|\varphi\|_1=1$ we obtain that $C=1$.
	
	The general case can be proved using the dilatation operator:
	\begin{align*}
		&\varphi_{\lambda}(t) = e^{-\lambda \pi t^2} = e^{-\pi \left(\sqrt{\lambda} t\right)^2} = \dfrac{1}{\sqrt{\lambda}} D_{\sqrt{\lambda}}\varphi(t) \implies\\
		                     &\implies \hat{\varphi}_{\lambda}(\omega) = \dfrac{1}{\sqrt{\lambda}} \F ( D_{\sqrt{\lambda}}\varphi )(\omega) = \dfrac{1}{\sqrt{\lambda}} \hat{\varphi}\left(\frac{\omega}{\sqrt{\lambda}}\right) = \dfrac{1}{\sqrt{\lambda}} e^{-\frac{1}{\lambda}\pi\omega^2}
	\end{align*}
	The passage to the multidimensional case is almost straightforward since $e^{-\lambda|t|^2} = \prod_{j=1}^d e^{-\lambda t_j^2}$, therefore:
	\begin{align*}
		\F(e^{-\lambda\pi|\cdot|^2})(\omega) &= \int_{\R^d} e^{-\lambda \pi |t|^2} e^{-2\pi i \omega \cdot t}dt = \prod_{j=1}^d \int_{\R} e^{-\lambda \pi t_j^2} e^{-2\pi i \omega_j t_j} dt_j = \\
										  &= \prod_{j=1}^d \dfrac{1}{\sqrt{\lambda}}e^{-\frac{1}{\lambda}\pi \omega_j^2} = \dfrac{1}{\lambda^{d/2}} e^{-\frac{1}{\lambda}\pi|\omega|^2}.
	\end{align*}
\end{proof}

\begin{teo}[Hausdorff-Young]
	Let $1 \leq p \leq 2$ and let $p'$ such that $\frac{1}{p} + \frac{1}{p'} = 1$. Then $\F : L^p(\R^d) \rightarrow L^{p'}(\R^d)$ and $\| \hat{f }\|_{p'} \leq \| f \|_p$.
\end{teo}
In the following we will need the sharp version of the Hausdorff-Young inequality: 
\begin{equation}\label{Hausdorff-Yound inequality}
	\| \hat{f} \|_{p'} \leq \left(\dfrac{p^{1/p}}{p'^{1/p'}}\right)^{d/2} \|f\|_p = A_p^d \|f\|_p
\end{equation}
where $A_p$ is the so-called Babenko-Bechner constant. This constant appears also in the sharp version of Young's theorem
\begin{teo}[Young]\label{Young theorem}
	Given $f \in L^p(\R^d)$ and $g \in L^q(\R^d)$, suppose that $\frac{1}{p}+\frac{1}{q}=1+\frac{1}{r}$ with $r \geq 1$. Then $f * g \in L^r(\R^d)$ and $\|f * g\|_r \leq \|f\|_p \|g\|_q$.
\end{teo}
Just like the Hausdorff-Young inequality, Young's inequality holds in a sharp version:
\begin{equation}\label{Young inequality sharp}
	\|f * g\|_r \leq (A_p A_q A_{r'})^d \|f\|_p \|g\|_q.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Short-Time Fourier Transform}\label{chapter STFT}
\section{STFT}\label{section STFT}
{\color{red}The \emph{short-time Fourier transform} or \emph{STFT} is a powerful tool, introduced by Gabor in \cite{gabor}, used to study properties of a signal locally both in time and frequency. The main idea behind the STFT is the following: if we want some information of the spectrum of a signal around a specific time, say $T$, we could choose an interval $(T-\Delta T, T + \Delta T)$ and take the Fourier transform of $f \chi_{(T-\Delta T, T + \Delta T)}$. Usually multiplying by a characteristic function will not give us a regular function (not even continuous) and in light of the duality between regularity and decay, the Fourier transform of $f \chi_{(T-\Delta T, T + \Delta T)}$ will not decay rapidly. Therefore a sharp cutoff in the time domain will result in a ``bad'' localization in the frequency domain. In order to avoid this kind of problems we could think to multiply the signal $f$ by a smooth function.}
\begin{defi}\label{STFT def}
	Fix a function $\phi \neq 0$ called \emph{window function}. The \textbf{short-time Fourier transform} of a function $f$ with window $\phi$ is defined as
	\begin{equation}\label{STFT formula}
		\V_{\phi}f(x,\omega) = \int_{\R^d} f(t)\overline{\phi(t-x)}e^{-2 \pi i \omega \cdot t} dt, \quad (x,\omega) \in \R^{2d}
	\end{equation}
\end{defi}
In the above definition we did not specify where $f$ and $\phi$ are chosen. Since we are taking the Fourier transform of the function $f T_x\overline{\phi}$, the STFT is well defined whenever the Fourier transform of this function is. For example if both $f$ and $\phi$ are in $L^2(\R^d)$ then $f T_x\overline{\phi}$ is in $L^1(\R^d)$ for every $x \in \R^d$ and so the integral in \eqref{STFT formula} is defined. In this special case the STFT can be written as a scalar product in $L^2(\R^d)$:
\begin{equation*}
	\V_{\phi}f(x,\omega) = \langle f, M_{\omega} T_x \phi \rangle = \langle f, \pi(x,\omega) \phi \rangle
\end{equation*}
In general, the STFT of $f$ with respect to $\phi$ will be defined whenever  $\langle f, M_{\omega} T_x \phi \rangle$ is an expression of some sort of duality. For example, if $f \in \mathcal{S}'(\R^d)$ and $\phi \in \mathcal{S}(\R^d)$ then $M_{\omega} T_x \overline{\phi} \in \mathcal{S}(\R^d)$, therefore $\langle f, M_{\omega} T_x \phi \rangle$ can be seen as the usual duality between tempered distributions and functions in the Schwartz space.\\
{\color{blue} Aggiungere scritture equivalenti della STFT??}
\subsection{Properties of STFT}\label{properties of STFT subsection}
In this section we will present and prove some properties about the STFT. An excellent reference is \cite{grochenig}.
\begin{teo}\label{orthogonality relation theorem}
	Let $f_1,f_2,\phi_1,\phi_2 \in L^2(\R^d$). Then $\V_{\phi_i}f_i \in L^2(\R^{2d})$ and the following holds:
	\begin{equation}\label{orthogonality relation formula}
		\langle \V_{\phi_1} f_1, \V_{\phi_2} f_2 \rangle = \langle f_1, f_2 \rangle \overline{\langle \phi_1, \phi_2 \rangle}
	\end{equation}
\end{teo}
\begin{proof}
	We start proving that, if $f \in L^2(\R^d)$ and $\phi \in \mathcal{S}(\R^d)$ the STFT of $f$ with window $\phi$ is in $L^2(\R^{2d})$. In order to do so, we need an equivalent formulation of the STFT:
	\begin{equation*}
		\V_{\phi} f (x,\omega)  = \int_{\R^d} f(t) \overline{\phi(t-x)}e^{-2 \pi i \omega \cdot t} dt = \F(f T_x \overline{\phi})(\omega).
	\end{equation*}
	Since we supposed $\phi \in \mathcal{S}(\R^d)$, the function $f T_x \overline{\phi}$ is in $L^2(\R^d)$ for every $x \in \R^d$, hence:
	\begin{align*}
		\| \V_{\phi} f \|_2^2 &= \int_{\R^{2d}} |\V_{\phi} f(x,\omega)|^2 dx d\omega \overset{\mathrm{Tonelli}}{=} \int_{\R^d} \left(\int_{\R^d} |\V_{\phi} f(x,\omega)|^2 \right) d\omega = \\
							  &= \int_{\R^d} \left(\int_{\R^d} |\F(f T_x \overline{\phi})|^2(\omega) d\omega\right) dx \overset{\mathrm{Plancherel}}{=}  \int_{\R^d} \left(\int_{\R^d} |f(t) \overline{\phi(t-x)}|^2 dt \right) dx \overset{\mathrm{Tonelli}}{=}\\
							  &= \int_{\R^d} |f(t)|^2 \left(\int_{\R^d} |\phi(t-x)|^2 dx\right) dt = \|f\|_2^2 \|\phi\|_2^2.
	\end{align*}
	
	Now, consider $f_1, f_2 \in L^2(\R^d)$ and $\phi_1, \phi_2 \in \mathcal{S}(\R^d)$. Both $\V_{\phi_1} f_1$ and $\V_{\phi_2} f_2$ are in $L^2(\R^{2d})$, so their product is in $L^1(\R^{2d})$, hence we can use Fubini's theorem in the following:
	\begin{align*}
		\langle \V{\phi_1} f_1, \V_{\phi_2} f_2 \rangle &= \int_{\R^{2d}} \V_{\phi_1} f_1(x,\omega) \overline{\V_{\phi_2} f_2(x,\omega)} dx d\omega \overset{\mathrm{Fubini}}{=} \\
													    &= \int_{\R^d} \left(\int_{\R^d} \F(f_1 T_x \overline{\phi_1})(\omega) \overline{\F(f_2 T_x \overline{\phi_2})(\omega)} d\omega \right) dx \overset{\mathrm{Parseval}}{=}\\
													    &= \int_{\R^d} \left( \int_{\R^d} f_1(t) \overline{\phi_1(t-x)} \, \overline{f_2(t)} \phi_2(t-x) dt \right)	dx \overset{\mathrm{Fubini}}{=}\\
													    &= \int_{\R^d} f_1(t) \overline{f_2(t)} \left(\int_{\R^{d}}\phi_2(t-x) \overline{\phi_1(t-x)}dx\right) dt = \langle f_1, f_2 \rangle \langle \phi_2,\phi_1\rangle.    
	\end{align*}
	The transition from $\mathcal{S}(\R^d)$ to whole $L^2(\R^d)$ is done trough a density argument. Indeed, for $f_1, f_2 \in L^2(\R^d)$ and $\phi_{1} \in \mathcal{S}(\R^d)$ fixed, the mapping $\phi_2 \in L^2(\R^d) \mapsto \langle \V_{\phi_1} f_1, \V_{\phi_2} f_2 \rangle$ is a linear functional and we just showed that it is bounded over $\mathcal{S}(\R^d)$, where it coincides with $\langle f_1, f_2 \rangle \langle \phi_2, \phi_1$. Since Schwartz's class is a dense subspace of $L^2(\R^d)$ it extends to a bounded linear operator for every $\phi_2 \in L^2(\R^d)$. Similarly, for fixed $f_1, f_2 \in L^2(\R^d)$ and $\phi_2 \in L^2(\R^d)$ the mapping $\phi_1 \in L^2(\R^d) \mapsto \langle \V_{\phi_1} f_1, \V_{\phi_2} f_2 \rangle$ is an antilinear functional that coincides with $\langle f_1, f_2 \rangle \langle \phi_2, \phi_1$ over $\mathcal{S}(\R^d)$, therefore it extends to a bounded linear functional over whole $L^2(\R^d)$.
\end{proof}
\begin{cor}
	If $f, \phi \in L^2(\R^d)$ then
	\begin{equation}\label{STFT is an isometry formula}
		\| \V_{\phi} f\|_2 = \| f \|_2 \| \phi \|_2
	\end{equation} 
	In particular, if $\| \phi \|_2 = 1$, $\V_{\phi}$ is an isometry from $L^2(\R^d)$ into $L^2(\R^{2d})$.
\end{cor}
\begin{proof}
	It is sufficient to consider \eqref{orthogonality relation formula} with $\phi_1 = \phi_2 = \phi$ and $f_1 = f_2 = f$.
\end{proof}
From the Cauchy-Schwarz inequality we immediately see that $\V_{\phi} f$ is in $L^{\infty}(\R^{2d})$:
\begin{equation}\label{STFT is bounded}
	|\V_{\phi} f(x,\omega)| = |\langle f, M_{\omega} T_x \phi\rangle| \overset{\mathrm{C-S}}{\leq} \|f\|_2 \|M_{\omega}T_x \phi\|_2 = \|f\|_2 \|\phi\|_2.
\end{equation}
Combing this with \eqref{STFT is an isometry formula} and using a simple interpolation argument we see that $\V_{\phi}f \in L^p(\R^{2d})$ for every $p \in [2,+\infty]$ and that 
\begin{equation}\label{STFT is in L^p for p>=2}
	\|\V_{\phi}f\|_p \leq \|f\|_2 \|\phi\|_2.
\end{equation}
This result is improved by the following theorem due to Lieb.
\begin{teo}\label{Lieb's inequality}
	If $f,\phi \in L^2(\R^d)$ and $2 \leq p \leq \infty$, then
	\begin{equation}\label{Lieb's inequality formula}
		\| \V_{\phi} \|_p^p = \int_{\R^{2d}} |\V_{\phi}(x,\omega)|^p dxd\omega \leq \left(\dfrac{2}{p}\right)^d \|f\|_2^p \cdot \|g\|_2^p
	\end{equation}
\end{teo}
\begin{proof}
	For this proof we will need to use an alternative form of the STFT:
	\begin{equation*}
		\V_{\phi} f(x,\omega) = \int_{\R^d} f(t) e^{-2\pi i \omega \cdot t} \overline{\phi(t-x)}dt = \F(f T_x \overline{\phi})(\omega)
	\end{equation*}
	Using Cauchy-Schwarz inequality it is immediate to see that $f T_x \overline{\phi} \in L^1(\R^d)$ for every $x \in \R^d$. In addition to that, since $\V_{\phi} f = \F(f T_x \overline{\phi}) \in L^2(\R^{2d})$, from Fubini's theorem we can say that $ \F(f T_x \overline{\phi}) \in L^2(\R^d) $ for almost every $x \in R^d$ and therefore also $f T_x \overline{\phi} \in L^2(\R^d)$ for a.e. $x \in \R^d$. Through an interpolation argument we obtain that $f T_x \overline{\phi} \in L^q(\R^d)$ for every $q \in [1,2]$.
	
	We start considering the $L^p$ norm of $\V_{\phi} f$:
	\begin{align}
		\|\V_{\phi} f\|_p &= \left(\int_{\R^{2d}} |\V_{\phi}f(x,\omega)|^p dxd\omega\right)^{1/p} \overset{\mathrm{Tonelli}}{=} \left[\int_{\R^d} \left(\int_{\R^d} |\V_{\phi} f (x,\omega)|^p d\omega\right)dx\right]^{1/p} =\nonumber \\
						  &= \left[\int_{\R^d} \left(\int_{\R^d} |\F(f T_x \overline{\phi})(\omega)|^p d\omega\right)dx\right]^{1/p} \overset{\eqref{Young inequality sharp}}{\leq} \nonumber\\
						  &\leq A_{p'}^d\left[\int_{\R^d} \left(\int_{\R^d} |f(t) \overline{\phi(t-x)}|^{p'}dt\right)^{p/p'}dx\right]^{1/p} \label{Lieb's inequality intermediate}
	\end{align}
	where the use of Young's inequality \eqref{Young inequality sharp} is justified since we noticed that $fT_x \overline{\phi}$ is in $L^q(\R^d)$ for every $q \in [1,2]$, so in particular it is in $L^{p'}(\R^d)$. Letting $\phi^*(t) = \overline{\phi(-t)}$ and considering the inner integral we have
	\begin{align*}
		\int_{\R^d} |f(t) \overline{\phi(t-x)}|^{p'}dt = \int_{\R^d} |f(t)|^{p'} |\phi^*(x-t)|^{p'}dt = \left(|f|^{p'} * |\phi^*|^{p'}\right)(x),
	\end{align*}
	so the expression in \eqref{Lieb's inequality intermediate} is the $L^{p/p'}(\R^d)$ norm of $|f|^{p'} * |\phi^*|^{p'}$. Since both $f$ and $\phi$ are in $L^2(\R^d)$ and $p' \leq 2$ we have that $|f|^{p'},|\phi^*|^{p'} \in L^{2/p'}(\R^d)$. Thanks to Young's theorem \ref{Young theorem} $|f|^{p'} * |\phi^*|^{p'}$ belongs to $L^r(\R^d)$, where $r$ is given by:
	\begin{equation*}
		\dfrac{1}{(2/p')} + \dfrac{1}{(2/p')} = 1 + \dfrac{1}{r} \implies r = \dfrac{1}{p'-1} = \dfrac{1}{\frac{p}{p-1}-1} = p-1 = \dfrac{p}{p'}
	\end{equation*}
	therefore, using the sharp version of Young's inequality \eqref{Young inequality sharp} in \eqref{Lieb's inequality intermediate} we obtain:
	\begin{align*}
		\|\V_{\phi} f\|_p &\leq A_{p'}^d \left(A_{2/p'}^d A_{2/p'}^d A_{(p/p')'}^d \| \,|f|^{p'} \|_{2/p'} \|\,|\phi^*|^{p'}\|_{2/p'} \right)^{1/p'}.
	\end{align*}
	However $\| \,|f|^{p'} \|_{2/p'} = (\int_{\R^d} (|f(x)|^{p'})^{2/p'}dx)^{p'/2} = \|f\|_2^{p'}$ and from a direct calculation (which can be found in \ref{constant in Lieb's inequality calculation}) one can see that $A_{p'}^d A_{2/p'}^{2d/p'}A_{(p/p')'}^{d/p'}=(2/p)^{d/p}$, which corresponds to the desired result.
\end{proof}

From a direct computation one can see that the adjoint operator of the STFT operator $\V_{\phi}$ is given by the following expression:
\begin{equation}\label{STFT adjoint}
	\V_{\phi}^* g(t) = \int_{\R^{2d}} g(x,\omega) \phi(t-x) e^{2 \pi i \omega \cdot t} dxd\omega = \int_{\R^{2d}} g(x,\omega) M_{\omega}T_x \phi (t) dxd\omega\quad \forall g \in L^2(\R^{2d})
\end{equation}
This adjoint operator appears in the following nice property
\begin{teo}\label{inversion formula theorem}
	Let $f \in L^2(\R^d)$ and $\phi, \gamma \in L^2(\R^{2d})$ such that $\langle \phi, \gamma \rangle \neq 0$. Then:
	\begin{equation}\label{inversion formula}
		f(t) = \dfrac{1}{\langle \phi, \gamma \rangle} \V_{\gamma}^* \V_{\phi} f(t) = \dfrac{1}{\langle \phi, \gamma \rangle} \int_{\R^{2d}} \V_{\phi}f(x,\omega)M_{\omega}T_x \gamma (t) dxd\omega \quad \forall t \in \R^d
	\end{equation}
\end{teo}
\begin{proof}
	Given $f,g \in L^2(\R^d)$ from \eqref{orthogonality relation formula} we have:
	\begin{equation*}
		\langle \V_{\phi} f, \V_{\gamma} g \rangle = \langle f,g \rangle \langle \gamma, \phi \rangle.
	\end{equation*}
	On the other hand:
	\begin{equation*}
		\langle \V_{\phi} f, \V_{\gamma} g \rangle = \langle \V_{\gamma}^* \V_{\phi} f, g \rangle,
	\end{equation*}
	therefore, letting $I$ be the identity operator over $L^2(\R^d)$, we have:
	\begin{align*}
		\langle \V_{\gamma}^* \V_{\phi} f, g \rangle = \langle f,g \rangle \langle \gamma, \phi \rangle \implies \langle (\V_{\gamma}^* \V_{\phi} - \langle \gamma, \phi \rangle I)f, g \rangle = 0.
	\end{align*}
	Since this holds for every $g \in L^2(\R^d)$ necessarily:
	\begin{equation*}
		(\V_{\gamma}^* \V_{\phi} - \langle \gamma, \phi \rangle I)f = 0 \implies \dfrac{1}{\langle \gamma, \phi \rangle}\V_{\gamma}^* \V_{\phi} f = f.
	\end{equation*}
\end{proof}
Therefore the adjoint operator $\V_{\gamma}^*$ acts, in some sense, as an inverse operator. This will be of paramount importance in the following.
\section{Bargmann Transform and Fock Space}\label{section Fock Space and Bargmann transform}
{\color{blue} Throughout this section we will consider the STFT with Gaussian window.} We choose
\begin{equation}\label{gaussian normalized}
	\varphi(x) = 2^{d/4} e^{-\pi |x|^2}
\end{equation}
where $|x|^2 = \sum_{k=1}^d x_k^2$ is the Euclidean norm of $x$ in $\R^d$. The factor $2^{d/4}$ is chosen so that $\|\varphi\|_2=1$. The STFT with Gaussian window becomes
\begin{equation}
	\V_{\varphi}f(x,\omega) = 2^{d/4} \int_{\R^d} f(t) e^{-\pi |t-x|^2} e^{-2 \pi i \omega \cdot t} dt
\end{equation}
Our aim now is to rearrange the terms in the above expression in order to make $z=x+i\omega \in \C^d$ appear. We want to highlight the fact that when talking about complex quantities $|z|^2 = z \overline{z} = |x|^2 + |\omega|^2$. {\color{blue}  Aggiungere remark sulla notazione usata}.
\begin{align*}
	\V_{\varphi}f(x,\omega) &= 2^{d/4} \int_{\R^d} f(t) e^{-\pi |t|^2 + 2 \pi x \cdot t - \pi |\omega|^2} e^{ - 2 \pi i \omega \cdot t} dt\\
							&= 2^{d/4} \int_{\R^d} f(t) e^{-\pi |t|^2} e^{2 \pi (x - i \omega) \cdot t} e^{-\frac{\pi}{2}\left(|x|^2 - 2 i x \cdot \omega - |\omega|^2\right)} e^{-\frac{\pi}{2}\left(|x|^2 + |\omega|^2 + 2 i x \cdot \omega\right)}dt\\
							&= 2^{d/4} e^{-\pi i x \cdot \omega} e^{-\frac{\pi}{2}\left(|x|^2 + |\omega|^2\right)} \int_{\R^d} f(t) e^{-\pi |t|^2} e^{2 \pi (x-i\omega) \cdot t}e^{-\frac{\pi}{2}(x-i\omega)^2}dt
\end{align*}
The rearrangement may seem arbitrary but actually is done is such a way that inside the integral $x$ and $\omega$ enter only via $\overline{z}$.
\begin{defi}\label{Bargmann transform}
	The \textbf{Bargmann transform} of a function $f$ on $\R^d$ is the function $\Barg f$ on $\C^d$ given by
	\begin{equation}\label{Bargmann transform formula}
		\Barg f(z) = 2^{d/4} \int_{\R^d} f(t) e^{2 \pi t \cdot z - \pi |t|^2 - \frac{\pi}{2}z^2}dt
	\end{equation}
\end{defi}
We recall that a function defined over $C^d$ is \emph{entire} if it is holomorphic over all $\C^d$.
\begin{defi}\label{Fock space}
	The \textbf{Fock space} $\Fock^2(\C^d)$ is the Hilbert space of all entire functions $F$ on $\C^d$ for which the norm
	\begin{equation}\label{Fock space norm}
		\|F\|^2_{\Fock} = \int_{\C^d} |F(z)|^2 e^{-\pi |z|^2}dz
	\end{equation}
	is finite.
\end{defi}
Clearly the norm of the Fock space is induced by the following scalar product
\begin{equation}\label{Fock space scalar product}
	\langle F,G \rangle_{\Fock} \int_{\C^d} F(z) \overline{G(z)} e^{-\pi |z|^2}dz
\end{equation}
\begin{prop}
	If $f$ is a function on $\R^d$ with polynomial growth then its Bargmann transform $\Barg f$ is an entire function on $\C^d$. Moreover, letting $z = x + i\omega$, the Bargmann transform of $f$ is related to its STFT through the following
	\begin{equation}\label{connection between Bargmann transform and STFT}
		\V_{\varphi} f(x,-\omega) = e^{\pi i x \cdot \omega} \Barg f(z) e^{-\pi |z|^2/2}
	\end{equation}
\end{prop}
\begin{proof}
	contenuto...
\end{proof}
\begin{prop}
	If $f \in L^2(\R^d)$ then
	\begin{equation}
		\|f\|_2 = \left(\int_{\C^d} |\Barg f(z)|^2 e^{-\pi |z|^2}dz\right)^{1/2} = \|\Barg f\|_{\Fock}.
	\end{equation}
	Thus $\Barg$ is an isometry from $L^2(\R^d)$ into $\Fock^2(\C^d)$.
\end{prop}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Localization Operators}\label{chapter localization operators}
One of the main problems in signal analysis or, in general, time-frequency analysis is to extract some informations about signal in order to analyse it. {\color{red}We saw that STFT can be a tool for this purpose, however it has some practical problems: it doubles the dimension of the output and it is highly redundant (SE AVANZA TEMPO).}\\
In this chapter we see two possible ways to deal with the problem of creating operators able to localize a signal both in time and frequency.
\section{Localization with projections}\label{Localization with projections section}
The most straightforward way to localize a signal, say in the time domain, is to use a sharp cut-off, which means a characteristic function. If we suppose to have a signal $f \in L^2(\R^d)$ and we want to localize it in a measurable subset $T \subseteq \R^d$ of the time domain we can consider the natural projection operator
\begin{equation}\label{time projection operator}
	P_T : L^2(\R^d) \rightarrow L^2(\R^d) \quad P_T f(t) = \chi_T(t) f(t)
\end{equation}
This is clearly a projection operator, which means that $P_T^2 = P_T = P_T^*$.\\
In the same fashion we can define an operator able to localize on a measurable subset $\Omega \subseteq \R^d$ in the frequency domain. Their definition it is not as direct as the one for time projections but it is still easy to understand
\begin{equation}\label{frequency projection operator}
	Q_{\Omega} : L^2(\R^d) \rightarrow L^2(\R^d) \quad Q_{\Omega} f(t) = \F^{-1} \left(\chi_{\Omega} \F f \right)(t) = \int_{\Omega} \hat{f}(\omega) e^{2 \pi i \omega \cdot t} d\omega
\end{equation}
It is also quite simple to show that this is a projection operator
\begin{align*}
	&Q_{\Omega}^2 = \F^{-1} \chi_{\Omega} \F \F^{-1} \chi_{\Omega} \F = \F^{-1} \chi_{\Omega} \chi_{\Omega} \F = \F^{-1} \chi_{\Omega} \F = Q_{\Omega}\\
	&Q_{\Omega}^* = \left( \F^{-1} \chi_{\Omega} \F\right)^* = \F^* \chi_{\Omega}^* \left(F^{-1}\right)^* = \F^{-1} \chi_{\Omega} \F = Q_{\Omega}
\end{align*}
where we used the fact that the Fourier transform is a unitary operator on $L^2(\R^d)$, namely that $\F^* = \F^{-1}$.\\
Moreover, both operators have norm less or equal than 1, independently of $T$ and $\Omega$:
\begin{align*}
	&\| P_T f \|_{L^2(\R^d)} = \| f \|_{L^2(T)} \leq \| f \|_{L^2(\R^d)} \\
	&\| Q_{\Omega} f \|_{L^2(\R^d)} = \| \F^{-1} \chi_{\Omega} \F f\|_{L^2(\R^d)} \overset{\text{Plancherel}}{=} \| \F f\|_{L^2(\Omega)} \leq \| \F f \|_{L^2(\R^d)} \overset{\text{Plancherel}}{=} \| f \|_{L^2(\R^d)}
\end{align*}

After defining these projection operators we may think to combine them into a single operator
\begin{equation*}\label{composition of projections}
	Q_{\Omega} P_T, \; P_T Q_{\Omega} : L^2(\R^d) \rightarrow L^2(\R^d)
\end{equation*}
which hopefully is able to localize a signal both in time and frequency ``near'' to the set $T \times \Omega$.\\
It is clear that these operators are linear and bounded, in particular their norms are less or equal than 1. We notice that these operators are one the adjoint of the other, indeed:
\begin{equation}\label{projection operators adjoint}
	(Q_{\Omega} P_T)^* = P_T^* Q_{\Omega}^* = Q_{\Omega} P_T.
\end{equation}

Up to now the only (essential) hypothesis on $T$ and $\Omega$ is that they are measurable. Clearly, by adding some requirements on $T$ and $\Omega$ we expect $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ to gain some properties.
\begin{prop}\label{projection operators are Hilbert-Schmidt}
	Let $T,\Omega \subset \R^d$ with finite measure. Then $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ are Hilbert-Schmidt integral operators of the form
	\begin{align}
		Q_{\Omega} P_T f(x) &= \int_{\R^d} K(x,t) f(t) dt \\
		P_T Q_{\Omega} f(x) &= \int_{\R^d} \overline{K(t,x)}f(t)dt
	\end{align}
	where
	\begin{equation}\label{integral kernel projection operators}
		K(x,t) = \chi_T(t) \int_{\Omega} e^{2 \pi i \omega \cdot (x-t)}d\omega
	\end{equation}
	which has $\| K \|_{L^2(\R^{2d})} = \sqrt{|T||\Omega|}$.
\end{prop}
\begin{proof}
	Given $f \in L^2(\R^d)$ we have:
	\begin{align*}
		Q_{\Omega}P_T f(x) = \int_{\Omega} e^{2 \pi i \omega \cdot x} \left(\int_{T} e^{-2\pi i \omega \cdot t} f(t)dt\right)d\omega \overset{\mathrm{Fubini}}{=} \int_{\R^d} \chi_T(t) \left(\int_{\Omega} e^{2 \pi i \omega \cdot (x -t)} d\omega\right)f(t)dt
	\end{align*}
	where the use of Fubini's theorem is allowed since $\Omega$ and $T$ have finite measure. This gives us the expression of $Q_{\Omega}P_T$. In order to obtain also the expression for $P_T Q_{\Omega}$ it suffices to recall from \eqref{projection operators adjoint} that $(Q_{\Omega} P_T)^* = P_T Q_{\Omega}$. Therefore, the integral kernel of $P_T Q_{\Omega}$ is given by Proposition \ref{condition integral operator self-adjoint}. Lastly, from Theorem \ref{representation of Hilbert-Schmidt integral operator} we have that $\|Q_{\Omega} P_T\|_{\mathrm{HS}} = \|P_T Q_{\Omega}\|_{\mathrm{HS}} = \|K\|_{L^2(\R^{2d})}$, and:
	\begin{align*}
		\|K\|_{L^2(\R^{2d})} &= \left(\int_{\R^{2d}}   |K(x,t)|^2 dxdt\right)^{1/2} =\\
							 &= \left(\int_{\R^{2d}} \chi_T(t) \Bigg| \int_{\R^d} e^{2 \pi i \omega \cdot (x-t)} \chi_{\Omega}(\omega)d\omega \Bigg|^2 dxdt \right)^{1/2} = \\
							 &= \left(\int_{\R^{2d}} \chi_T(t) \big|\F^{-1}(\chi_{\Omega})(x-t)\big|^2dxdt\right)^{1/2} \overset{\mathrm{Tonelli}}{=} \\
							 &= \left[ \int_{\R^d} \chi_T(t) \left(\int_{\R^d} \big|\F^{-1}(\chi_{\Omega})(x-t)\big|^2 dx\right)dt\right]^{1/2} = \\
							 &= \left(\int_{\R^d} \| T_t \F^{-1} (\chi_{\Omega})\|_2^2 \chi_T(t)dt\right)^{1/2} \overset{T_t\,\mathrm{isometry} + \mathrm{Plancherel}}{=}\\
							 &= \| \chi_{\Omega} \|_2 \|\chi_T\|_2 = \sqrt{|T| |\Omega|}
	\end{align*}
\end{proof}
If we compare the integral kernels of $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ we see that $K(x,t) \neq \overline{K(t,x)}$, hence, by proposition \ref{condition integral operator self-adjoint}, we immediately conclude that both operator are not self-adjoint. {\color{red} Since it is better to deal with self-adjoint operators when it comes to spectral properties, it would be nice if we could construct those starting from $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$.} This is easily done by considering:
\begin{align}
	(Q_{\Omega} P_T)^* Q_{\Omega} P_T &= P_T^* Q_{\Omega}^* Q_{\Omega} P_T= P_T Q_{\Omega} P_T\\
	(P_T Q_{\Omega})^* P_T Q_{\Omega} &= Q_{\Omega}^* P_T^* P_T Q_{\Omega} = Q_{\Omega} P_T Q_{\Omega}
\end{align} 
These are, by construction, self-adjoint operators, and since both $Q_{\Omega} P_T$ and $P_T Q_{\Omega}$ are compact operators (thanks to Proposition \ref{projection operators are Hilbert-Schmidt} and Theorem \ref{Hilbert-Schmidt operators are compact and bounded}) they are also compact (we recall that composition between a general operator and a compact one is compact). Hence, by Theorem \ref{self-adjoint compact operators are diagonalizable} they can be diagonalized.\\
{\color{red} Aggiungere qualcosa sulle prolate spheroidal wave functions?}


\section{Daubechies' localization operators}\label{Daubechies' localization operators section}
{\color{red} Projection operators considered in the previous section are powerful tools in signal analysis and quantum mechanics}. Nevertheless they treat time and frequency in a separate way. We already saw that a good tool to simultaneously study a signal in time and frequency is the STFT but we also pointed out its limits. {\color{red} However we can think to construct localization operators using the STFT instead of the Fourier transform}. This is exactly what was done by Ingrid Daubechies in 1988 in \cite{daubechies}. From Theorem \ref{inversion formula theorem} we know that the adjoint operator of $\V_{\phi}$ acts as inverse operator. If we choose a window $\phi \in L^2(\R^{2d})$ normalized, \ref{inversion formula} becomes
\begin{equation*}
	f(t) = \V_{\phi}^* \V_{\phi}f(t)
\end{equation*}
The key idea is to multiply $\V_{\phi} f$ by a \emph{weight function} $F(\omega,t)$, which logically should highlight some features of $\V_{\phi}f$ :
\begin{equation}\label{Daubechies' localization operator def}
	L_{F,\phi} : L^2(\R^d) \rightarrow L^2(\R^d) \quad L_{F,\phi}f(t) = \V_{\phi}^* F \V_{\phi} f(t)
\end{equation}
Related to this localization operator is the sesquilinear form $\L_{F,\phi} : L^2(\R^d) \times L^2(\R^d) \rightarrow \R$ defined by the expression
\begin{equation}\label{sesquilinear form localization operator}
	\L_{F,\phi}(f,g) = \int_{\R^{2d}} F(x,\omega) \V_{\phi}f(x,\omega) \overline{\V_{\phi}g(x,\omega)} dxd\omega
\end{equation}
Indeed, assuming $\L_{F,\phi}$ is bounded, we could define $ L_{F,\phi} f$ through Riesz' representation theorem as the only element of $L^2(\R^d)$ such that
\begin{equation}\label{L_F through duality}
	\L_{F,\phi}(f,g) = \langle L_{F,\phi}f,g \rangle  = \int_{\R^d} L_{F,\phi}f(t) \overline{g(t)}\quad \forall g \in L^2(\R^d)
\end{equation}
and therefore $L_{F,\phi}$ as the function which maps $f$ into its representation.

\begin{prop}\label{F in L^p L_F bounded}
	Let $F \in L^p(\R^{2d})$ for $p \in [1,+\infty]$. Then $L_{F,\phi}$ is bounded and $\|L_{F,\phi}\| \leq \|F\|_p$.
\end{prop}
\begin{proof}
	Let $f,g \in L^2(\R^d)$. We have:
	\begin{align*}
		|\L_{F,\phi} (f,g)| \leq \int_{\R^{2d}} |F(x,\omega)| |\V_{\phi}f(x,\omega)| |\V_{\phi}g(x,\omega)| dxd\omega 
	\end{align*}
	From \eqref{STFT is in L^p for p>=2} we know that, given $f \in L^2(\R^d)$, $\V_{\phi} f \in L^p(\R^d)$ for every $p \in [2,+\infty]$ and $\|\V_{\phi} f\|_p \leq \|f\|_2$. We want to find an exponent $q \geq 2$ in order to apply (generalized) H\"older's inequality:
	\begin{align*}
		\dfrac{1}{p} + \dfrac{1}{q} + \dfrac{1}{q} = 1 \implies q =  \dfrac{2p}{p-1},
	\end{align*}
	which is greater or equal than 2 regardless of $p$. Applying H\"older's inequality with exponents $p$, $q$ and $q$ we have:
	\begin{align*}
		|\L_{F,\phi} (f,g)| \leq \|F\|_p \|\V_{\phi}f\|_q \|\V_{\phi}g\|_q \leq \|F\|_p \|f\|_2 \|g\|_2.
	\end{align*}
	Taking the supremum above all normalized $f,g \in L^2(\R^d)$ gives us the boundedness of $\L_{F,\phi}$ and, in the end, of $L_{F,\phi}$.
\end{proof}

In previous section we managed to prove that projection operators $Q_{\Omega}P_T$ and $P_T Q_{\Omega}$ are Hilbert-Schmidt operators, provided both $T$ and $\Omega$ have finite measure, which is equivalent to asking that $\chi_T$ and $\chi_{\Omega}$ are in $L^1(\R^d)$. An analogous result holds for Daubechies's localization operators.
\begin{teo}\label{F integrable L_F Hilbert-Schmidt}
	Let $F \in L^1(\R^{2d})$. Then $L_{F,\phi}$ is an Hilbert-Schmidt integral operator with kernel
	\begin{equation}\label{integral kernel localization operator}
		K_F(s,t) = \int_{\R^{2d}} F(x,\omega) M_{\omega} T_x \phi(s) \overline{M_{\omega} T_x \phi(t)} dx d\omega
	\end{equation}
	Moreover $\|K_F\|_2 \leq \|F\|_1$.
\end{teo}
\begin{proof}
		Let $f,g \in L^2(\R^d)$. We begin showing that $F(x,\omega) f(t) \overline{M_{\omega}T_{x} \phi(t)} \, \overline{g(s)}M_{\omega}T_x \phi(s)$ belongs to $L^1(\R^{2d} \times \R^d \times \R^d)$:
	\begin{align*}
		&\int_{\R^{4d}} |F(x,\omega) f(t) \overline{M_{\omega}T_{x} \phi(t)} \, \overline{g(s)}M_{\omega}T_x \phi(s)| dx d\omega dt ds \overset{\mathrm{Tonelli}}{=}\\
		&= \int_{\R^{2d}} |F(x,\omega)|\left( \int_{\R^d} |f(t)| |M_{\omega}T_x \phi(t)|dt\right) \left( \int_{\R^d} |g(s)| |M_{\omega}T_x g(s)|ds\right)dx d\omega \overset{\mathrm{C-S}}{\leq} \\
		&\leq \|f\|_2 \|\phi\|_2 \|g\|_2 \|\phi\|_2 \int_{\R^{2d}} |F(x,\omega)|dx d\omega = \|F\|_1 \|f\|_2 \|g\|_2
	\end{align*}
	Now we can apply Fubini's theorem in the expression of $\langle L_{F,\phi}f,g\rangle$:
	\begin{align*}
		\langle L_{F,\phi}f,g \rangle &= \int_{\R^{2d}} F(x,\omega) \left(\int_{\R^d} f(t) \overline{M_{\omega}T_x \phi(t)}dt\right) \overline{\left(\int_{\R^d} g(s) \overline{M_{\omega}T_x \phi(s)}ds\right)} dx d\omega \overset{\mathrm{Fubini}}{=} \\
									  &= \int_{\R^d} \left[\int_{\R^d}\left( \int_{\R^{2d}} F(x,\omega) M_{\omega}T_x \phi(s) \overline{M_{\omega} T_x \phi(t)} dx d\omega \right) f(t) dt\right] \overline{g(s)}ds = \\
									  &= \int_{\R^d} \left(\int_{\R^d} K_F(s,t) f(t)dt\right) \overline{g(s)} ds.
	\end{align*}
	Since this holds for every $f$ and $g$ we can conclude that $L_{F,\phi} f = \int_{\R^d} K_F(\cdot,t) f(t)dt $. In order to prove that $L_{F,\phi}$ is Hilbert-Schmidt all we have left to do is to show that $K_F \in L^2(\R^{2d})$:
	\begin{align*}
		\|K_F\|_2^2 &= | \langle K_F, K_F \rangle |\leq \int_{\R^{2d}} \left( \int_{\R^{2d}} |F(x,\omega)| |M_{\omega}T_x \phi(s)| |M_{\omega} T_x \phi(t)| dx d\omega\right)\\
					&\phantom{=| \langle K_F, K_F \rangle |\leq \int_{\R^{2d}}} \! {\cdot} \left(\int_{\R^{2d}} |F(y,\xi)| |M_{\xi}T_y \phi(s)| |M_{\xi} T_y \phi(t)| dy d\xi\right) ds dt \overset{\mathrm{Fubini}}{=} \\
					&= \int_{\R^{4d}} |F(x,\omega)| |F(y,\xi)| \left(\int_{\R} |M_{\omega} T_x \phi(t)| |M_{\xi} T_y \phi(t)| dt\right) \\
					&\phantom{= \int_{\R^{4d}} |F(x,\omega)| |F(y,\xi)|} \!\cdot\left(\int_{\R} |M_{\omega}T_x \phi(s)| |F(y,\xi)| |M_{\xi}T_y \phi(s)| ds\right) dx d\omega dy d\xi \overset{\mathrm{C-S}}{\leq}\\
					& \leq \|\phi\|_2^4 \int_{\R^{2d}} |F(x,\omega)| dx d\omega \int_{\R^{2d}} |F(y,\xi)| dy d\xi = \|F\|_1^2.
	\end{align*}
\end{proof}
Reminding Theorem \ref{Hilbert-Schmidt operators are compact and bounded} we observe that, if $F$ is integrable, the corresponding localization operator $L_{F,\phi}$ is compact. Moreover, since we have the explicit expression of the integral kernel, from Proposition \ref{condition integral operator self-adjoint} follows immediately the next sufficient condition on $F$ in order to make $L_{F,\phi}$ self-adjoint.
\begin{prop}\label{condition localization operator self-adjoint}
	If $F \in L^1(\R^{2d})$ is a real-valued function then $L_{F,\phi}$ is self-adjoint.
\end{prop}
We will now prove that localization operators with integrable weight function are trace-class operators. In light of Proposition \ref{trace-class operators are Hilbert-Schmidt}, we highlight that this is a stronger condition than just being an Hilbert-Schmidt operator.
\begin{teo}\label{F integrable L_F trace-class}
	Let $F \in L^1(\R^{2d})$. Then $L_{F,\phi}$ is a trace-class operator. Moreover, given an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$, the following holds:
	\begin{equation}\label{trace of localization operators}
		\sum_{n=1}^{+\infty} |\langle L_{F,\phi}e_n,e_n \rangle| \leq \|F\|_1 \quad \mathrm{tr}L_{F,\phi} = \int_{\R^{2d}} F(x,\omega) dx d\omega
	\end{equation}
\end{teo}
\begin{proof}
	We start proving that $L_{F,\phi}$ is a trace-class operator. Given an orthonormal basis $\{e_n\}_{n \in \N}$ of $L^2(\R^d)$:
	\begin{align*}
		\sum_{n=1}^{+\infty} |\langle L_{F,\phi} e_n, e_n \rangle| &= \sum_{n=1}^{+\infty} \Big| \int_{\R^{2d}} F(x,\omega) \V_{\phi}e_n(x,\omega) \overline{\V_{\phi} e_n(x,\omega)} dx d\omega \Big| \leq \\
																   &\leq \sum_{n=1}^{+\infty} \int_{\R^{2d}} |F(x,\omega)| |\V_{\phi} e_n(x,\omega)\|^2 dxd\omega =\\
																   &= \int_{\R^{2d}} |F(x,\omega)| \sum_{n=1}^{+\infty} |\langle e_n, M_{\omega}T_x \phi \rangle|^2 dx d\omega \overset{\mathrm{Parseval}}{=}\\
																   &= \int_{\R^{2d}} |F(x,\omega)| \|M_{\omega} T_x \phi\|_2^2 dx d\omega = \|\phi\|_2^2 \|F\|_1 = \|F\|_1.
	\end{align*}
	where the exchange between series and integral is due to the monotone convergence theorem. Now that we know that $L_{F,\phi}$ is trace-class we can compute its trace:
	\begin{align*}
		\mathrm{tr}L_{F,\phi} &= \sum_{n=1}^{+\infty} \langle L_{F,\phi}e_n e_n \rangle = \sum_{n=1}^{+\infty}  \int_{\R^{2d}} F(x,\omega) |\V_{\phi}e_n(x,\omega)|^2 dx d\omega=\\
							  &= \lim_{N \rightarrow +\infty} \sum_{n=1}^{N} \int_{\R^{2d}} F(x,\omega) |\V_{\phi}e_n(x,\omega)|^2 dx d\omega =\\
							  &= \lim_{N \rightarrow +\infty} \int_{\R^{2d}} F(x,\omega) \sum_{n=1}^{N} |\V_{\phi}e_n(x,\omega)|^2 dx d\omega.
	\end{align*}
	Since
	\begin{align*}
		|F(x,\omega)| \sum_{n=1}^{N} |\V_{\phi}e_n(x,\omega)|^2 \leq |F(x,\omega)| \sum_{n=1}^{+\infty} |\V_{\phi}e_n(x,\omega)|^2 = |F(x,\omega)| \in L^1(\R^{2d})
	\end{align*}
	we can apply Lebesgue's dominated convergence theorem to conclude that: 
	\begin{align*}
		\mathrm{tr}L_{F,\phi} = \lim_{N \rightarrow +\infty} \int_{\R^{2d}} F(x,\omega) \sum_{n=1}^{N} |\V_{\phi}e_n(x,\omega)|^2 dx d\omega = \int_{\R^{2d}} F(x,\omega) dx d\omega.
	\end{align*}
\end{proof}
So far, except for \ref{F in L^p L_F bounded}, we considered only the case $F \in L^1(\R^{2d})$. As the last result of the section we will deal with the more generic case $F \in L^p(\R^{2d})$ for $p < +\infty$.
\begin{prop}\label{F in L^p L_F compact}
	Let $F \in L^p(\R^{2d})$ with $1 \leq p < \infty$. Then the corresponding localization operator $L_{F,\phi}$ is compact.
\end{prop}
\begin{proof}
	Given $F \in L^p(\R^{2d})$ it is sufficient to consider a sequence $F_n$ of functions in $L^1(\R^{2d})$ such that $F_n \rightarrow F$ in $L^p(\R^{2d})$. For example, we can suppose that $F_n$ are in Schwartz's class $\mathcal{S}(\R^{2d})$, which is a well-known dense subspace of $L^p(\R^{2d})$ for $p < +\infty$. Indeed, from Proposition \ref{F in L^p L_F bounded}, we have that $\| L_{F_n,\phi} - L_{F,\phi}\| \leq \|F_n - F\|_p$, so $L_{F_n,\phi} \rightarrow L_{F,\phi}$ in $\B(L^2(\R^d))$. Since $\mathcal{S}(\R^{2d}) \subset L^1(\R^{2d})$, $L_{F_n,\phi}$ are compact, thus also $L_{F,\phi}$ is.
\end{proof}

\subsection{Spherically Symmetric Weights}\label{spherically symmetric weights section}
In this section we will consider the special case in which the window for the STFT is a Gaussian \eqref{gaussian normalized} and the weight $F$ is spherically symmetric. Letting $r_j^2 = x_j^2 + \omega_j^2$ for $j=1,\ldots,d$ and $r^2=(r_1^2\ldots,r_d^2) \in \R^d$, the hypothesis about $F$ can be rephrased in the following way
\begin{equation}\label{spherically symmetric weight}
	F(x,\omega) = \mathdutchcal{F}(r^2).
\end{equation}
{\color{blue} In order to highlight the dependence of $F$ through $\mathdutchcal{F}$ the corresponding localization operator will be denoted as $L_{\mathdutchcal{F},\varphi}$}.
For this operators a complete characterization of the spectrum and eigenspaces is given in \cite{daubechies}. Before stating and proving the theorem we need to introduce some special function, namely \emph{Hermite functions}. In dimension $d=1$ Hermite functions are given by
\begin{equation}\label{Hermite function 1-dimensional}
	H_k(t) = \dfrac{2^{1/4}}{\sqrt{k!}}\left(-\dfrac{1}{2\sqrt{\pi}}\right)^k e^{\pi t^2} \dfrac{d^k}{dt^k}\left(e^{-2\pi t^2}\right)
\end{equation}
where $k \in \N$ ($\N$ is supposed to contain also 0). Hermite functions have lots of interesting and useful properties which can be found in \cite[][Section 1.7]{folland_harmonic}. We cite some of them which will be useful in the following.
\begin{enumerate}[label=(\roman*)]
	\item $\{H_k\}_{k \in \N}$ is an orthonormal basis of $L^2(\R)$;
	\item $H_0(t) = \varphi(t)$ where $\varphi$ is the normalized Gaussian given by \eqref{gaussian normalized};
	\item Setting $H_{-1} = 0$, the following recursive relation holds
	\begin{equation}\label{Hermite functions recursion}
		2\sqrt{\pi}t H_k = \sqrt{k+1}H_{k+1} + \sqrt{k}H_{k-1} \quad \text{for } k=0,1,\ldots;
	\end{equation}
	\item Hermite functions are eigenfunctions of $\F$, specifically
	\begin{equation}\label{Hermite functions are eigenfunctions of Fourier transform}
		\F H_k =(-i)^k H_k.
	\end{equation}
\end{enumerate}
Hermite functions in generic dimension are given through multiplication of 1-dimensional Hermite functions. Explicitly, given a multi-index $k = (k_1,\ldots,k_d) \in \N^d$ the corresponding Hermite function is given by
\begin{equation}\label{Hermite function d-dimensional}
	H_k(t) = \prod_{j=1}^d H_{k_j}(t_j).
\end{equation}
It is still true that $d$-dimensional Hermite functions (now ranging between all possible multi-indices) form an orthonormal basis of $L^2(\R^d)$ and using \eqref{Hermite functions are eigenfunctions of Fourier transform} it is easy to see that $d$-dimensional Hermite functions are still eigenfunction of the Fourier transform, namely
\begin{equation}\label{d-dimensional Hermite functions are eigenfunctions of Fourier transform}
	\F H_k = (-i)^{|k|}H_k
\end{equation}
where $|k|=k_1 + \cdots k_d$ is the length of the multi-index.
\begin{teo}\label{eigenvalues and eigenvectors localization operator spherically symmetric weight}
	Eigenfuctions of $L_{\mathdutchcal{F},\varphi}$ are the $d$-dimensional Hermite functions $H_k$, with corresponding eigenvalues
	\begin{equation}\label{eigenvalues of localization operators with radial weight}
		\lambda_k = \dfrac{1}{k!}\int_0^{+\infty} {\cdots} \int_0^{+\infty} \mathdutchcal{F}\left(\frac{s_1}{\pi},\ldots,\frac{s_d}{\pi}\right) \left(\prod_{j=1}^{d} s_j^{k_j} \right) e^{-(s_1 + \cdots + s_d)} ds_1 \cdots ds_d
	\end{equation}
\end{teo}
Before proving the theorem we need the following lemma:
\begin{lem}\label{integral of translated Gaussian lemma}
	Given $z \in \C$ we have
	\begin{equation}\label{integral of translated Gaussian formula}
		\int_{\R} e^{-2\pi(t^2 + zt)}dt = \dfrac{1}{2^{1/2}} e^{\pi z^2/2}
	\end{equation}
\end{lem}
\begin{proof}
	Letting $z = \mathrm{Re}z + i\mathrm{Im}z = u + iv$ we have:
	\begin{align*}
		\int_{\R} e^{-2\pi(t^2 + zt)}dt &= \int_{\R} e^{-2\pi(t^2 + ut)} e^{-2\pi i v t}dt = e^{\pi u^2/2} e^{-2\pi(t^2 + ut + u^2/4)}e^{-2\pi i v t}dt\\
										&= e^{\pi u^2/2} \int_{\R} e^{-2\pi(t + u/2)^2} e^{-2\pi i v t}dt = e^{\pi u^2/2} \F(T_{-u/2}e^{-2\pi(\cdot)^2})(v) \overset{\ref{properties of translation modulation and dilation operators}\ref{Fourier transform of translation} + \ref{Fourier transform of Gaussian proposition}}{=}\\
										&= e^{\pi u^2/2} e^{2 \pi i (u/2)v}\dfrac{1}{2^{1/2}}e^{-\pi v^2/2} = \dfrac{1}{2^{1/2}} e^{\pi (u^2 + 2iuv - v^2)/2} = \dfrac{1}{2^{1/2}} e^{\pi z^2/2}
		\end{align*}
\end{proof}
\begin{proof}[Proof of Theorem \ref{eigenvalues and eigenvectors localization operator spherically symmetric weight}]
	Since Hermite functions are an orthonormal basis of $L^2(\R^n)$ it is sufficient to prove that $\langle  L_{\mathdutchcal{F},\varphi}H_k,H_l\rangle = \langle \mathdutchcal{F} \V_{\varphi}H_k, \V_{\varphi}H_l \rangle =  \lambda_k \prod_{j=1}^d \delta_{k_j,l_j}$, which means that the scalar product is different from zero if and only if $k=l$. We start by computing the STFT of an Hermite function
	{\allowdisplaybreaks[1]
	\begin{align*}
		\V_{\varphi}H_k(x,\omega) &= \int_{\R^d} H_k(t) e^{-2\pi i \omega \cdot t}\, 2^{d/4} e^{-\pi|t-x|^2}dt = \int_{\R^d} \prod_{j=1}^d H_{k_j}(t_j) e^{-2\pi i \omega \cdot t}\, 2^{d/4} e^{-\pi|t-x|^2}dt =\\
		&= \prod_{j=1}^{d} 2^{1/4}\int_{\R} H_{k_j}(t_j) e^{-2\pi i \omega_j t_j} e^{-\pi(t_j-x_j)^2}dt_j =\\
		&= \prod_{j=1}^{d} 2^{1/4}\int_{\R} \dfrac{2^{1/4}}{\sqrt{k_j!}}\left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} e^{\pi t_j^2} \dfrac{d^{k_j}}{dt^{k_j}}\left(e^{-2\pi t_j^2}\right) e^{-2\pi i \omega_j t_j} e^{-\pi(t_j-x_j)^2}dt_j = \\
		&= \prod_{j=1}^d \dfrac{2^{1/2}}{\sqrt{k_j!}} \left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} \int_{\R} \dfrac{d^{k_j}}{dt^{k_j}}\left(e^{-2\pi t_j^2}\right) e^{\pi(t_j^2 - 2i\omega_j t_j -t_j^2 + 2t_j x_j - x_j^2)}dt_j = \\
		&= \prod_{j=1}^d \dfrac{2^{1/2}}{\sqrt{k_j!}} \left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} e^{-\pi x_j^2}\int_{\R} \dfrac{d^{k_j}}{dt^{k_j}}\left(e^{-2\pi t_j^2}\right) e^{2\pi(x_j - i\omega_j)t_j}dt_j \overset{\mathrm{integration\ by\  parts}}{=} \\
		&= \prod_{j=1}^d \dfrac{2^{1/2}}{\sqrt{k_j!}} \left(-\dfrac{1}{2\sqrt{\pi}}\right)^{k_j} e^{-\pi x_j^2} [2\pi(x_j - i\omega_j)]^{k_j} (-1)^{k_j}\int_{\R}  e^{-2\pi[t_j^2 - (x_j - i \omega_j)t_j]} dt_j =\\
		&=\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}}2^{1/2}( x_j - i\omega_j)^{k_j} e^{-\pi x_j^2} \dfrac{1}{2^{1/2}} e^{\pi (x_j - i \omega_j)^2/2} = \\
		&= \prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}} (x_j - i \omega_j)^{k_j} e^{-\pi i \omega_j x_j} e^{-\pi(x_j^2 + \omega_j^2)/2} =\\
		&= \left(\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}} (x_j - i \omega_j)^{k_j} \right) e^{-\pi i \omega \cdot x} e^{-\pi(r_1^2 + \cdots + r_d^2)/2}
	\end{align*}}
	Before computing the scalar product between $L_{\mathdutchcal{F},\varphi} H_k$ and $H_l$, we introduce the angular coordinate $\theta_j$ such that $x_j + i \omega_j = r_j e^{i\theta_j}$. Therefore we have:
	\begin{align}
		\langle  L_{\mathdutchcal{F},\varphi}H_k,H_l\rangle &= \int_{\R^{2d}} F(x,\omega) \left(\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j}}{k_j!}} r_j^{k_j}e^{-i k_j \theta_j}\right)e^{-\pi i \omega \cdot x} e^{-\pi(r_1^2 + \cdots + r_d^2)/2} \cdot \nonumber\\
															&\hphantom{=\int_{\R^{2d}} F(x,\omega)} \,\overline{\left(\prod_{m=1}^d \sqrt{\dfrac{\pi^{l_m}}{l_m!}} r_m^{l_m}e^{-i l_m \theta_m}\right)e^{-\pi i \omega \cdot x} e^{-\pi(r_1^2 + \cdots + r_d^2)/2}}dxd\omega= \nonumber\\
															&=\int_{\R^{2d}} F(x,\omega) \left(\prod_{j=1}^d \sqrt{\dfrac{\pi^{k_j + l_j}}{k_j!l_j!}} r_j^{k_j + l_j} e^{i(l_j - k_j)\theta_j} \right) e^{-\pi(r_1^2 + \cdots + r_d^2)}dxd\omega. \label{scalar product Hermite functions}
	\end{align}
	For every pair of coordinated $(x_j,\omega_j)$ we can switch to polar coordinates $(r_j,\theta_j)$. Since $F(x,\omega) = \mathdutchcal{F}(r^2)$ is independent of angular coordinates, only functions depending on those are $e^{i(l_j - k_j)\theta_j}$, for which:
	\begin{equation*}
		\int_0^{2\pi} e^{i(l_j - k_j)\theta_j}d\theta_j = \begin{cases}
			\displaystyle \frac{e^{i(l_j - k_j)\theta_j}}{i(l_j - k_j)} \bigg\rvert_0^{2\pi}=0 \quad & \mathrm{if\ } k_j \neq l_j\\
			\phantom{a}\\
			\displaystyle \int_0^{2\pi} d\theta_j = 2\pi \quad &\mathrm{if\ } k_j=l_j
		\end{cases}
	\end{equation*}
	Therefore, if $k \neq l$, the whole integral is 0, otherwise, letting $k_j = l_j$ in \eqref{scalar product Hermite functions}:
	\begin{align*}
		\langle  L_{\mathdutchcal{F},\varphi}H_k,H_k\rangle &= (2\pi)^d \dfrac{\pi^{|k|}}{k!}\int_0^{+\infty} {\cdots} \int_0^{+\infty} \mathdutchcal{F}(r_1^2,\ldots,r_d^2) e^{-\pi(r_1^2 + \cdots + r_d^2)}\left(\prod_{j=1}^{d} r_j^{2k_j + 1} \right) dr_1 \cdots dr_d=\\
															&= \dfrac{1}{k!} \int_0^{+\infty} \cdots \int_0^{+\infty} \mathdutchcal{F}(r_1^2,\ldots,r_d^2) e^{-\pi(r_1^2 + \cdots + r_d^2)} \left(\prod_{j=1}^{d} (\pi r_j^2)^{k_j} \right) \pi r_1 dr_1 \cdots \pi r_d dr_d
	\end{align*}
	where $k! = k_1! \cdots k_d!$. With the change of variable $s_j = \pi r_j^2$, we finally obtain:
	\begin{equation*}
		\langle  L_{\mathdutchcal{F},\varphi}H_k,H_k\rangle = \dfrac{1}{k!} \int_0^{+\infty} \cdots \int_0^{+\infty} \mathdutchcal{F}\left(\frac{s_1}{\pi},\ldots,\frac{s_d}{\pi}\right) \left(\prod_{j=1}^{d} s_j^{k_j} \right) e^{-(s_1 + \cdots + s_d)} ds_1 \cdots ds_d
	\end{equation*}
	which is exactly the expression \eqref{eigenvalues of localization operators with radial weight}.
\end{proof}
In conclusion, we will consider two meaningful examples, namely when the weight $F$ is the characteristic function of a disk centred around the origin and when it is a Gaussian. In order to make computations easier we confine ourselves in the case $d=1$.
\begin{es}[Localization on a disk]
	We study arguably the most simple case, namely when $F$ is the characteristic function of the disk $\mathdutchcal{B}_R = \{(x,\omega) \in \R^2 : x^2 + \omega^2 \leq R^2\}$:
	\begin{equation*}
			F(x,\omega) = \mathdutchcal{F}(r^2) = \begin{cases}
				1 \quad \mathrm{if\ } x^2 + \omega^2 = r^2 \leq R^2\\
				0 \quad \mathrm{otherwise}
			\end{cases} 
	\end{equation*}
	In order to highlight that $F$ is the characteristic function of $\mathdutchcal{B}_R$ we let $L_{\mathdutchcal{F},\varphi} = L_{\mathdutchcal{B}_R,\varphi}$. Noticing that $\mathdutchcal{F}(\frac{s}{\pi}) = \chi_{[0,\pi R^2]} (s)$, expression \eqref{eigenvalues of localization operators with radial weight} brings to:
	\begin{align*}
		\lambda_k(R) = \dfrac{1}{k!} \int_{0}^{+\infty} \chi_{[0,\pi R^2]}(s) s^k e^{-s} ds = \dfrac{1}{k!} \int_{0}^{\pi R^2} s^k e^{-s} ds = \gamma(k+1,\pi R^2)
	\end{align*}
	where $\gamma$ is the lower incomplete gamma function. An easy integration by part, when $k \geq 1$, leads to:
	\begin{equation*}
		\int_{0}^{\pi R^2} s^k e^{-s} ds = -(\pi R^2)^k e^{-\pi R^2} + k \int_{0}^{\pi R^2} s^{k-1} e^{-s} ds.
	\end{equation*}
	Iterating this process gives us the following formula for the $k$-th eigenvalue:
	\begin{equation*}
		\lambda_k = 1 - e^{-\pi R^2} \sum_{j=0}^{k} \dfrac{(\pi R^2)^j}{j!}, \quad k=0,1,\ldots
	\end{equation*}
	Since $(\pi R^2)^j/j!$ is strictly positive, it follows immediately that the sequence of eigenvalues is strictly decreasing. Moreover, since $F$ is real-valued, from Proposition \ref{condition localization operator self-adjoint} we have that $L_{\mathdutchcal{B}_R,\varphi}$ is self-adjoint, as well as compact. Therefore, from Corollay \ref{norm is greatest eigenvalue} we conclude that:
	\begin{equation*}
		\| L_{\mathdutchcal{B}_R,\varphi} \| = |\lambda_0| = 1 - e^{-\pi R^2}.
	\end{equation*}
	Recalling the definition of the norm for operators between Hilbert spaces \ref{dual norm}, we obtain that, for every normalized $f \in L^2(\R^d)$:
	\begin{equation*}
		\begin{gathered}
			\lambda_0 = \|L_{\mathdutchcal{B}_R,\varphi} \| \geq |\langle L_{\mathdutchcal{B}_R,\varphi} f, f \rangle | = \int_{\mathdutchcal{B}_R} |\V_{\varphi} f(x,\omega) |^2 dx d\omega \implies \\
			\implies  \int_{\mathdutchcal{B}_R} |\V_{\varphi} f(x,\omega) |^2 \leq 1 - e^{-\pi R^2}.
		\end{gathered}
	\end{equation*}
	The left-hand side of the last expression represents the energy of $\V_{\varphi} f$ concentrated on the disk $\mathdutchcal{B}_R$.
	{\color{blue} Aggiungere commento che colleghi questo risultato a quello nella sezione 6}.
\end{es}

\begin{es}[Localization with Gaussian weight]
	Another natural choice for the weight function $F$ is a Gaussian:
	\begin{equation*}
		F(x,\omega) = e^{-\alpha \pi (x^2 + \omega^2)}
	\end{equation*}
	where $\alpha > 0$ is a dilatation parameter. In this case $\mathdutchcal{F}(\frac{s}{\pi}) = e^{-\alpha s}$, so from \eqref{eigenvalues of localization operators with radial weight} and integrating by parts $k+1$ times we obtain the eigenvalues of $L_{\mathdutchcal{F},\varphi}$:
	\begin{align*}
		\lambda_k = \dfrac{1}{k!} \int_0^{+\infty} s^k e^{-(1+\alpha)s}ds = (1+\alpha)^{-(k+1)}, \quad k=0,1,\ldots
	\end{align*}
	Like the previous case, eigenvalues are already ordered in decreasing order and $F$ is still real-valued, therefore
	\begin{equation*}
		\|L_{\mathdutchcal{F},\varphi}\| = 1+\alpha.
	\end{equation*}
\end{es}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Uncertainty principles}\label{chapter uncertainty principles}

\begin{lem}\label{Heisenberg's uncertainty principle lemma}
	Let $f \in L^2(\R)$. Then
	\begin{equation}\label{Heisenberg's uncertainty principle lemma formula}
		\int_{\R} t^2 |f(t)|^2dt + \int_{\R} \omega^2 |\hat{f}(\omega)|^2d\omega = \dfrac{1}{2\pi} \sum_{k=0}^{+\infty} (2k+1)|\langle f,H_k \rangle|^2
	\end{equation}
	where $H_k$ is the $k$-th Hermite function. In particular we have
	\begin{equation}\label{Heisenberg's uncertainty principle lemma inequality}
		\int_{\R} t^2 |f(t)|^2dt + \int_{\R} \omega^2 |\hat{f}(\omega)|^2d\omega \geq \dfrac{\|f\|_2^2}{2\pi}
	\end{equation}
	with equality if and only if $f$ is a multiple of $H_0$.
\end{lem}
\begin{proof}
	Our aim is to exploit the fact that Hermite functions are an orthonormal basis of $L^2(\R)$ by computing $\langle tf, H_k\rangle$. {\color{red} Before going on we remark that the previous expression is not an $L^2$ scalar product but is a duality between a tempered distribution and a function in the Schwartz class}. To do so, we just need to use the recursive relation \eqref{Hermite functions recursion}:
	\begin{equation*}
		\langle tf, H_k \rangle = \langle f, tH_k \rangle = \dfrac{1}{2\sqrt{\pi}} \left( \sqrt{k+1} \langle f, H_{k+1} \rangle+ \sqrt{k}\langle f,H_{k-1}\rangle \right).
	\end{equation*}
	To compute the similar quantity for $\hat{f}$ we also need to recall that Hermite functions are eigenfunction of the Fourier transform \eqref{Hermite functions are eigenfunctions of Fourier transform}:
	\begin{align*}
		\langle \omega \hat{f}, H_k \rangle &= \langle \hat{f}, \omega H_k \rangle = \dfrac{1}{2\sqrt{\pi}} \left( \sqrt{k+1} \langle \hat{f}, H_{k+1} \rangle + \sqrt{k}\langle \hat{f},H_{k-1}\rangle \right) \overset{\eqref{Hermite functions are eigenfunctions of Fourier transform} + \F\;\mathrm{unitary}}{=} \\
											&= \dfrac{1}{2\sqrt{\pi}} \left( \sqrt{k+1} (-i)^{k+1}\langle f, H_{k+1} \rangle + \sqrt{k}(-i)^{k-1}\langle f,H_{k-1}\rangle \right)=\\
											&= \dfrac{1}{2\sqrt{\pi}} (-i)^{k-1} \left( \sqrt{k}\langle f,H_{k-1}\rangle - \sqrt{k+1} \langle f, H_{k+1} \rangle  \right)
	\end{align*}
	Now, since $H_k$ form an orthonormal basis of $L^2(\R)$ we can use Parseval's identity:
	\begin{align*}
		\int_{\R} t^2 |f(t)|^2dt + \int_{\R} \omega^2 |\hat{f}(\omega)|^2d\omega &= \sum_{k=0}^{+\infty} \left( |\langle tf, H_k \rangle|^2 + |\langle \omega \hat{f}, H_k\rangle|^2 \right) = \\
								 											     &= \dfrac{1}{2\pi}\sum_{k=0}^{+\infty} \left[ (k+1)|\langle f,H_{k+1}\rangle|^2 + k|\langle f,H_{k-1}\rangle| \right] = \\
								 											     &= \dfrac{1}{2\pi}\sum_{k=0}^{+\infty} (2k+1)|\langle f,H_k\rangle|^2.
	\end{align*}	
	Since $(2k+1) \geq 1$ inequality \eqref{Heisenberg's uncertainty principle lemma inequality} immediately proven, and equality hold if and only if $\langle f,H_k \rangle = 0$ for every $k > 1$, which means exactly that $f$ is a multiple of $H_0$.
\end{proof}
\begin{lem}\label{Heisenberg's uncertainty principle lemma d dimensions}
	Let $f \in L^2(\R^d)$. Then, for every $j = 1,\ldots,d$:
	\begin{equation}\label{Heisenberg's uncertainty principle lemma d dimensions formula}
		\int_{\R^d} t_j^2 |f(t)|^2 dt + \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega = \dfrac{1}{2\pi} \sum_{k \in \N_0^d} (2k_j + 1) |\langle f, H_k \rangle|^2
	\end{equation}
	where $\N_0 = \N \cup \{0\}$ and $H_k$ is the d-dimensional Hermite functions related to the multi-index $k \in \N_0^d$. In particular we have
	\begin{equation}\label{Heisenberg's uncertainty principle lemma d dimensions inequality}
		\int_{\R^d} t_j^2 |f(t)|^2dt + \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega \geq \dfrac{\|f\|_2^2}{2\pi}
	\end{equation}
	with equality if and only if $f$ is a multiple of $H_0$.
\end{lem}
\begin{proof}
	contenuto...
\end{proof}
{\color{red}If one of the integrals in \eqref{Heisenberg's uncertainty principle lemma inequality} is infinite the inequality is trivially satisfied, therefore we can focus our attention for those function for which the integrals are finite}. This lemma is sufficient in order to prove Heisenberg's uncertainty principle.
\begin{teo}\label{Heisenberg's uncertainty principle theorem}
	Let $f \in L^2(\R^d)$ and $a,b \in \R^d$. Then:
	\begin{equation}\label{Heisenberg's uncertainty principle formula}
		\left( \int_{\R^d} |t-a|^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} |\omega - b|^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \geq \dfrac{d\|f\|_2^2}{4\pi}
	\end{equation}
	where $|t-a| = \sum_{j=1}^{d}(t_j - a_j)^2$ is the Euclidean distance.
\end{teo}

\begin{proof}
	Firsly, we notice that it is sufficient to prove the inequality when $a=b=0$, since the generic case can be recovered from this special one by means of a phase-space translation. Indeed, given $f \in L^2(\R^d)$ we can consider $g = M_{-b}T_{-a}f$ for which we have:
	\begin{align*}
		|f(t)|^2 &= |(T_a M_b g)(t)|^2 = |e^{2\pi i b \cdot (t-a)} g(t-a)|^2 = |g(t-a)|^2\\
		|\hat{f}(\omega)|^2 &= |\F(T_a M_b g)(\omega)|^2 \overset{\ref{properties of translation modulation and dilation operators}}{=} |M_{-a} T_b \hat{g}(\omega)|^2 = |e^{-2\pi i a \cdot \omega} \hat{g}(\omega - b)|^2 = |\hat{g}(\omega-b)|^2
	\end{align*}
	and $\|f\|_2 = \|g\|_2$. In light of this, we will consider $a=b=0$.
	
	We start proving that, for every component, the following holds:
	\begin{equation}\label{Heisenberg's uncertainty principle component formula}
		\left( \int_{\R^d} t_j^2\, |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} \omega_j^2\, |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \geq \dfrac{\|f\|_2^2}{4\pi}
	\end{equation} We observe that in the left-hand side of \eqref{Heisenberg's uncertainty principle component formula}, apart from a square root, we have the product of two integrals, while in \eqref{Heisenberg's uncertainty principle lemma d dimensions inequality} we have an estimate for the sum of these. The transition from the latter to the former estimate can be done through a dilation argument. Precisely, given $f \in L^2(\R^d)$, we consider the following dilation:
	\begin{equation*}
		g(t) = \lambda^{-d/2}f(t/\lambda) \implies \hat{g}(\omega) = \lambda^{d/2} \F(D_{1/\lambda}g) \overset{\ref{properties of translation modulation and dilation operators}\ref{Fourier transform of dilation}}{=} \lambda^{d/2} \hat{f}(\lambda \omega).
	\end{equation*}
	We remark that this dilation is different from the one considered in Section \ref{Fourier transform and its properties section}, expression \eqref{dilation operator def}. Indeed, now we chose the dilation so that $\|g\|_2 = \|f\|_2$, while previously the dilation was chosen in order to preserve the $L^1$ norm. Putting $g$ in \eqref{Heisenberg's uncertainty principle lemma d dimensions inequality} provides us:
	\begin{align*}
		\dfrac{\|f\|_2^2}{2\pi} &\leq \int_{\R^d} t_j^2 |g(t)|^2dt + \int_{\R^d} \omega_j^2 |\hat{g}(\omega)|^2d\omega =\\
								&= \dfrac{1}{\lambda^d} \int_{\R^d} t_j^2 |f(t/\lambda)|^2dt  + \lambda^d \int_{\R^d} \omega^2 |\hat{f}(\lambda \omega)|^2d\omega = \\
								&= \lambda^2 \int_{\R^d} t_j^2 |f(t)|^2dt + \dfrac{1}{\lambda^2} \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega.
	\end{align*}
	We can choose $\lambda$ in order to minimize the last expression. Thus, deriving with respect to $\lambda^2$ and putting the derivative to 0 we obtain that the minimum is achieved when $\lambda^2 = (\int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2d\omega)^{1/2} (\int_{\R^d} t_j^2 |f(t)|^2dt)^{-1/2}.$ If we substitute this $\lambda$ into the last expression we obtain exactly \eqref{Heisenberg's uncertainty principle component formula}. We can now prove \eqref{Heisenberg's uncertainty principle formula}:
	\begin{align*}
		\left( \int_{\R^d} |t|^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} |\omega|^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} = \\
		=\left( \sum_{j=1}^d \int_{\R^d} t_j^2 |f(t)|^2dt \right)^{1/2} \left( \sum_{j=1}^d \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2}.
	\end{align*}
	We can see the last expression as the product of the Euclidean norm of vectors $( \| t_j f \|_2 )_{j=1}^d$ and $( \| \omega_j \hat{f} \|_2)_{j=1}^d$. Thus, from Cauchy-Schwarz inequality in $\R^d$, we obtain:
	\begin{align*}
		&\left( \int_{\R^d} |t|^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} |\omega|^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \geq \\
		\geq &\sum_{j=1}^d \left( \int_{\R^d} t_j^2 |f(t)|^2dt \right)^{1/2} \left( \int_{\R^d} \omega_j^2 |\hat{f}(\omega)|^2 d\omega \right)^{1/2} \geq \dfrac{d \|f\|_2^2}{4 \pi}.
	\end{align*}
	{\color{red} Manca pezzo sull'uguaglianza...From Lemma \ref{Heisenberg's uncertainty principle lemma d dimensions} we know that the latter inequality becomes an equality if and only if $f$ is }
\end{proof}





For a generic function $f \in L^2(\R^d)$ the left-hand side of \eqref{Heisenberg's uncertainty principle formula} may be infinite, in which case the statement is trivially satisfied. {\color{blue}We shall comment a mathematical interpretation of Heisenberg's uncertainty principle. This can be written in the following form
\begin{equation*}
	\left(\int_{\R^d}|t-a|^2 \dfrac{|f(t)|^2}{\|f\|_2^2} dt\right)^{1/2} \left(\int_{\R^d} |\omega-b|^2 \dfrac{|\hat{f}(\omega)|^2}{\|\hat{f}\|_2^2}d\omega\right)^{1/2} \geq \dfrac{d}{4\pi}
\end{equation*}
so we may directly assume that $f$ is normalized. In such a case $|f|^2$ can be seen as a probability distribution. If these integrals are finite for some $a$ and $b$ are always finite and their minimum is achieved when
\begin{equation*}
	a = \bar{t} = \int_{\R^d} t |f(t)|^2 dt, \quad b = \bar{\omega} = \int_{\R^d} \omega |\hat{f}(\omega)|^2 d\omega
\end{equation*}
which are the mean of $|f|^2$ and $|\hat{f}|^2$, respectively. In this case previous integrals represent the standard deviation of $|f|^2$ and $|\hat{f}|^2$, which we indicate with $\Delta_x f$ and $\Delta_{\omega} f$. It is fair to believe that a function $|f|^2$ is mostly concentrated around its mean and that its standard deviation is a measure of how spread it is. In light of these arguments, Heisenberg's uncertainty principle can written as
\begin{equation*}
	\Delta_x f \cdot \Delta_{\omega} f \geq \dfrac{d}{4\pi}
\end{equation*}
In this form the uncertainty principle has a heuristic yet meaningful interpretation:\\
\emph{a function and its Fourier transform can not be simultaneously too concentrated}.\\
\textbf{AGGIUNGERE INTERPRETAZIONI IN MECCANICA QUANTISTICA E ANALISI DEI SEGNALI}.}
\section{Donoho-Stark's uncertainty principle}\label{section Donoho-Stark's UP}
\begin{defi}\label{epsilon-concetrated def}
	A function $f \in L^2(\R^d)$ is $\mathbf{\varepsilon}$-\textbf{concetrated} on a measurable set $T \subseteq \R^d$ if
	\begin{equation*}
		\left(\int_{T^c} |f(t)|^2 dt \right)^{1/2} \leq \varepsilon \| f \|_2
	\end{equation*}
	where $T^c = \R^d \setminus T$ denotes the complement set of $T$.
\end{defi}
\begin{teo}[Donoho-Stark's uncertainty principle]\label{Donoho-Stark's uncertainty principle theorem}
	Let $f \in L^2(\R^d) \setminus \{0\}$, suppose that $f$ is $\varepsilon_T$-concentrated on $T \subseteq \R^d$ while $\hat{f}$ is $\varepsilon_{\Omega}$-concentrated on $\Omega \subseteq \R^d$. Then
	\begin{equation}\label{Donoho-Stark's uncertainty principle formula}
		|T| \, |\Omega| \geq (1 - \varepsilon_T - \varepsilon_{\Omega})^2
	\end{equation}
\end{teo}
\begin{proof}
	The result is trivial if $T$ or $\Omega$ have infinite measure. Hence we will suppose that they both have finite measure.\\
	Concentration can be stated in an equivalent way through projection operators introduced in Section \ref{Localization with projections section}, indeed:
	\begin{align*}
		&\left( \int_{T^c} |f(t)|^2 dt\right)^{1/2} = \|f - \chi_T f\|_2  = \|f - P_T f\|_2 \leq \varepsilon_{T} \|f\|_2\\
		&\left( \int_{\Omega^c} |\hat{f}(\omega)|^2 d\omega\right)^{1/2} = \|\hat{f} - \chi_{\Omega} \hat{f} \|_2 = \|f - \F^{-1} (\chi_{\Omega} \hat{f}) \|_2 = \|f - Q_{\Omega}f \|_2 \leq \varepsilon_{\Omega} \|f\|_2
	\end{align*}
	In Section \ref{Localization with projections section} we also noticed that $\|Q_{\Omega}\| \leq 1$, hence
	\begin{align*}
		\|f - Q_{\Omega}P_Tf \|_2 &= \|f - Q_{\Omega}f + Q_{\Omega}f - Q_{\Omega}P_T f \|_2 \leq \|f-Q_{\Omega}f \|_2 + \|Q_{\Omega}(f - P_T f) \|_2 \leq \\
								  &\leq \|f-Q_{\Omega}f \|_2 + \|(f - P_T f) \|_2 \leq (\varepsilon_{\Omega} + \varepsilon_T)\|f\|_2.
	\end{align*}
	and consequently
	\begin{align*}
		&\|f\|_2 = \|f - Q_{\Omega}P_T f + Q_{\Omega}P_T f \|_2 \leq \|f - Q_{\Omega}P_T f\|_2 + \| Q_{\Omega}P_T f  \|_2 \implies\\
		\implies &\|Q_{\Omega}P_T f \|_2 \geq \|f\|_2 - \|f - Q_{\Omega}P_T f\|_2 \geq (1-\varepsilon_{\Omega} - \varepsilon_T)\|f\|_2
	\end{align*}
	Thanks to Proposition \ref{projection operators are Hilbert-Schmidt} we know that $\| Q_{\Omega}P_T\|_{\mathrm{HS}} = \sqrt{|T| \, |\Omega|}$ and from Theorem \ref{Hilbert-Schmidt operators are compact and bounded} we know that $\|Q_{\Omega}P_T\| \leq \| Q_{\Omega}P_T\|_{\mathrm{HS}}$, therefore
	\begin{equation*}
		(1-\varepsilon_{\Omega} - \varepsilon_T)\|f\|_2 \leq \|Q_{\Omega}P_T f \|_2 \leq \sqrt{|T| \, |\Omega|} \|f\|_2.
	\end{equation*}
\end{proof}
\section{Lieb's uncertainty principle}\label{section Lieb's UP}
{\color{blue} Up to now we presented two uncertainty principles related to the Fourier transform}. {\color{red}However uncertainty principles can be stated for every time of time-frequency analysis}. {\color{blue}In this and in the following section we present some uncertainty principles for the STFT.}

We start considering a simple form uncertainty principle for the STFT. After this we will see how to turn Lieb's inequality \ref{Lieb's inequality} into an uncertainty principle.
\begin{prop}\label{weak uncertainty principle for STFT}
	Let $f,\phi \in L^2(\R^d)$ normalized, $U \subseteq \R^{2d}$ and $\varepsilon \in  [0,1]$. Suppose that
	\begin{equation*}
		\int_U |\V_{\phi}f(x,\omega)|^2 dxd\omega \geq 1- \varepsilon.
	\end{equation*}
	Then $|U| \geq 1- \varepsilon$.
\end{prop}
\begin{proof}
	From \eqref{STFT is bounded} we see that $|\V_{\phi}f(x,\omega)| \leq 1$ for all $(x,\omega) \in \R^{2d}$, therefore
	\begin{equation}\label{weak uncertainty principle for STFT formula}
		1-\varepsilon \leq \int_U |\V_{\phi}f(x,\omega)|^2 dxd\omega \leq \|\V_{\phi}f\|_{\infty}^2 |U| \leq |U|.
	\end{equation}
\end{proof}

\begin{teo}[Lieb's uncertainty principle]\label{Lieb's uncertainty principle}
	Suppose that $\|f\|_2 = \|\phi\|_2 = 1$. If $U \subseteq \R^{2d}$ and $\varepsilon \in [0,1]$ are such that
	\begin{equation*}
		\int_U |\V_{\phi}f(x,\omega)|^2 dxd\omega \geq 1- \varepsilon.
	\end{equation*}
	Then
	\begin{equation*}\label{Lieb's uncertainty principle formula}
		|U| \geq \sup_{p>2} (1-\varepsilon)^{\frac{p}{p-2}} \left(\dfrac{p}{2}\right)^{\frac{2d}{p-2}}.
	\end{equation*}
\end{teo}
\begin{proof}
	If $|U| = \infty$ the result is trivial hence we can suppose that $U$ has finite measure. It is sufficient to use H\"older's inequality with exponents $p/2$ and $(p/2)' = p/(p-2)$:
	\begin{align*}
		1-\varepsilon &\leq \int_U |\V_{\phi}f(x,\omega)|^2 dxd\omega= \int_{\R^{2d}} |\V_{\phi}f(x,\omega)|^2 \chi_U(x,\omega)dxd\omega \overset{\mathrm{H\"older}}{\leq} \\
					  &\leq \left(\int_{\R^{2d}} |\V_{\phi}f(x,\omega)|^{2 \frac{p}{2}} dxd\omega\right)^{\frac{2}{p}} \left(\int_{\R^{2d}} \chi_U(x,\omega)^{\frac{p}{p-2}}dxd\omega\right)^{\frac{p-2}{p}} \overset{\eqref{Lieb's inequality formula}}{\leq}\\
					  &\leq \left(\dfrac{2}{p}\right)^{\frac{2d}{p}} \| f \|_2^2\, \|g\|_2^2\, |U|^{\frac{p-2}{p}} = \left(\dfrac{2}{p}\right)^{\frac{2d}{p}} |U|^{\frac{p-2}{p}}.
	\end{align*}
	We point out that the use of H\"older's inequality is justified because $U$ has finite measure and, since $\V_{\phi}f \in L^q(\R^{2d})$ for every $q \geq 2$, $|\V_{\phi}f|^2 \in L^q(\R^{2d})$ for every $q \geq 1$. Since this result holds for every $p > 2$, taking the supremum over all possible $p$ leads to \eqref{Lieb's uncertainty principle formula}.
\end{proof}


\section{Nicola-Tilli's uncertainty principle or Faber-Krahn Inequality for the STFT}\label{section Faber-Krahn inequality fot STFT}
Lieb's uncertainty principle and Lieb's inequality are general results for the STFT since they hold for every possible window $\phi \in L^2(\R^d)$. One may think that for specific choices of the window it is possible to obtain improved results. In this last section we present a recent result, due to Nicola and Tilli and presented in \cite{nicolatilli_fk}, about the STFT with Gaussian window. In this work, they considered the following variational problem, related to the maximal energy of the STFT that can be trapped into a measurable set $\Omega \subset \R^{2d}$ with prescribed measure $s > 0$:
\begin{equation*}
	\max_{f \in L^2(\R^d) \setminus \{0\}} \dfrac{\int_{\Omega} |\V_{\varphi} f(x,\omega)|^2 dx d\omega}{\| f\|_2^2}
\end{equation*}
We notice that the numerator can be written also in the following way:
\begin{equation*}
	\int_{\Omega} |\V_{\varphi} f(x,\omega)|^2 dx d\omega = \langle \chi_{\Omega} \V_{\varphi} f, \V_{\varphi} f \rangle = \langle \V_{\varphi}^* \chi_{\Omega} \V_{\varphi} f,f \rangle = \langle L_{\Omega,\varphi} f,f \rangle.
\end{equation*}
where $L_{\Omega,\varphi}$ is the localization operator with weight $\chi_{\Omega}$. Therefore, the maximum is the norm of $L_{\Omega,\varphi}$, so the problem can be rephrased in the following way: what is the measurable set $\Omega \subset \R^{2d}$ with measure $s$ that maximizes the norm of the corresponding localization operator $L_{U,\varphi}$? The problem is completely solved and the solution is presented in the following theorem.
\begin{teo}[Theorem 4.1 \cite{nicolatilli_norm}]\label{faberkrahn theorem}
	For every $f \in L^2(\R^d)$ such that $\|f\|_{L^2} = 1$ and every measurable subset $\Omega \subset \R^{2d}$ with finite measure we have
	\begin{equation}\label{bound STFT nicola-tilli}
		\int_{\Omega}  |\V f(x,\omega)|^2 \dxdo \leq G(|\Omega|)
	\end{equation}
	where $G(s)$ is given by
	\begin{equation}\label{G}
		G(s) \coloneqq \int_0^s e^{\left(-d!\tau\right)^{1/d}} d\tau
	\end{equation}
	Moreover, equality occurs if and only if $f$ is a Gaussian of the kind
	\begin{equation}\label{translated Gaussian}
		f(x) = c e^{2 \pi i  x \cdot \omega_0} \varphi(x-x_0) = c\, \pi(x_0, \omega_0) \varphi (x) \quad x \in \R^d
	\end{equation}
	for some unimodular $c \in \C$ and some $(x_0,\omega_0) \in  \R^{2d}$ and $\Omega$ is equivalent, in measure, to a ball of centre $(x_0,\omega_0)$.
\end{teo}
For the sake of completeness, we mention that the Theorem in \cite{nicolatilli_fk} is presented in a slightly different way, namely:
\begin{equation*}
	\int_{\Omega}  |\V f(x,\omega)|^2 \dxdo \leq \dfrac{\gamma\left(d, \pi(|\Omega|/\omega_{2d})^{1/d}\right)}{(d-1)!}
\end{equation*}
where $\omega_{2d}$ is the volume of the unit ball in $\R^{2d}$ and $\gamma$ is the lower incomplete gamma function. Recalling the definition of $\gamma$:
\begin{equation*}
	\gamma\left(d, \pi(|\Omega|/\omega_{2d})^{1/d}\right) = \int_0^{\pi(|\Omega|/\omega_{2d})^{1/d}} t^{d-1} e^{-t}dt 
\end{equation*}
and since $\omega_{2d} = \pi^d / d!$, through the change of variable $t^d = d! \tau$  one obtains \eqref{bound STFT nicola-tilli}.

Once Theorem \ref{faberkrahn theorem} has been established, arguing like previous section we immediately obtain an uncertainty principle, which is sharp.
{\color{red}\begin{cor}\label{nicola-tilli's uncertainty principle cor}
	Let $f \in L^2(\R^d)$ with $\|f\|_2 = 1$, $\Omega \subset \R^{2d}$ measurable, $\varepsilon \in [0,1)$ and suppose that
	\begin{equation*}
		\int_{\Omega} |\V_{\varphi}(x,\omega)|^2 dx d\omega \geq 1 - \varepsilon.
	\end{equation*}
	Then
	\begin{equation}\label{nicola-tilli's uncertainty principle formula}
		|\Omega| \geq G^{-1} (1-\varepsilon).
	\end{equation}
\end{cor}
\begin{remark}
	We point out that $G$ is invertible since it is monotonically increasing. Moreover, its image is $[0,1)$, therefore its inverse $G^{-1} : [0,1) \rightarrow [0,+\infty)$ is itself monotonically increasing. This implies that, letting $\varepsilon \rightarrow 0$ in \eqref{nicola-tilli's uncertainty principle formula}, which means that $\Omega$ contains more and more energy, we have $|\Omega| \rightarrow +\infty$. If we compare this with Lieb's uncertainty principle we immediately realise how strong this result is, since letting $\varepsilon=0$ in \eqref{Lieb's uncertainty principle formula} yields to:
	\begin{equation*}
		|\Omega| \geq \sup_{p > 2} \left(\dfrac{p}{2}\right)^{\frac{2d}{p-2}}.
	\end{equation*}
\end{remark}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Recent results}\label{chapter recent results}
\section{Norm of localization operators: results from Nicola-Tilli}\label{section norm of localization operators}
{\color{blue}In Section \ref{Daubechies' localization operators section} we obtained some basic results for the norm of Daubechies' localization operators, independently of the choice of the window $\phi$ for the STFT. It is reasonable to think that, for specific windows those estimates can be improved and, hopefully, find some sharp bounds. Thanks to \ref{faberkrahn theorem} Nicola and Tilli accomplished this task in the case the window of the STFT is a normalized Gaussian. As done previously, since the windows is fixed one for all we will drop the pedex $\phi$.}

As for the results in Section \ref{Daubechies' localization operators section}, assumptions on the weight function $F$ are related to its integrability and boundedness. The problem we are consider is, in fact, finding an optimal estimate of the type
\begin{equation}\label{sharp estimate Nicola-Tilli}
	\|L_F\|_{L^2(\R^d) \rightarrow L^2(\R^d)} \leq C
\end{equation}
where $F$ satisfies the following constraints:
\begin{equation}\label{constraints Nicola-Tilli}
	\|F\|_{\infty} \leq A \quad \text{and} \quad \|F\|_p \leq B.
\end{equation}
Clearly the constant $C$ will depend on $p$, $A$ and $B$. In \cite{nicolatilli_norm} this problem is completely solved: the constant $C$ is computed (explicitly in some cases), weight functions $F$ which achieve this bound are explicitly found and also function $f$ and $g$ such that $|\langle L_f f, g \rangle| = \|L_F\| = C$ are found. Before reporting the main Theorem in \cite{nicolatilli_norm}, we define the following number which will appear many times in the following
\begin{equation}\label{kappa_p}
	\kappa_p \coloneqq \dfrac{p-1}{p}.
\end{equation}
\begin{teo}\label{Nicola Tilli norm theorem}
	Assume $p \in [1,\infty)$, $A \in (0, \infty]$ and $B \in (0,\infty)$ with the additional condition that $A < \infty$ when $p=1$. Let $F$ satisfy the constraints in \eqref{constraints Nicola-Tilli}.
	\begin{enumerate}[label=(\roman*)]
		\item If $p=1$, then
		\begin{equation}\label{Nicola-Tilli bound p=1}
			\|L_F\| \leq A\;G(B/A)
		\end{equation}
		and equality occurs if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$
		\begin{equation}\label{Nicola-Tilli maximal function p=1}
			F(z) = A e^{i\theta} \chi_{\mathdutchcal{B}} (z-z_0) \quad \forall z \in \R^{2d}
		\end{equation}
		where $\mathdutchcal{B} \subset \R^{2d}$ is the ball of measure $B/A$ centred at the origin.
		
		\item If $p>1$ and $\frac{B}{A} \leq \kappa_p^{d/p}$, then
		\begin{equation}\label{Nicola-Tilli bound p>1 first regime}
			\|L_F\| \leq \kappa_p^{d \kappa_p}B,
		\end{equation}
		with equality if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$,
		\begin{equation}\label{Nicola-Tilli maximal function p>1 first regime}
			F(z) = e^{i \theta} \lambda e^{\frac{\pi}{p-1}|z-z_0|^2} \quad \forall z \in \R^{2d}
		\end{equation}
		where $\lambda = \kappa_p^{-d/p}B$.\label{Nicola-Tilli norm theorem case 2}	
		\item If $p>1$ and $\frac{B}{A} > \kappa_p^{d/p}$, then
		\begin{equation}\label{Nicola-Tilli bound p>1 second regime}
			\|L_F\| \leq \int_{0}^{A} G(u_{\lambda}(t))dt,
		\end{equation}
		where $u_{\lambda}(t) = \left[-\log\left(\left(\frac{t}{\lambda}\right)^{p-1}\right) \right]^d$ and $\lambda>A$ is uniquely determined by the condition $p\int_{0}^{A} t^{p-1}u_{\lambda}(t)dt = B^p$. Equality in \eqref{Nicola-Tilli bound p>1 second regime} if and only if, for some $\theta \in \R$ and some $z_0 \in \R^{2d}$,
		\begin{equation}\label{Nicola-Tilli maximal function p>1 second regime}
			F(z) = e^{i\theta} \min \{ \lambda e^{-\frac{\pi}{p-1}|z-z_0|^2}, A \}
		\end{equation}
	\end{enumerate}
	Finally, in all the cases, condition $|\langle L_F f,g \rangle| = \|L_F\|$ holds for some, $f,g \in L^2(\R^d)$ such that $\|f\|_2 = \|g\|_2 = 1$, if and only if both $f$ and $g$ are of the kind \eqref{translated Gaussian}, possibly with different $c$'s, but with the same $(x_0,\omega_0) \in \R^{2d}$ which coincides with the centre of $F$.
\end{teo}
We will not give the proof of these results since some of its parts are similar to the one we will see in the following section. Moreover, we point out that the case $A=
\infty$ means we are dropping the $L^{\infty}$ constraint.

\section{Generic case}
In this section we will deal with a generalized version of the problem considered in \cite{nicolatilli_norm}. Indeed, we want to find the optimal constant $C$ such that
\begin{equation*}
	\| L_F \|_{L^2(\R^d) \rightarrow L^2(\R^d)} \leq C
\end{equation*}
under the following constraints on $F$:
\begin{equation}\label{constraints generic case}
	\|F \|_p \leq A \quad \text{and} \quad \|F\|_q \leq B.
\end{equation}
where $p,q \in (1,\infty)$ and $A,B \in (0,\infty)$. In this setting it is no more possible to find an explicit expression for $C$ and $F$, {\color{red} although they can be easily computed numerically.}

Theorem \ref{Nicola Tilli norm theorem}\ref{Nicola-Tilli norm theorem case 2} includes the case when $F$ satisfies just an $L^p$ constraint by taking $A$ ($L^{\infty}$ constraint) equal to $\infty$. In the current setting we have an $L^p$ and an $L^q$ bound, hence, thanks to \ref{Nicola-Tilli bound p>1 first regime}, it is straightforward to see that
\begin{equation*}
	\| L_F\|_{L_2 \rightarrow L_2} \leq \min\{\kappa_p^{d\kappa_p}A, \, \kappa_q^{d\kappa_q}B\}.
\end{equation*}
Suppose that the first term is smaller than the second, which means:
\begin{equation}\label{condition B/A first}
	\kappa_p^{d\kappa_p}A \leq \kappa_q^{d\kappa_q}B \iff \dfrac{B}{A} \geq \left(\dfrac{\kappa_p^{\kappa_p}}{\kappa_q^{\kappa_q}}\right)^d
\end{equation}
Clearly, for $B$ sufficiently large we expect that the solution of current problem is the same as the one with just an $L^p$ constraint, namely the one given by \eqref{Nicola-Tilli maximal function p>1 first regime}. Therefore, we want to compare its $L^q$ norm with the bound given by $B$:
\begin{align*}
	\| F \|_q^q &= \int_{\R^{2d}} |F(z)|^q dz = \lambda^q \int_{\R^{2d}} e^{-\frac{q\pi}{p-1}|z-z_0|^2} dz \overset{z' = \left(\frac{q\pi}{p-1}\right)^{1/2}(z-z_0)}{=}\\
	&= \lambda^q \left(\dfrac{p-1}{q\pi}\right)^d \int_{\R^{2d}} e^{-|z'|^2}dz' = \lambda^q \left(\dfrac{p-1}{q\pi}\right)^d \pi^d = \lambda^q \left(\dfrac{p-1}{q}\right)^d.
\end{align*}
Since we want $F$ to satisfy the $L^q$ constraint we should have
\begin{equation*}
	\lambda \left(\dfrac{p-1}{q}\right)^{d/q} \leq B \overset{\lambda = \kappa_p^{-d/p}A}{\implies} \left(\dfrac{p}{p-1}\right)^{d/p} \left(\dfrac{p-1}{q}\right)^{d/q}A \leq B
\end{equation*}
which is equivalent to
\begin{equation*}\label{condition B/A second}
	\dfrac{B}{A} \geq \kappa_p^{d\left(\frac1q - \frac1p\right)}\left(\dfrac{p}{q}\right)^{\frac{d}{q}}.
\end{equation*}
If this condition were less restrictive than the one given by \eqref{condition B/A first} we would have solved the problem. Unfortunately, this is not the case. Indeed it is always true, regardless of $p$ and $q$, that
\begin{equation}\label{curious inequality between conjugate exponents original}
	\kappa_p^{d\left(\frac1q - \frac1p\right)}\left(\dfrac{p}{q}\right)^{\frac{d}{q}} \geq \left(\dfrac{\kappa_p^{\kappa_p}}{\kappa_q^{\kappa_q}}\right)^d
\end{equation}
The proof of this inequality can be found in \ref{curious inequality between conjugate exponents}.

Despite this fact, at least we can say that, if $B/A \geq \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)}\left(\frac{p}{q}\right)^{\frac{d}{q}}$ or $B/A \leq \kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)}\left(\frac{p}{q}\right)^{\frac{d}{p}}$, the problem is already solved and the solution is given by Theorem \ref{Nicola Tilli norm theorem}. Therefore, from now on, we will suppose to be in the intermediate case, that is
\begin{equation}\label{intermediate regime}
	\kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < \dfrac{B}{A} < \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}
\end{equation}
We notice that the condition is well-posed, since it is actually true that
\begin{align}\label{another inequality between conjugate exponents original}
	\kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}
\end{align}
whenever $p \neq q$ (proof is in \ref{another inequality between conjugate exponents}).

Before tackling the problem in this intermediate regime, we prove a Theorem from \cite{nicolatilli_norm} which gives a bound for $\| L_F \|$ in terms of the distribution function of $|F|$.
\begin{teo}\label{norm limitation}
	Assume $F \in L^p(\R^{2d})$ for some $p \in [1,+\infty)$ and let $\mu(t) = |\{|F|>t\}|$ be the distribution function of $|F|$. Then
	\begin{equation}\label{norm limitation formula}
		\| L_F \| \leq \int_0^{\infty} G(\mu(t))dt
	\end{equation}
	Equality occurs if and only if $F(z)=e^{i\theta}\rho(|z-z_0|)$ for some $\theta \in \R$, $z_0 \in \R^{2d}$ and some nonincreasing function $\rho : [0,+\infty) \rightarrow [0,+\infty)$
\end{teo}
\begin{proof}
	For the sake of brevity we denote the variable $(x,\omega) \in R^{2d}$ as $z$ and therefore $dxd\omega$ as $dz$. Let $f,g \in L^2(\R^d)$ such that $\| f \|_2 = \| g \|_2 = 1$.
	{\color{blue} Since we are in a Hilbert space $\| L_F \|$ can be computed as the supremum of $|\langle L_F f,g \rangle|$ over all normalized $f$ and $g$. Therefore we are interested in estimating the previous scalar product}
	\begin{equation}\label{first inequality}
	\begin{aligned}
		| \langle L_F f, g \rangle |  &= | \L_{F}(f,g) | \leq \int_{\R^{2d}} |F(z)| \cdot |\V f(z)| \cdot | \V g(z) | dz \overset{\text{C-S}}{\leq} \\
									  &\leq \left( \int_{\R^{2d}} |F(z)| \cdot |\V f(z)|^2 dz \right)^{1/2} \left( \int_{\R^{2d}} |F(z)| \cdot |\V g(z)|^2 dz \right)^{1/2} 
	\end{aligned}
	\end{equation}
	Since the result is symmetric in $f$ and $g$ we can study just one of the terms. Letting $m = \esssup |F(z)|$ and assuming $m>0$ (otherwise every result is trivial) we can use the ``layer cake'' representation \cite[][Theorem 1.13]{liebloss}:
	\begin{equation*}
		|F(z)| = \int_0^m \chi_{\{|F|>t\}}(z)dt 
	\end{equation*}
	in order to find
	\begin{align*}
		\int_{\R^{2d}} |F(z)| \cdot |\V f(z)|^2 dz &= \int_{\R^{2d}} \left( \int_0^m \chi_{\{|F|>t\}}(z)dt \right) |\V f(z)|^2 dz \overset{\text{Tonelli}}{=}\\
											 &= \int_0^m \left( \int_{\R^{2d}} \chi_{\{|F|>t\}}(z) |\V f(z)|^2 dz \right)dt = \int_0^m \left( \int_{\{|F|>t\}} |\V f(z)|^2 dz \right)dt
	\end{align*}
	We notice that the quantity in the inner integral is exactly the one in the theorem \ref{faberkrahn theorem}, hence
	\begin{equation}\label{limitation with G}
		\int_{\R^{2d}} |F(z)| \cdot |\V f(z)|^2 dz \leq \int_0^m G\left(|\{|F|>t\}|\right)dt = \int_0^m G(\mu(t)) dt
	\end{equation}
	We point out that since $\mu(t) = 0$ for $t > m$ and that $G(0)=0$, the previous expression is equivalent to \eqref{norm limitation formula}.
	
	Because $p<\infty$, from Proposition \ref{F in L^p L_F compact} we know that $L_F$ is a compact operator, therefore there exist normalized $f$ and $g$ which achieve equality in the supremum of the norm, namely $\langle L_F f, g \rangle = \| L_F \|$. Therefore equality in \eqref{norm limitation formula} occurs if and only if all the previous inequalities become equalities. Equality in \eqref{limitation with G} occurs if and only if
	\begin{equation}\label{first equality}
		\int_{\{|F|>t\}} |\V f(z)|^2 dz = G(\mu(t))
	\end{equation}
	for a.e. $t \in (0,m)$. Thanks to Theorem \ref{faberkrahn theorem}, for just one $t_0 \in (0,m)$ we can infer that $\{|F|>t_0\}$ is (equivalent to) a ball centred in $z_0 = (x_0,\omega_0)$ and that $f$ is a Gaussian of the kind \eqref{translated Gaussian} with the same centre $z_0$. Then, still by theorem \ref{faberkrahn theorem}, since \eqref{first equality} holds a.e in $(0,m)$ and that $f$ is always the same we have that also the other levels sets $\{|F|>t\}$ are equivalent to balls centred at the same $z_0$. Finally, we can extend the result to every $t \in (0,m)$ because $\{|F|>t\} = \bigcup_{s > t} \{|F|>s\}$. Since Theorem \ref{faberkrahn theorem} is a ``if and only if'', these conditions on $F$ and $f$ are also sufficient to guarantee equality in \eqref{limitation with G}. Clearly the same result holds for $g$ which has to be a Gaussian, possibly with different coefficient $c$ but the same centre.
	
	In the end it turns that $|F|$ is spherically symmetric and radially decreasing as claimed in theorem's statement.
	
	Conditions for $f$ and $g$ imply that $\V g  = e^{i \alpha} \V f$ for some $\alpha \in \R$. This provides equality in \eqref{first inequality} when using Cauchy-Schwarz inequality. Lastly we shall prove that also the first inequality in \eqref{first inequality}, that is
	\begin{equation*}
		\left\vert \int_{\R^{2d}} F(z) \V f(z) \overline{\V g(z)} dz \right\vert \leq \int_{\R^{2d}} |F(z)| \cdot | \V f(z)| \cdot |\V g(z)| dz
	\end{equation*}
	becomes an equality, which is true if and only if
	\begin{equation*}
		e^{-i\theta}  \int_{\R^{2d}} F(z) \cdot |\V f(z)|^2 dz = \int_{\R^{2d}} |F(z)| \cdot | \V f(z)|^2 dz
	\end{equation*}
	for some $\theta \in \R$. This, in turn, is equivalent to the condition
	\begin{equation*}
		e^{-i \theta}F(z) \cdot |\V f(z)|^2 = |F(z)| \cdot |\V f(z)|^2 \quad \text{for a.e. } z \in \R^{2d}
	\end{equation*}
	but since $|\V f(z)|^2 > 0$, equality in \ref{first inequality} with $f$ and $g$ as \eqref{translated Gaussian} occurs if and only if $F(z) = e^{i \theta} |F(z)|$.
\end{proof}

In light of the previous Theorem it is natural to seek for sharp upper bound for the right-hand side of \eqref{norm limitation formula}. Since this involves the distribution function $|F|$ we shall search this bound between all the possible distribution functions. In order to do so we need to rephrase constraints \eqref{constraints generic case} in terms of $\mu$. This can be easily done one more time thanks to a more general version of the ``layer cake'' representation (see \cite[][Theorem 1.13]{liebloss} or \cite[][Proposition 1.1.4]{grafakos}):
\begin{equation*}
	\|F\|_p^p = p \int_0^{\infty} t^{p-1}|\{|F|>t\}|dt.
\end{equation*}
Hence, constraints \eqref{constraints generic case} become
\begin{equation}\label{constraints generic case distribution function}
	p \int_0^{\infty} t^{p-1} u(t)dt \leq A^p \quad \text{and} \quad q \int_0^{\infty} t^{q-1} u(t)dt \leq B^q
\end{equation}
and we can define the proper space of possible distribution functions
\begin{equation}\label{distribution function space}
	\mathcal{C} = \{u : (0,+\infty) \rightarrow [0,+\infty) \text{ such that } u \text{ is decreasing and satisfies } \eqref{constraints generic case distribution function}\}.
\end{equation}
Up to now we have rephrased our original question in terms of the following variational problem:
\begin{equation}\label{nonstandard variational problem formulation}
	\sup_{v \in \mathcal{C}} I(v) \quad \text{where} \quad I(v) \coloneqq \int_0^{+\infty} G(v(t))dt
\end{equation}
Firstly, we shall prove existence of maximizers.
\begin{prop}\label{existence of maximizer}
	The supremum in \eqref{nonstandard variational problem formulation} is finite and it is attained by at least one function $u \in \mathcal{C}$. Moreover, every extremal function $u$ achieves equality in at least one of the constraints \eqref{constraints generic case distribution function}.
\end{prop}
\begin{proof}
	Considering, for example, the first constraint in \eqref{constraints generic case distribution function}, we see that
	\begin{equation*}
		t^p u(t) = p \int_0^t \tau^{p-1} u(t) d\tau \overset{u \text{ decreasing}}{\leq} p \int_0^t \tau^{p-1} u(\tau) d\tau \leq A^p
	\end{equation*}
	hence functions in $\mathcal{C}$ are pointwise bounded by $A^p/t^p$. It is straightforward to verify that $G$ in \eqref{G} is increasing, that $G(s) \leq s$ and that $G(s) \leq 1$. Using these properties we have:
	\begin{align*}
		I(u) &= \int_0^{+\infty} G(u(t))dt = \int_0^1 G(u(t))dt + \int_1^{+\infty} G(u(t))dt \overset{G(s) \leq 1}{\leq} 1 + \int_1^{+\infty} G(u(t))dt \overset{G \text{ increasing}}{\leq}\\ &\leq 1 + \int_1^{+\infty} G(A^p/t^p)dt \overset{G(s) \leq s}{\leq} 1 + \int_1^{+\infty} \dfrac{A^p}{t^p}dt < \infty
	\end{align*}
	therefore the supremum in \eqref{nonstandard variational problem formulation} is finite.
	
	Let $\{u_n\}_{n \in \N} \subset \mathcal{C}$ be a maximizing sequence. Since every $u_n$ is pointwise bounded by $A^p/t^p$, thanks to {\color{red}Helly's selection theorem} we can say that, up to a subsequence, $u_n$ converges pointwise to a decreasing function $u$, which is still in $\mathcal{C}$, indeed:
	\begin{equation*}
		\int_0^{+\infty} t^{p-1} u(t) = \int_0^{+\infty} \lim_{n \rightarrow \infty} t^{p-1} u_n(t) dt \overset{\text{Fatou's lemma}}{\leq} \liminf_{n \rightarrow \infty} \int_0^{+\infty} t^{p-1} u_n(t) dt \leq \dfrac{A^p}{p}
	\end{equation*}
	and the same holds for $q$ instead of $p$.
	
	Now we have to prove that $u$ is actually achieving the supremum. We already saw that the following holds:
	\begin{equation*}
		|G(u_n(t))| \leq \chi_{(0,1)}(t) + \dfrac{A^p}{t^p} \chi_{(1,+\infty)}
	\end{equation*}
	and that the left-hand side is a function in $L^1(0,+\infty)$. This allows us to use dominated convergence theorem to conclude that 
	\begin{equation*}
		I(u) = \int_0^{+\infty} G(u(t)) = \lim_{n \rightarrow \infty} \int_0^{\infty} G(u_n(t))dt = \lim_{n \rightarrow \infty} I(u_n) = \sup_{v \in \mathcal{C}} I(v)
	\end{equation*}

	Lastly we need to show that $u$ achieves equality at least in one of the constraints \eqref{constraints generic case distribution function}. Suppose that this is not true. If we let $u_{\varepsilon}(t) = (1+\varepsilon) u(t)$, then for $\varepsilon > 0$ sufficiently small constraints are still satisfied and since $G$ is strictly increasing $I(u_{\varepsilon}) > I(u)$, which contradicts the hypothesis that $u$ is a maximizer.
\end{proof}
{\color{blue} In order to do some ``meaningful'' calculus of variations over $I$ we need to enlarge $\mathcal{C}$, because the monotonicity assumption is quite strict. We will show that removing this hypothesis leaves the supremum unchanged and that maximizers are indeed monotonic.}
\begin{prop}\label{monotonicity of maximizer}
	Let $\mathcal{C}' = \{u : (0,+\infty) \rightarrow [0,+\infty) \text{ such that } u \text{ is measurable and satisfies } \eqref{constraints generic case distribution function}$. Then
	\begin{equation}
		\sup_{v \in \mathcal{C}} I(v) = \sup_{v \in \mathcal{C}'} I(v).
	\end{equation}
	In particular, any function $u \in \mathcal{C}$ achieving the supremum on the left-hand side also achieves it on the right-hand side.
\end{prop}
\begin{proof}
	Let $u \in \mathcal{C}'$. We define its \emph{decreasing rearrangement} as:
	\begin{equation}
		u^*(s) = \sup\{t \geq 0 : |\{u>t\}|>s\},
	\end{equation}
	with the convention that $\sup \emptyset = 0$. It is clear from the definition that $u^*$ is a non-increasing function. Moreover, one can see (\cite[][Section 10.12]{hardy_littlewood_polya}, \cite[][Proposition 1.4.5]{grafakos}) that $u^*$ is right-continuous and that $u$ and $u^*$ are \emph{equi-measurable}, which means that they have the same distribution function. Moreover, we already pointed out that constraints \eqref{constraints generic case distribution function} imply that $u$ is pointwise bounded by, for example, $A^p/t^p$, therefore $u^*$ takes only finite values. Our aim is to show that $u^* \in \mathcal{C}$. Letting $\nu$ be the Radon measure with density $t^{p-1}$, we start proving that $\nu(\{u>s\}) \geq \nu(\{u^* > s\})$, indeed:
	\begin{align*}
		\nu(\{u>s\}) &= \int_{\{u>s\}} t^{p-1} dt \overset{t^{p-1}\; \mathrm{increasing}}{\geq} \int_0^{|\{u>s\}|} t^{p-1} dt  \overset{\mathrm{equi-measurability}}{=}\\
					 &= \int_0^{|\{u^*>s\}|} t^{p-1} dt \overset{u^*\; \mathrm{decreasing}}{\underset{\mathrm{right-continuous}}{=}} \int_{\{u^*>s\}} t^{p-1} dt = \nu(\{u^* > s\}).
	\end{align*}
	Then, using one more time the ``layer cake'' representation:
	\begin{align*}
		\int_{0}^{+\infty} t^{p-1} u(t) dt &= \int_{0}^{+\infty} u(t) d\nu(t) = \int_{0}^{+\infty} \nu(\{u>s\}) ds \geq \\
										   &= \int_{0}^{+\infty} \nu(\{u^* > s\}) ds = \int_{0}^{+\infty} u^*(t) d\nu(t) = \int_{0}^{+\infty} t^{p-1} u^*(t) dt.
	\end{align*}
	If we swap $p$ with $q$ we conclude that $u^* \in \mathcal{C}$. Moreover, always from equi-measurability, we have:
	\begin{align*}
		I(u) &= \int_{0}^{+\infty} G(u(t)) dt = \int_{0}^{+\infty} \int_0^{u(t)} e^{-(d!\tau)^{1/d}} d\tau dt = \int_{0}^{+\infty} \int_{0}^{+\infty} \chi_{\{u>\tau\}}(t) e^{-(d!\tau)^{1/d}} d\tau dt \overset{\mathrm{Tonelli}}{=}\\
			 &=  \int_{0}^{+\infty} |\{u > \tau\}| e^{-(d!\tau)^{1/d}} d\tau = \int_{0}^{+\infty} |\{u^* > \tau\}| e^{-(d!\tau)^{1/d}} d\tau = I(u^*).
	\end{align*}
	Taking the supremum over all possible $u \in \mathcal{C'}$ we have:
	\begin{align*}
		\sup_{v \in \mathcal{C}'} I(v)= \sup_{v \in \mathcal{C}'} I(v^*) \leq \sup_{v \in \mathcal{C}} I(v).
	\end{align*}
	Inequality $\sup_{v \in \mathcal{C}'} I(v) \geq \sup_{v \in \mathcal{C}} I(v)$ is trivial since $\mathcal{C}' \supset \mathcal{C}$.
	
\end{proof}
Before stating and proving the theorem that gives the only maximal function of \eqref{nonstandard variational problem formulation}, we introduce the following notation:
\begin{equation*}\label{Log- def}
	\Log(x) = \max\{-\log(x),\;0\}, \quad x>0.
\end{equation*}
\begin{teo}\label{nonstandard variational problem solution theorem}
	There exist a unique function $u \in \mathcal{C}$ achieving the supremum in \eqref{nonstandard variational problem formulation} that is:
	\begin{equation}\label{nonstandard variational problem solution formula}
		u(t) = \dfrac{1}{d!} \left[\Log\left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right)\right]^d, \quad t>0
	\end{equation}
	where $\lambda_1, \lambda_2$ are both positive and uniquely determined by
	\begin{equation*}
		p\int_0^{+\infty} t^{p-1}u(t)dt = A^p, \quad q\int_0^{+\infty} t^{q-1}u(t)dt = B^q
	\end{equation*}
\end{teo}
\begin{proof}
{\color{blue} We will split the proof in several parts. Firstly we will show that maximizers are given by \eqref{nonstandard variational problem solution formula}. Then we will show that multipliers $\lambda_1$ and $\lambda_2$ are both strictly positive and unique.}

Let $M = \sup\{t \in (0,+\infty)\ : u(t) > 0\}$. From Proposition \ref{existence of maximizer} we know that $u$ has to achieve at least one of the constraints, therefore $M>0$. Consider now a closed interval $[a,b] \subset (0,M)$ and a function $\eta \in L^{\infty}(0,M)$ supported in $[a,b]$. Without loss of generality we can suppose that $\eta$ is orthogonal, in the $L^2$ sense, to $t^{p-1}$ and $t^{q-1}$, explicitly
\begin{equation}\label{variations are orthogonal to constraints}
	\int_a^b t^{p-1} \eta(t) dt=0, \quad \int_a^b t^{q-1} \eta(t) dt=0.
\end{equation}
On $[a,b]$ we have that $u(t) \geq u(b) > 0$, hence, for $|\varepsilon|$ sufficiently small, $u+\varepsilon\eta$ is still a nonnegative function which satisfies \eqref{constraints generic case distribution function}, therefore $u+\varepsilon\eta \in \mathcal{C}'$. Since we are supposing that $u$ is a maximizer, the function $\varepsilon \mapsto I(u+\varepsilon\eta)$ has a maximum for $\varepsilon = 0$. Since $\eta$ is supported in a compact interval we can differentiate under the integral sign and obtain
\begin{equation*}
	0 = \dfrac{d}{d\varepsilon}I(u+\varepsilon\eta) \lvert_{\varepsilon=0} = \int_a^b G'(u(t))\eta(t)dt.
\end{equation*}
We would like to extend this result to every $\eta$ in $L^2(a,b)$ satisfying \eqref{variations are orthogonal to constraints}. {\color{blue}Since $L^{\infty}(a,b)$ is dense in $L^2(a,b)$, there exist a sequence $\{\eta_k\}_{k \in \N} \subset L^{\infty}(a,b)$ such that $\eta_k \rightarrow \eta$ in $L^2(a,b)$. We can consider the projection operator $P$ such that, given $\psi \in L^2(a,b)$, $P\psi$ is the orthogonal projection of $\psi$ onto $ X = \mathrm{span}\{t^{p-1},t^{q-1}\}^{\perp} \subset L^2(a,b)$. Since $P$ is continuous we have that $P\eta_k \rightarrow P\eta = \eta$, hence
\begin{gather}
	0 = \int_a^b G'(u(t)) P\eta_k(t) dt = \langle G'(u), P\eta_n \rangle_{L^2(a,b)} \rightarrow \langle G'(u), \eta \rangle_{L^2(a,b)} = \int_a^b G'(u(t)) \eta(t) dt \implies \nonumber\\
	\implies \int_a^b G'(u(t)) \eta(t) dt = 0. \label{orthogonality of G'}
\end{gather}}
Since \eqref{orthogonality of G'} holds for every $\eta \in X$ it must be that 
\begin{equation*}
	G'(u) \in X^{\perp} = \left(\mathrm{span}\{t^{p-1},t^{q-1}\}^{\perp}\right)^{\perp} = \mathrm{span}\{t^{p-1},t^{q-1}\} \quad \text{in } (a,b).
\end{equation*}
By letting $a \rightarrow 0^+$ and $b \rightarrow M^-$ we then obtain
\begin{equation}\label{expression G'(u)}
	G'(u(t)) = \lambda_1 t^{p-1} + \lambda_2 t^{q-1} \quad \text{for a.e. } t \in (0,M)
\end{equation}
for some $\lambda_1,\lambda_2 \in \R$. Since $u$ is decreasing actually \eqref{expression G'(u)} holds for every $t \in (0,M)$. {\color{red} We point out that this argument enables us to say that maximizers have to achieve equality in both constraints in \eqref{constraints generic case distribution function}. Indeed, if, for example, we had that $q \int_0^{\infty} t^{q-1}u(t)dt < B^q$, the second condition of orthogonality in \eqref{variations are orthogonal to constraints} could be removed (since, for sufficiently small $\varepsilon$, a variation non-orthogonal to $t^{q-1}$ would be admissible), providing us with the solution of the same variational problem but without the $L^q$ constraint. Since we already know that actually this solution does not satisfy the $L^q$ constraint we conclude that $u$ has to achieve equality in both constraints. With the very same thinking we can say that neither $\lambda_1$ nor $\lambda_2$ can be 0.}


 Recalling the expression of \eqref{G} we see that $G'(s) = e^{-(d!s)^{1/d}}$ and {\color{blue} easily invert \eqref{expression G'(u)}}, thus obtaining
\begin{equation}\label{expression u}
	u(t) = \begin{cases}
		\dfrac{1}{d!} \left[-\log\left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) \right]^d & t \in (0,M)\\
		0 & t \in (M,+\infty)
	\end{cases}
\end{equation}
We remark that a priori it was possible that $M=+\infty$, but from the explicit expression of maximizers we see that this is not possible since $u$ has to be nonnegative.

Our main goal now is to show that multipliers $\lambda_1, \lambda_2$ are both positive and unique since this will give us uniqueness for the maximizer.

We start by proving that both multipliers are positive. Suppose that one of them, for example $\lambda_2$, is negative. Consider an interval $[a,b] \subset (0,M)$ and a variation $\eta \in L^{\infty}(0,M)$ supported in $[a,b]$. Thanks to the Gram-Schmidt process we can construct a variation orthogonal to $t^{p-1}$. Since $\eta$ is arbitrary we can also suppose that it is not orthogonal to $t^{q-1}$, in particular we can ask that $\int_{a}^{b}t^{q-1}\eta(t)dt <0$. Therefore, the directional derivative of $G$ along $\eta$ is:
\begin{equation*}
	\int_{a}^{b} G'(u(t))\eta(t)dt = \int_{a}^{b} (\lambda_1 t^{p-1} + \lambda_2 t^{q-1})\eta(t)dt = \lambda_2 \int_{a}^{b} t^{q-1} \eta(t)dt > 0
\end{equation*}
which contradicts the fact that $u$ is a maximizer.

Now that we now that both multipliers are positive we can prove that $u$ is continuous, which is equivalent to say that $M=T$, where $T$ is the unique positive number such that $\lambda_1 T^{p-1} + \lambda_2 T^{q-1} = 1$ (uniqueness of $T$ follows from the positivity of multipliers).

We start supposing that $M < T$, which means that $\lim_{t \rightarrow M^-} u(t) > 0$. Consider the following variation 
\begin{equation*}
	\eta(t) = \left\{
	\begin{aligned}
		-1 + \alpha \frac{t}{M} + \beta,\quad & t \in (M-M\delta,M)\\
		1,\quad					   & t \in (M,M+M\delta)\\
		0,\quad				   & \text{otherwise}
	\end{aligned}\right.
\end{equation*}
where $\delta>0$ is small enough so that $M-M\delta >0$ and $M+M\delta < T$, while $\alpha$ and $\beta$ are constants, depending on $\delta$, to be determined. Since we want this to be an admissible variation we need to impose that $\eta$ is orthogonal to $t^{p-1}$ and $t^{q-1}$. For example, the first condition is:
\begin{align*}
	0 &= \int_{M-M\delta}^{M+M\delta} t^{p-1} \eta(t) dt = -\int_{M-M\delta}^M t^{p-1}dt + \int_{M-M\delta}^M t^{p-1}\left(\alpha \frac{t}{M} + \beta\right) dt + \int_M^{M+M\delta} t^{p-1}dt \overset{\tau=t/M}{=}\\
	  &= M^p \int_{1-\delta}^1 \tau^{p-1}(\alpha \tau + \beta) d\tau - M^p \int_{1-\delta}^1 \tau^{p-1} d\tau + M^p \int_1^{1+\delta} \tau^{p-1} d\tau \overset{1/\delta}{\implies}\\
	  &\implies \fint_{1-\delta}^1 \tau^{p-1}(\alpha \tau + \beta) d\tau = \alpha \fint_{1-\delta}^1 \tau^{p} d\tau + \beta \fint_{1-\delta}^1 \tau^{p-1} d\tau = \fint_{1-\delta}^1 \tau^{p-1} d\tau - \fint_1^{1+\delta} \tau^{p-1} d\tau 
\end{align*}
The equation stemming from the orthogonality with $t^{q-1}$ is analogous. Therefore we obtained a nonhomogeneous linear system for $\alpha$ and $\beta$
\textbf{VA RESO MEGLIO}
\begin{equation}\label{system continuity}
	\begin{pmatrix}
		 \fint_{1-\delta}^1 \tau^{p} d\tau &   \fint_{1-\delta}^1 \tau^{p-1} d\tau\\
		 \fint_{1-\delta}^1 \tau^{q} d\tau &   \fint_{1-\delta}^1 \tau^{q-1} d\tau
	\end{pmatrix}
	\begin{pmatrix}
		\alpha\\
		%\phantom{a} \\
		%\phantom{a} \\
		\beta
	\end{pmatrix}=
	\begin{pmatrix}
		\fint_{1-\delta}^1 \tau^{p-1} d\tau - \fint_1^{1+\delta} \tau^{p-1} d\tau\\
		\fint_{1-\delta}^1 \tau^{q-1} d\tau - \fint_1^{1+\delta} \tau^{q-1} d\tau
	\end{pmatrix}
\end{equation}
{\color{blue}This system has a unique solution if and only if the determinant of the matrix is not 0. We can show this directly:
\begin{align*}
	& \fint_{1-\delta}^1 \tau^{p} d\tau \fint_{1-\delta}^1 \tau^{q-1} d\tau - \fint_{1-\delta}^1 \tau^{q} d\tau \fint_{1-\delta}^1 \tau^{p-1} d\tau=\\
	&= \dfrac{1}{\delta^2} \int_{(1-\delta,1)^2} \left(\tau^p \sigma^{q-1} - \tau^{p-1}\sigma^q\right) d\tau d\sigma = \dfrac{1}{\delta^2} \int_{(1-\delta,1)^2} \tau^{p-1} \sigma^{q-1} \left( \tau - \sigma \right) d\tau d\sigma = \\
	&= \dfrac{1}{\delta^2} \left( \int_{Q_1} \tau^{p-1} \sigma^{q-1} \left( \tau - \sigma \right) d\tau d\sigma + \int_{Q_2} \tau^{p-1} \sigma^{q-1} \left( \tau - \sigma \right) d\tau d\sigma \right)=
\end{align*}
where $Q_1=(1-\delta,1)^2 \cap \{\tau > \sigma\}$ and $Q_2=(1-\delta,1)^2 \cap \{\tau < \sigma\}$. In the second integral we can consider the change of variable that swaps $\tau$ and $\sigma$. In this case the new domain is $Q_1$, hence:
\begin{align*}
	= \dfrac{1}{\delta^2} \int_{Q_1} \left(\tau^{p-1}\sigma^{q-1} - \tau^{q-1}\sigma^{p-1}\right) \left(\tau - \sigma\right) d\tau d\sigma
\end{align*}
In $Q_1$ $\tau - \sigma > 0$ and the sign of $\tau^{p-1}\sigma^{q-1} - \tau^{q-1}\sigma^{p-1}$ is constant, in fact:
\begin{equation*}
	\tau^{p-1}\sigma^{q-1} - \tau^{q-1}\sigma^{p-1} > 0 \iff \left(\dfrac{\tau}{\sigma}\right)^{p-q} > 1 \overset{\tau > \sigma}{\iff} p>q
\end{equation*}
Therefore the determinant of the matrix is always not 0.}\\
%If we see it as a function of $\delta$ we can expand the terms in a Taylor series:
%\begin{align*}
%	& \fint_{1-\delta}^1 \tau^{p} d\tau \fint_{1-\delta}^1 \tau^{q-1} d\tau - \fint_{1-\delta}^1 \tau^{q} d\tau \fint_{1-\delta}^1 \tau^{p-1} d\tau=\\
%	&=\left(1-\dfrac{p}{2}\delta + \dfrac{p(p-1)}{6}\delta^2 + o(\delta^2)\right) \left(1-\dfrac{q-1}{2}\delta + \dfrac{(q-1)(q-2)}{6}\delta^2 + o(\delta^2)\right) +\\
%	&-\left(1-\dfrac{q}{2}\delta + \dfrac{q(q-1)}{6}\delta^2 + o(\delta^2)\right) \left(1-\dfrac{p-1}{2}\delta + \dfrac{(p-1)(p-2)}{6}\delta^2 + o(\delta^2)\right)=\\
%	&=(1-1) + \delta \left(-\dfrac{q-1}{2} - \dfrac{p}{2} + \dfrac{p-1}{2} + \dfrac{q}{2} \right) \ldots\\
%	&=\dfrac{p-q}{12}\delta^2 + o(\delta^2)
%\end{align*}
%therefore the determinant is always non 0.\\
The derivative of $G$ along $\eta$ is nonpositive because $u$ is supposed to be a maximizer, therefore
\begin{align*}
	0 &\geq \int_{M-M\delta}^{M+M\delta} G'(u(t))\eta(t)dt = -\int_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt\, +\\
	  &+ \int_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right)\left(\alpha \dfrac{t}{M}+\beta\right) dt + \int_M^{M+M\delta}dt = \\
	  &= -\int_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt + \lambda_1 M^p \int_{1-\delta}^1 t^{p-1}(\alpha t + \beta) dt +\\
	  &+ \lambda_2 M^q \int_{1-\delta}^1 t^{q-1}(\alpha t + \beta) dt + M\delta
\end{align*}
Dividing by $M\delta$ and rearranging we obtain:
\begin{equation}\label{inequality continuity}
	\begin{aligned}
		\fint_{M-M\delta}^M \left(\lambda_1 t^{p-1} + \lambda_2 t^{q-1}\right) dt \geq  1 &+ \lambda_1 M^{p-1} \fint_{1-\delta}^1 t^{p-1}(\alpha t + \beta) dt\\ &+ \lambda_2 M^{q-1} \fint_{1-\delta}^1 t^{q-1}(\alpha t + \beta) dt
	\end{aligned}
\end{equation}
We notice that the last two terms are exactly the one that appear in the orthogonality condition, therefore, to understand their behavior as $\delta$ approaches 0, we need to study the right-hand side of the system \eqref{system continuity}. If we expand the first term in its Taylor series with respect to $\delta$ we have:
\begin{align*}
	\left(1-\dfrac{p-1}{2}\delta + o(\delta) \right) - \left(1+\dfrac{p-1}{2}\delta + o(\delta) \right) = -(p-1)\delta + o(\delta)
\end{align*}
and the same is for the other term. Since they both are of order $\delta$, if we let $\delta \rightarrow 0^+$ in \eqref{inequality continuity} we obtain
\begin{equation*}
	\lambda_1 M^{p-1} + \lambda_2 M^{q-1} \geq 1
\end{equation*}
Since the function $\lambda_1 t^{p-1}  + \lambda_2 t^{q-1}$ is strictly increasing (because $\lambda_1$ and $\lambda_2$ are both positive) $M \geq T$ which is absurd because we supposed that $M<T$. This allows us to write $u$ as in \eqref{nonstandard variational problem solution formula}.

Lastly we shall prove that multipliers $\lambda_1, \lambda_2$, and hence maximizer, are unique. For this proof it is convenient to express $u$ in a slightly different way:
\begin{equation*}
	u(t) = \dfrac{1}{d!}\left[ \Log\left((c_1t)^{p-1} + (c_2t)^{q-1}\right) \right]^d
\end{equation*} 
To emphasize that $u$ is parametrized by $c_1, c_2$ we may write $u(t;c_1,c_2)$. Now we define
\begin{equation*}
	f(c_1,c_2) = p\ \int_0^T t^{p-1}u(t;c1,c2)dt, \quad g(c_1,c_2) = q\ \int_0^T t^{q-1}u(t;c1,c2)dt
\end{equation*}
We want to highlight that, even if it is not explicit, also $T$ depends on $c_1$ and $c_2$. Nevertheless these functions are differentiable since both $T$ and $u$ are differentiable with respect to $(c_1,c_2)$, functions $t^{p-1}u$ and $t^{q-1}u$ and their derivatives are bounded in $(0,T)$. 
Our maximizer $u$ satisfies the constraints only if $f(c_1,c_2)=A^p, \, g(c_1,c_2) = B^q$. Therefore to prove uniqueness of the maximizer we need to show that level sets $\{f=A^p\}$ and $\{g=B^q\}$ intersect in only a point.\\
First of all we are studying endpoints. For example, if $c_2=0$:
\begin{align*}
	f(c_1,0) &= p\int_0^{1/{c_1}} t^{p-1}\dfrac{1}{d!}\left[-\log(c_1t)^{p-1}\right]^d dt \overset{\tau = c_1t}{=} \\
			 &= \dfrac{p(p-1)^d}{c_1^p d!}\int_0^1 \tau^{p-1}\left[-\log(\tau)\right]^d d\tau = \dfrac{\kappa_p^d}{c_1^p} = A^p \implies c_{1,f} = \dfrac{\kappa_p^{d/p}}{A}
\end{align*}
The same can be done for $g$ and setting $c_1=0$ thus we obtain four points
\begin{equation*}
	c_{1,f} = \dfrac{\kappa_p^{d/p}}{A},\ c_{1,g} = \left(\dfrac{p-1}{q}\right)^{d/q}\dfrac1{B},\ c_{2,f} = \left(\dfrac{q-1}{p}\right)^{d/p}\dfrac1{A},\ c_{2,g} = \dfrac{\kappa_q^{d/q}}{B}
\end{equation*}
In the regime we are considering one has that $c_{1,f} < c_{1,g}$ and $c_{2,f} > c_{2,g}$, indeed
\begin{align*}
	&c_{1,f} < c_{1,g} \iff \dfrac{\kappa_p^{d/p}}{A} < \left(\dfrac{p-1}{q}\right)^{d/q}\dfrac1{B} \iff \dfrac{B}{A} < \kappa_p^{d\left( \frac1{q}-\frac1{p}\right)}\left(\dfrac{p}{q}\right)^{d/q}\\
	&c_{2,f} > c_{2,g} \iff \left(\dfrac{q-1}{p}\right)^{d/p}\dfrac1{A} > \dfrac{\kappa_q^{d/q}}{B} \iff \dfrac{B}{A} > \kappa_q^{d\left( \frac1{q}-\frac1{p}\right)}\left(\dfrac{p}{q}\right)^{d/p}
\end{align*}
which are exactly conditions in \eqref{intermediate regime}.
Since there is this dispositions of these points we expect there is an intersection between level sets. Firstly we notice that, for every $c_1 \in (0,c_{1,f})$, there exist a unique value of $c_2$ for which $f(c_1,c_2) = A^p$. Indeed, from previous computations we notice that $f(c_1,0)$ is a decreasing function hence $f(c_1,0) > A^p$, while $\lim_{c_2 \rightarrow +\infty} f(c_1,c_2) = 0$. The uniqueness of this value follows from strict monotonicity of $f(c_1,\cdot)$, in fact:
\begin{equation}\label{df/dc1}
	\pfrac{f}{c_1}(c_1,c_2) = -\dfrac{p(p-1)}{(d-1)!}c_1^{p-2}\ \int_0^T \dfrac{t^{2(p-1)}}{(c_1t)^{p-1}+(c_2t)^{q-1}} \left[ -\log\left((c_1t)^{p-1} + (c_2t)^{q-1}\right) \right]^{d-1}dt
\end{equation}
is always strictly negative. We point out that the term $\frac{\partial T}{\partial c_1}(c_1,c_2)u(T;c_1,c_2)$, that should appear since $T$ depends on $c_1$, is zero because $u$ is 0 in $T$. The same is true for $g$, therefore on the interval $(0,c_{1,f})$ the level sets of $f$ and $g$ can be seen as the graph of two functions $\varphi, \gamma$. Since $\frac{\partial f}{\partial c_2}, \frac{\partial g}{\partial c_2} < 0$ for every $(c_1,c_2)$, from the implicit function theorem we have that $\varphi$ and  $\gamma$ are differentiable with respect to $c_1$.\\
After defining $\varphi$ and $\gamma$ we want to prove that $(\varphi-\gamma)' < 0$. Still from the implicit function theorem we have
\begin{equation*}
	\begin{split}
		\dfrac{d}{d c_1}(\varphi - \gamma)(c_1) = -\dfrac{\pfrac{f}{c_1}(c_1,\varphi(c_1))}{\pfrac{f}{c_2}(c_1,\varphi(c_1))}  + 
		\dfrac{\pfrac{g}{c_1}(c_1,\gamma(c_1))}{\pfrac{g}{c_2}(c_1,\gamma(c_1))} < 0 \iff \\
		\mathcal{I}(c_1) = \pfrac{f}{c_1}(c_1,\varphi(c_1)) \pfrac{g}{c_2}(c_1,\gamma(c_1)) - \pfrac{f}{c_2}(c_1,\varphi(c_1)) \pfrac{g}{c_1}(c_1,\gamma(c_1)) > 0
	\end{split}
\end{equation*}
As for \eqref{df/dc1} the other derivatives are computed. To simplify the notation we define $h(t;c_1,c_2) = \frac{1}{(d-1)!}\frac{1}{(c_1t)^{p-1}+(c_2t)^{q-1}}\left[ -\log\left((c_1t)^{p-1} + (c_2t)^{q-1}\right) \right]^{d-1}$. From Fubini's theorem we can write the product of the integrals as a double integral
\begin{equation*}
	\begin{split}
		\mathcal{I}(c_1) &=p(p-1)q(q-1)c_1^{p-2}\gamma(c_1)^{q-2} \int_{[0,T]^2} h(t;c_1,\varphi(c_2)) h(s;c_1,\gamma(c_2)) t^{2(p-1)} s^{2(q-1)} dtds +\\ &-p(q-1)q(p-1)c_1^{p-2}\varphi(c_1)^{q-2} \int_{[0,T]^2} h(t;c_1,\varphi(c_2)) h(s;c_1,\gamma(c_2)) t^{p+q-2} s^{p+q-2} dtds
	\end{split}
\end{equation*}
When level sets intersect we have $\varphi(c_1)=\gamma(c_1)$. In this situation we can factorize the terms outside the integral and notice that the sign of $\mathcal{I}$ depends on the sign of:
\begin{equation*}
	\begin{split}
		&\int_{[0,T]^2} h(t;c_1,\varphi(c_1)) h(s;c_1,\gamma(c_1))\left( t^{2(p-1)} s^{2(q-1)} - t^{p+q-2} s^{p+q-2} \right)dtds=\\
		=&\int_{[0,T]^2} h(t;c_1,\varphi(c_1)) h(s;c_1,\gamma(c_1)) t^{p-2}s^{q-2}\left( t^p s^q - t^q s^p \right)dtds\\
	\end{split}
\end{equation*}
In order to simplify the notation once again we set $H(t,s;c_1) = h(t;c_1,\varphi(c_1)) h(s;c_1,\gamma(c_1))$. Let $T_1 = [0,T]^2 \cap \{t>s\}$ and $T_2 = [0,T]^2 \cap \{t<s\}$. We can split the above integral in two parts:
\begin{equation*}
	\begin{split}
		\int_{T_1} H(t,s;c_1)t^{p-2}s^{q-2}\left( t^p s^q - t^q s^p \right)dtds + \int_{T_2} H(t,s;c_1)t^{p-2}s^{q-2}\left( t^p s^q - t^q s^p \right)dtds
	\end{split}
\end{equation*}
Then, considering the change of variables that swaps $t$ and $s$, the domain of integration becomes $T_1$ and since $H$ is symmetric in $t$ and $s$ we have that the previous quantity is equal to
\begin{equation*}
	\begin{split}
		&\int_{T_1} H(t,s;c_1)\left( t^{p-2} s^{q-2} - t^{q-2} s^{p-2} \right) \left( t^p s^q - t^q s^p \right)dtds =\\
		=& \int_{T_1} H(t,s;c_1)\dfrac{1}{t^2 s^2}\left( t^p s^q - t^q s^p \right)^2 dtds
	\end{split}
\end{equation*}
which is strictly positive.

Now we are able to prove the uniqueness of multipliers.\\
First of all, since $(\varphi-\gamma)'<0$ whenever $\varphi(c_1) = \gamma(c_1)$, for every point of intersection there exist $\delta>0$ such that $\varphi(t) > \gamma(t)$ for $t \in (c_1 - \delta, c_1)$ while $\varphi(t) < \gamma(t)$ for $t \in (c_1, c_1 + \delta)$.\\
Define $c_1^* \coloneqq \sup \{c_1 \in [0,c_{1,f}] : \forall t \in [0,c_1] \ \varphi(t) \geq \gamma(t)\}$. This is an intersection point between $\varphi$ and $\gamma$ (if $\varphi(c_1^*) > \gamma(c_1^*)$ due to continuity there would be $\varepsilon > 0$ such that $\varphi(c_1^*+\varepsilon) > \gamma(c_1^*+\varepsilon)$ which contradicts the definition of $c_1^*$) and it is the first one, because we saw that after every intersection point there is an interval where $\varphi < \gamma$. Lastly, since $\varphi(0) > \gamma(0)$ and $\varphi(c_{1,f}) = 0 < \gamma(c_1,f)$ we have that $0 < c_1^* < c_{1,f}$.\\
Suppose now that there is a second point of intersection $\tilde{c}_1$ after the first one. Since immediately after $c_1^*$ we have that $\varphi$ becomes smaller than $\gamma$, this second point of intersection is given by $\tilde{c}_1 = \sup \{c_1 \in [c_1^*,c_{1,f}] : \forall t \in [c_1^*,c_1] \ \varphi(t) \leq \gamma(t)\}$. Considering that this is an intersection point, there exist an interval before $\tilde{c}_1$ where $\varphi$ is strictly greater than $\gamma$ which is absurd, hence $c_1^*$ is the only intersection point between $\varphi$ and $\gamma$.\\
Therefore $(c_1^*, \varphi(c_1^*) = c_2^*)$ is the unique pair of multipliers for which 
\begin{equation*}
	p\int_0^T t^{p-1} u(t;c_1^*,c_2^*)dt = A^p, \quad q\int_0^T t^{q-1} u(t;c_1^*,c_2^*)dt = B^q
\end{equation*}
and, in the end, $u(t;c_1^*,c_2^*)$ is the unique maximizer for \eqref{nonstandard variational problem formulation}.
\end{proof}

\appendix
\chapter{Unconditional convergence}
In some cases we had to deal with series over multiple indices. Formally, the possibility of manipulating these series is related to the property of \emph{unconditional convergence}.
\begin{defi}\label{unconditional convergence def}
	Let $\{x_j\}_{j \in \mathcal{J}}$ be a countable subset of a Banach space $X$. The series $\sum_{j \in \mathcal{J}} x_j$ is said to \textbf{converge unconditionally} to some $x \in X$ if, for every $\varepsilon > 0$, there exist a finite subset $J_0$ of $\mathcal{J}$ such that
	\begin{equation*}
		\| x - \sum_{j \in J} x_j \| \leq \varepsilon
	\end{equation*}
	for every finite set $J \supseteq J_0$.
\end{defi}

The notion of unconditional convergence is of crucial importance in cases where an exchange between an operator and a series or a certain order of summation is required. Here we present two results that are useful in such situations.

\begin{prop}\label{equivalence unconditional convergence}
	Let $\{x_j\}_{j \in \mathcal{J}}$ be a countable subset of a Banach space $X$ and $x \in X$. Then following statements are equivalent:
	\begin{enumerate}[label=(\roman*)]
		\item $\sum_{j \in \mathcal{J}} x_j$ converges unconditionally to $x$;
		\item for every enumeration $\sigma : \N \rightarrow \mathcal{J}$
		\begin{equation*}
			\lim_{N \rightarrow +\infty} \| x - \sum_{n=1}^N f_{\sigma(n)} \| = 0. 
		\end{equation*}
		In particular, $x$ does not depend on the enumeration $\sigma$.
	\end{enumerate}
\end{prop}
\begin{proof}
	contenuto...
\end{proof}

\begin{prop}\label{exchange of series and operator}
	Let $A \in \B(X,Y)$. If $\sum_{j \in \mathcal{J}} x_j$ converges unconditionally to $x$ in $X$, then $\sum_{j \in \mathcal{J}} Ax_j$ converges unconditionally to $Ax$ in $Y$.
\end{prop}
\begin{proof}
	Let $\varepsilon > 0$. By definition, there exist $J_0 \subseteq \mathcal{J}$ finite such that $\| x - \sum_{j \in J} x_j \|_X \leq \varepsilon$ for every finite set $J \supseteq J_0$. Then:
	\begin{align*}
		\|Ax - \sum_{j \in J} Ax_j \|_Y = \|A(x - \sum_{j \in J} x_j) \|_Y \leq \|A\| \|x - \sum_{j \in J} x_j \|_X < \|A\| \varepsilon,
	\end{align*}
	thus the series $\sum_{j \in \mathcal{J}} Ax_j$ converges unconditionally to $Ax$.
\end{proof}

\begin{prop}\label{exchange order double series}
	Suppose that $\sum_{(n,m) \in \N^2} x_{n,m}$ converges unconditionally to $x \in X$. Then the inner partial sum $s_{n,M} = \sum_{m=1}^{M} x_{n,m}$ converges to some $y_n \in X$ for every $n \in \N$ and $x = \sum_{n \in \N} y_n$ with unconditional convergence. Similarly, $\sum_{n=1}^{N} x_{n,m}$ converges to some $z_m \in X$ for every $m \in \N$ and $x = \sum_{m \in \N} z_m$.
\end{prop}
\begin{proof}
	Since $\sum_{(n,m) \in \N^2} x_{n,m}$ is unconditionally convergent to $x$, given $\varepsilon > 0$, by definition there exist $J_0 \subset \N^2$ such that $\|x - \sum_{(n,m) \in J} x_{n,m}\| < \varepsilon$ for every finite set $J$ containing $J_0$. Without loss of generality, we can suppose that $J_0$ is of the form $J_0 = \{(n,m) \in \N^2 : n \leq N_0,\, m \leq M_0\}$ for some $N_0,M_0 \in \N$. In such a way we have:
	\begin{equation}\label{exchange order double series estimate 1}
		\| x - \sum_{n=1}^N \sum_{m=1}^M x_{n,m} \| < \varepsilon
	\end{equation}
	for every $N \geq N_0$ and $M \geq M_0$. Consider now $I \subset \N$ finite and $M_1,M_2 \in \N$ such that $M_0 \leq M_1 < M_2$. Then:
	\begin{align*}
		\|\sum_{n \in I} s_{n,M_2} - \sum_{n \in I} s_{n,M_1}\| &= \| \sum_{n \in I} \sum_{m=1}^{M_2} x_{n,m} - \sum_{n \in I} \sum_{m=1}^{M_1} x_{n,m} \| = \| \sum_{n \in I} \sum_{m=M_1+1}^{M_2} x_{n,m}\| =\\
									  &= \| \sum_{n \in I} \sum_{m=M_1+1}^{M_2} x_{n,m} + \sum_{(n,m) \in J_0} x_{n,m} -\sum_{(n,m) \in J_0} x_{n,m} \|
	\end{align*}
	Letting $J = J_0 \cup (I \times \{M_1+1,\ldots,M_2\}) \subset \N^2$ and using triangular inequality we obtain:
	\begin{equation}\label{exchange order double series estimate 2}
		\|\sum_{n \in I} (s_{n,M_2} - s_{n,M_1})\| \leq \|x - \sum_{(n,m) \in J} x_{n,m} \| + \|x - \sum_{(n,m) \in J_0} x_{n,m} \| < 2\varepsilon
	\end{equation}
	because $J \supseteq J_0$. This proves that, for every $n \in \N$ and for every $I \subset \N$ finite, the sequence $\{ \sum_{n \in I} s_{n,M}\}_{M \in \N}$ is a Cauchy sequence in $X$ which is a Banach space, therefore it is convergent. In particular, taking $I = \{n\}$, we obtain that the sequence $\{s_{n,M}\}_{M \in \N}$ converges to some $y_n \in X$ for every $n \in \N$. Moreover, since $I$ is finite, we have that $\sum_{n \in I} s_{n,M} \overset{M \rightarrow +\infty}{\longrightarrow} \sum_{n \in I} y_n$.
	
	Now we have to show that $\sum_{n \in \N} y_n$ converges unconditionally to $x$. Consider $I \subset \N$ finite such that $\{1,\ldots,N_0\} \subseteq I$. First of all we notice that:
	\begin{equation*}
		\lim_{M_2 \rightarrow +\infty} \left( \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} s_{n,M_2}\right) = \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} y_n.
	\end{equation*}
	Therefore, taking the $\limsup_{M_2 \geq M_1}$ in \eqref{exchange order double series estimate 3} we obtain
	\begin{equation}\label{exchange order double series estimate 3}
		\| \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} y_n \| = \limsup_{M_2 \geq M_1} \|  \sum_{n \in I} s_{n,M_1} - \sum_{n \in I} s_{n,M_2} \| \leq 2\varepsilon.
	\end{equation}
	Then we have:
	\begin{align*}
		\|x - \sum_{n \in I} y_n \| \leq \| x - \sum_{n \in I} s_{n,M_1} \| + \| \sum_{n \in I} s_{n,M_1} -  \sum_{n \in I} y_n \| \overset{\eqref{exchange order double series estimate 1} + \eqref{exchange order double series estimate 3}}{<} 3\varepsilon.
	\end{align*}

	The last part of the statement easily follows swapping $n$ and $m$ in previous computations.
\end{proof}


\chapter{Calculations}

\section{Constant in Lieb's inequality}\label{constant in Lieb's inequality calculation}
In the last part of the proof of Lieb's inequality \ref{Lieb's inequality} we used the following equality $A_{p'}^d A_{2/p'}^{2d/p'}A_{(p/p')'}^{d/p'}=(2/p)^{d/p}$ without proving it. We recall that the Babenko-Bechner constant $A_p$ is given by:
\begin{equation*}
	A_p = \left(\dfrac{p^{1/p}}{p'^{1/p'}}\right)^{1/2}.
\end{equation*}
Since there is an exponent $1/2$ in the Babenko-Bechner constant it is better to compute the square of $A_{p'}^d A_{2/p'}^{2d/p'}A_{(p/p')'}^{d/p'}$. For the sake of clarity we are going to compute every single term and then we are going to multiply them.
\begin{itemize}
	\item $\displaystyle A_{p'}^2 = \dfrac{p'^{1/p'}}{p^{1/p}}$;
	\item In order to compute $A_{2/p'}^{2\cdot 2/p'}$ we start computing $(2/p')'$:
	\begin{equation*}
		\left(\dfrac{2}{p'}\right)' = \dfrac{2/p'}{2/p'-1} = \dfrac{2}{2-p'},
	\end{equation*}
	therefore
	\begin{align*}
		A_{2/p'}^{2\cdot 2/p'} &= \left[  \left(\dfrac{2}{p'}\right)^{p'/2} \left(\dfrac{2-p'}{2}\right)^{(2-p')/2}\right]^{2/p'} = \dfrac{2}{p'} \left(\dfrac{2-p'}{2}\right)^{(2-p')/(p')} = \\
							   &= \dfrac{2^{2(1-1/p')}}{p'}(2-p')^{2/p'-1} = \dfrac{2^{2/p}}{p'} (2-p')^{1/p'-1/p};
	\end{align*}
	\item Like the previous case, we start computing $(p/p')'$:
	\begin{equation*}
		\left(\dfrac{p}{p'}\right)' = \dfrac{p/p'}{p/p'-1} = \dfrac{p}{p-p'},
	\end{equation*}
	hence
	\begin{align*}
		A_{(p/p')'}^{2/p'} &= \left[ \left(\dfrac{p}{p-p'}\right)^{(p-p')/p} \left(\dfrac{p'}{p}\right)^{p'/p}\right]^{1/p'} = \left(\dfrac{p}{p-p'}\right)^{1/p' - 1/p} \left(\dfrac{p'}{p}\right)^{1/p} = \\
						   &= \left(1-\dfrac{p'}{p}\right)^{1/p-1/p'} \left(\dfrac{p'}{p}\right)^{1/p} = (2-p')^{1/p-1/p'}\left(\dfrac{p'}{p}\right)^{1/p}.
	\end{align*}
	We are now ready to calculate the product of the three constant:
	\begin{align*}
		A_{p'}^2 A_{2/p'}^{2 \cdot 2/p'} A_{(p/p')'}^{2/p'} &= \dfrac{p'^{1/p'}}{p^{1/p}} \dfrac{2^{2/p}}{p'} (2-p')^{1/p'-1/p} (2-p')^{1/p-1/p'}\left(\dfrac{p'}{p}\right)^{1/p} = \\
															&= 2^{2/p} p^{-2/p}p'^{1/p'+1/p-1} = \left(\dfrac{2}{p}\right)^{2/p}
	\end{align*}
	which is the desired result.
\end{itemize}



\section{Curious inequality between conjugate exponent}\label{curious inequality between conjugate exponents}
We consider inequality \eqref{curious inequality between conjugate exponents original}, which we rewrite for the sake of clarity:
\begin{equation*}
	\kappa_p^{d\left(\frac1q - \frac1p\right)}\left(\dfrac{p}{q}\right)^{\frac{d}{q}} \geq \left(\dfrac{\kappa_p^{\kappa_p}}{\kappa_q^{\kappa_q}}\right)^d
\end{equation*}
Firstly we want to restate in a more concise way. Recalling that $\kappa_p =\frac{1}{p'}$, where $p'$ is the conjugate exponent of $p$, we have:
\begin{align*}
	\left(\dfrac{1}{p'}\right)^{\frac{1}{q}-\frac{1}{p}}\left(\dfrac{p}{q}\right)^{\frac{1}{q}} \geq \left(\dfrac{1}{p'}\right)^{\frac{1}{p'}} \left(\dfrac{1}{q'}\right)^{-\frac{1}{q'}}\iff \left(\dfrac{1}{p'}\right)^{\frac{1}{q}-\frac{1}{p} - \frac{1}{p'}}\left(\dfrac{1}{q'}\right)^{\frac{1}{q'}} \left(\dfrac{p}{q}\right)^{\frac{1}{q}} \geq 1
\end{align*}
but, since $\frac{1}{p}+\frac{1}{p'}=1$ and $\frac{1}{q} - 1 = \frac{1}{q'}$, in conclusion we have:
\begin{equation*}
	\left(\dfrac{p'}{q'}\right)^{\frac1{q'}} \left(\dfrac{p}{q}\right)^{\frac{1}{q}} \geq 1
\end{equation*}
In order to prove that this inequality holds for every pair of $p,q>1$ we consider the left-hand side as function of $x=\frac{1}{p}$ and $y=\frac{1}{q}$ (therefore $\frac{1}{p'} = 1-x$ and $\frac{1}{q'} = 1-y$). If we take the logarithm of this quantity we want to show that:
	\begin{equation*}
		f(x,y) = (1-y)\left[\log(1-y) - \log(1-x)\right] + y\left[\log(y) - \log(x)\right] \geq 0.
	\end{equation*}
	The partial derivative of $f$ with respect to $x$ is:
	\begin{equation*}
		\pdfrac{f}{x}(x,y) = \dfrac{1-y}{1-x} - \dfrac{y}{x} = \dfrac{x-y}{x(1-x)}.
	\end{equation*}
	Since $x \in (0,1)$, $\pfrac{f}{x}(x,y)$ is negative for $x<y$ and positive for $x>y$, so $f$ has a minimum for $x=y$ where $f(x,x) = 0$.

\section{Another inequality between conjugate exponents}\label{another inequality between conjugate exponents}
We want to prove that inequality \eqref{another inequality between conjugate exponents original} holds, namely that:
\begin{align*}
	\kappa_q^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{p}} < \kappa_p^{d\left(\frac{1}{q}-\frac{1}{p}\right)} \left(\dfrac{p}{q}\right)^{\frac{d}{q}}
\end{align*}
whenever $p \neq q$. As for the previous section, since $\kappa_p = \frac{1}{p'}$ and $\kappa_q = \frac{1}{q'}$ we can write the inequality in a more concise way:
\begin{align*}
	\left(\dfrac{1}{q'}\right)^{\frac{1}{q}-\frac{1}{p}} \left(\dfrac{p}{q}\right)^{\frac{1}{p}} < \left(\dfrac{1}{p'}\right)^{\frac{1}{q}-\frac{1}{p}} \left(\dfrac{p}{q}\right)^{\frac{1}{q}} \iff \left(\dfrac{q\phantom{'} \, p'}{q' \, p}\right)^{\frac{1}{q}-\frac{1}{p}} < 1 \iff \left(\dfrac{q-1}{p-1}\right)^{\frac{1}{q}-\frac{1}{p}} < 1.
\end{align*}
If $q>p$ the base is greater than 1 while the exponent is less than 1, whereas if $q<p$ the converse happens, which proves that inequality holds whenever $p \neq q$.

\nocite{*}
\printbibliography


	

\end{document}32